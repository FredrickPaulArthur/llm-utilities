{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8d91c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pprint import pprint\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70bfed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Fredrick\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Fredrick! It‚Äôs great to meet you. I‚Äôm an AI language model that loves chatting about all sorts of topics‚Äîwhether you‚Äôre into books, movies, science, cooking, travel, or just want to share a funny story from your day, I‚Äôm all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‚Äë2024, so I can riff on everything from classic literature to the latest tech trends. I don‚Äôt have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What‚Äôs on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation‚Äîmaybe a new sci‚Äëfi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Fredrick\n",
      "AI: Hello Fredrick! It‚Äôs great to meet you. I‚Äôm an AI language model that loves chatting about all sorts of topics‚Äîwhether you‚Äôre into books, movies, science, cooking, travel, or just want to share a funny story from your day, I‚Äôm all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‚Äë2024, so I can riff on everything from classic literature to the latest tech trends. I don‚Äôt have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What‚Äôs on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation‚Äîmaybe a new sci‚Äëfi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "Human: What is AWS Glue?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**AWS Glue** is Amazon‚ÄØWeb‚ÄØServices‚Äô fully‚Äëmanaged, serverless data‚Äëintegration service. In plain English, it‚Äôs a set of tools that help you move, clean, enrich, and catalog data so you can run analytics or machine‚Äëlearning workloads without having to spin up and manage your own ETL (Extract‚ÄëTransform‚ÄëLoad) infrastructure.\n",
      "\n",
      "Below is a quick‚Äëlook‚Äëaround of the main pieces, why people love it, and a few things to watch out for.\n",
      "\n",
      "---\n",
      "\n",
      "## 1Ô∏è‚É£ Core Components\n",
      "\n",
      "| Component | What it does | Typical use‚Äëcase |\n",
      "|-----------|--------------|------------------|\n",
      "| **Glue Data Catalog** | Central metadata repository (tables, schemas, partitions, locations). It‚Äôs the ‚ÄúHive Metastore‚Äù for the AWS world. | Acts as a single source of truth for Athena, Redshift Spectrum, EMR, and even external tools like Tableau. |\n",
      "| **Crawlers** | Scans data stores (S3, JDBC, DynamoDB, etc.) and automatically infers schema, partitions, and populates the Data Catalog. | You drop a new CSV/JSON/Parquet file into an S3 bucket and a crawler registers it as a table in minutes. |\n",
      "| **ETL Jobs** | Serverless Spark (or Python/Scala) scripts that read from sources, apply transformations, and write to targets. | Typical pipelines: ‚ÄúRead raw logs from S3 ‚Üí parse ‚Üí enrich with lookup table ‚Üí write cleaned data to Parquet in another bucket.‚Äù |\n",
      "| **Triggers & Workflows** | Event‚Äëdriven or schedule‚Äëdriven orchestration (cron, on‚Äëdemand, on‚Äëcompletion of another job). | Build multi‚Äëstep pipelines: e.g., run a crawler ‚Üí start a job ‚Üí notify Slack when done. |\n",
      "| **Development Endpoints** | Interactive notebooks (Jupyter, Zeppelin) that let you prototype ETL code against live data. | Data engineers love the ‚Äúwrite‚Äëonce, test‚Äëmany‚Äù loop before committing a production job. |\n",
      "| **Glue Studio** (visual UI) | Drag‚Äëand‚Äëdrop canvas for building ETL jobs without writing code. | Great for analysts or quick PoCs where you want to see the flow graphically. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2Ô∏è‚É£ How It Works (high‚Äëlevel flow)\n",
      "\n",
      "1. **Ingest** ‚Äì Data lands in a source (commonly S3, but also RDS, DynamoDB, Kafka, etc.).\n",
      "2. **Catalog** ‚Äì A **crawler** runs (or you manually define a table) and registers the dataset in the **Glue Data Catalog**.\n",
      "3. **Transform** ‚Äì A **Glue job** (Spark‚Äëbased) reads the cataloged table, applies transformations (SQL‚Äëlike, Python UDFs, joins, window functions, etc.).\n",
      "4. **Load** ‚Äì The job writes the result to a destination (another S3 location, Redshift, Snowflake via JDBC, etc.).\n",
      "5. **Consume** ‚Äì Downstream services (Athena, Redshift Spectrum, QuickSight, SageMaker) query the cleaned data directly from the catalog.\n",
      "\n",
      "All of this runs on **serverless Spark**: you never provision EC2 instances or manage a Spark cluster. Glue automatically scales the underlying compute up or down based on the job‚Äôs workload.\n",
      "\n",
      "---\n",
      "\n",
      "## 3Ô∏è‚É£ Why Teams Choose Glue\n",
      "\n",
      "| Benefit | Detail |\n",
      "|---------|--------|\n",
      "| **Zero‚Äëops** | No clusters to patch, no capacity planning. You just pay for the compute seconds you actually use. |\n",
      "| **Integrated catalog** | One place to store schema metadata that many AWS analytics services can read. |\n",
      "| **Auto‚Äëschema inference** | Crawlers can detect column types, partitions, and even nested structures in JSON/Parquet. |\n",
      "| **Pay‚Äëas‚Äëyou‚Äëgo pricing** | Charged per DPU‚Äëhour (Data Processing Unit) for jobs, per crawler run, and per catalog storage (tiny cost). |\n",
      "| **Built‚Äëin security** | IAM policies, VPC endpoints, encryption at rest (S3/KMS) and in transit, and fine‚Äëgrained column‚Äëlevel permissions via Lake Formation. |\n",
      "| **Extensibility** | You can write custom Spark code, use Python libraries, or call external APIs from within a job. |\n",
      "| **Serverless notebooks** | Development endpoints let you prototype with familiar tools (Jupyter). |\n",
      "\n",
      "---\n",
      "\n",
      "## 4Ô∏è‚É£ Typical Use Cases\n",
      "\n",
      "| Scenario | How Glue fits |\n",
      "|----------|---------------|\n",
      "| **Data lake ingestion** | Crawl raw logs ‚Üí register ‚Üí transform ‚Üí store as columnar Parquet for analytics. |\n",
      "| **ETL for reporting** | Pull data from RDS ‚Üí cleanse ‚Üí load into Redshift or Snowflake. |\n",
      "| **Machine‚Äëlearning feature pipelines** | Extract features from S3 ‚Üí join with reference tables ‚Üí write feature store files for SageMaker. |\n",
      "| **Regulatory compliance** | Use crawlers + Lake Formation to enforce column‚Äëlevel masking before data is exposed to analysts. |\n",
      "| **Event‚Äëdriven pipelines** | Trigger a Glue job when a new file lands in an S3 bucket (via S3 event ‚Üí Lambda ‚Üí Glue start‚Äëjob). |\n",
      "\n",
      "---\n",
      "\n",
      "## 5Ô∏è‚É£ Pricing Snapshot (as of 2024)\n",
      "\n",
      "| Resource | Unit | Approx. cost (US‚ÄëEast‚Äë1) |\n",
      "|----------|------|--------------------------|\n",
      "| **Glue Data Catalog storage** | per 1‚ÄØM tables/partitions per month | $1.00 |\n",
      "| **Crawler run** | per 1‚ÄØ000‚ÄØseconds of runtime | $0.44 |\n",
      "| **ETL job** | per DPU‚Äëhour (1‚ÄØDPU = 4‚ÄØvCPU + 16‚ÄØGB RAM) | $0.44 (standard) or $0.88 (G.1X) |\n",
      "| **Glue Studio visual jobs** | same as ETL job pricing | ‚Äì |\n",
      "| **Development endpoint** | per DPU‚Äëhour while active | $0.44 |\n",
      "\n",
      "> **Tip:** Because Glue jobs are billed per second (rounded up to the nearest 1‚ÄØsecond), you can keep costs low by using **job bookmarks** (process only new data) and **dynamic partition pruning** to avoid scanning whole tables.\n",
      "\n",
      "---\n",
      "\n",
      "## 6Ô∏è‚É£ Things to Keep in Mind\n",
      "\n",
      "| Potential drawback | Mitigation |\n",
      "|--------------------|------------|\n",
      "| **Cold‚Äëstart latency** ‚Äì First run of a job can take ~30‚Äë60‚ÄØseconds to spin up Spark executors. | Keep a ‚Äúwarm‚Äù job running on a schedule (e.g., every hour) if you need sub‚Äësecond response. |\n",
      "| **Limited language support** ‚Äì Native support is Scala, Python (PySpark), and Spark SQL. No native Java or .NET. | Use PySpark for most cases; you can still call Java libraries via Py4J if needed. |\n",
      "| **Cost visibility** ‚Äì Large Spark jobs can consume many DPUs quickly. | Enable **job metrics** and CloudWatch alarms on DPU usage; break jobs into smaller stages if possible. |\n",
      "| **Complex transformations** ‚Äì Very custom logic may be easier in a dedicated EMR cluster. | Evaluate whether Glue‚Äôs serverless model fits the complexity; you can still launch an EMR cluster from a Glue workflow if needed. |\n",
      "| **Versioning of libraries** ‚Äì Glue provides a set of pre‚Äëinstalled Spark packages; custom packages must be uploaded as .zip/.whl. | Use **Glue 4.0** (the latest runtime) which supports Python‚ÄØ3.9 and a broader set of libraries, and test your dependencies locally first. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7Ô∏è‚É£ Quick ‚ÄúHello‚ÄëWorld‚Äù Example (Python/Glue‚ÄØETL)\n",
      "\n",
      "```python\n",
      "import sys\n",
      "from awsglue.transforms import *\n",
      "from awsglue.utils import getResolvedOptions\n",
      "from awsglue.context import GlueContext\n",
      "from pyspark.context import SparkContext\n",
      "\n",
      "args = getResolvedOptions(sys.argv,\n",
      "                          ['JOB_NAME',\n",
      "                           'SOURCE_DB',\n",
      "                           'SOURCE_TABLE',\n",
      "                           'TARGET_PATH'])\n",
      "\n",
      "sc = SparkContext()\n",
      "glueContext = GlueContext(sc)\n",
      "spark = glueContext.spark_session\n",
      "\n",
      "# 1Ô∏è‚É£ Read from the Data Catalog\n",
      "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
      "    database=args['SOURCE_DB'],\n",
      "    table_name=args['SOURCE_TABLE']\n",
      ")\n",
      "\n",
      "# 2Ô∏è‚É£ Simple transformation: drop nulls & rename a column\n",
      "transformed = datasource \\\n",
      "    .filter(f=lambda x: x[\"price\"] is not None) \\\n",
      "    .rename_field(\"old_name\", \"new_name\")\n",
      "\n",
      "# 3Ô∏è‚É£ Write out as partitioned Parquet\n",
      "glueContext.write_dynamic_frame.from_options(\n",
      "    frame=transformed,\n",
      "    connection_type=\"s3\",\n",
      "    connection_options={\"path\": args['TARGET_PATH'],\n",
      "                        \"partitionKeys\": [\"year\", \"month\"]},\n",
      "    format=\"parquet\"\n",
      ")\n",
      "\n",
      "job.commit()\n",
      "```\n",
      "\n",
      "You‚Äôd submit this script as a **Glue job**, pass the arguments (e.g., `--SOURCE_DB mydb --SOURCE_TABLE raw_logs --TARGET_PATH s3://my‚Äëlake/cleaned/`), and Glue will spin up the Spark executors, run the code, and shut down automatically.\n",
      "\n",
      "---\n",
      "\n",
      "## 8Ô∏è‚É£ Where to Learn More\n",
      "\n",
      "- **AWS Documentation** ‚Äì ‚ÄúAWS Glue Developer Guide‚Äù (covers crawlers, jobs, Studio, and best practices).  \n",
      "- **AWS Glue Workshops** ‚Äì Hands‚Äëon labs in the AWS Training portal.  \n",
      "- **AWS re:Invent videos** ‚Äì Look for ‚ÄúServerless ETL with AWS Glue‚Äù (2023/2024 sessions).  \n",
      "- **GitHub** ‚Äì The `aws-samples/aws-glue-samples` repo has ready‚Äëmade job scripts and notebooks.  \n",
      "\n",
      "---\n",
      "\n",
      "### üé§ Your Turn!\n",
      "Do you have a specific data source you‚Äôre trying to bring into a lake, or a particular transformation you‚Äôre curious about (e.g., handling nested JSON, joining with a RDS table, or using Glue Studio‚Äôs visual editor)? Let me know, and we can sketch out a concrete pipeline together! \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Fredrick\n",
      "AI: Hello Fredrick! It‚Äôs great to meet you. I‚Äôm an AI language model that loves chatting about all sorts of topics‚Äîwhether you‚Äôre into books, movies, science, cooking, travel, or just want to share a funny story from your day, I‚Äôm all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‚Äë2024, so I can riff on everything from classic literature to the latest tech trends. I don‚Äôt have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What‚Äôs on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation‚Äîmaybe a new sci‚Äëfi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "Human: What is AWS Glue?\n",
      "AI: **AWS Glue** is Amazon‚ÄØWeb‚ÄØServices‚Äô fully‚Äëmanaged, serverless data‚Äëintegration service. In plain English, it‚Äôs a set of tools that help you move, clean, enrich, and catalog data so you can run analytics or machine‚Äëlearning workloads without having to spin up and manage your own ETL (Extract‚ÄëTransform‚ÄëLoad) infrastructure.\n",
      "\n",
      "Below is a quick‚Äëlook‚Äëaround of the main pieces, why people love it, and a few things to watch out for.\n",
      "\n",
      "---\n",
      "\n",
      "## 1Ô∏è‚É£ Core Components\n",
      "\n",
      "| Component | What it does | Typical use‚Äëcase |\n",
      "|-----------|--------------|------------------|\n",
      "| **Glue Data Catalog** | Central metadata repository (tables, schemas, partitions, locations). It‚Äôs the ‚ÄúHive Metastore‚Äù for the AWS world. | Acts as a single source of truth for Athena, Redshift Spectrum, EMR, and even external tools like Tableau. |\n",
      "| **Crawlers** | Scans data stores (S3, JDBC, DynamoDB, etc.) and automatically infers schema, partitions, and populates the Data Catalog. | You drop a new CSV/JSON/Parquet file into an S3 bucket and a crawler registers it as a table in minutes. |\n",
      "| **ETL Jobs** | Serverless Spark (or Python/Scala) scripts that read from sources, apply transformations, and write to targets. | Typical pipelines: ‚ÄúRead raw logs from S3 ‚Üí parse ‚Üí enrich with lookup table ‚Üí write cleaned data to Parquet in another bucket.‚Äù |\n",
      "| **Triggers & Workflows** | Event‚Äëdriven or schedule‚Äëdriven orchestration (cron, on‚Äëdemand, on‚Äëcompletion of another job). | Build multi‚Äëstep pipelines: e.g., run a crawler ‚Üí start a job ‚Üí notify Slack when done. |\n",
      "| **Development Endpoints** | Interactive notebooks (Jupyter, Zeppelin) that let you prototype ETL code against live data. | Data engineers love the ‚Äúwrite‚Äëonce, test‚Äëmany‚Äù loop before committing a production job. |\n",
      "| **Glue Studio** (visual UI) | Drag‚Äëand‚Äëdrop canvas for building ETL jobs without writing code. | Great for analysts or quick PoCs where you want to see the flow graphically. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2Ô∏è‚É£ How It Works (high‚Äëlevel flow)\n",
      "\n",
      "1. **Ingest** ‚Äì Data lands in a source (commonly S3, but also RDS, DynamoDB, Kafka, etc.).\n",
      "2. **Catalog** ‚Äì A **crawler** runs (or you manually define a table) and registers the dataset in the **Glue Data Catalog**.\n",
      "3. **Transform** ‚Äì A **Glue job** (Spark‚Äëbased) reads the cataloged table, applies transformations (SQL‚Äëlike, Python UDFs, joins, window functions, etc.).\n",
      "4. **Load** ‚Äì The job writes the result to a destination (another S3 location, Redshift, Snowflake via JDBC, etc.).\n",
      "5. **Consume** ‚Äì Downstream services (Athena, Redshift Spectrum, QuickSight, SageMaker) query the cleaned data directly from the catalog.\n",
      "\n",
      "All of this runs on **serverless Spark**: you never provision EC2 instances or manage a Spark cluster. Glue automatically scales the underlying compute up or down based on the job‚Äôs workload.\n",
      "\n",
      "---\n",
      "\n",
      "## 3Ô∏è‚É£ Why Teams Choose Glue\n",
      "\n",
      "| Benefit | Detail |\n",
      "|---------|--------|\n",
      "| **Zero‚Äëops** | No clusters to patch, no capacity planning. You just pay for the compute seconds you actually use. |\n",
      "| **Integrated catalog** | One place to store schema metadata that many AWS analytics services can read. |\n",
      "| **Auto‚Äëschema inference** | Crawlers can detect column types, partitions, and even nested structures in JSON/Parquet. |\n",
      "| **Pay‚Äëas‚Äëyou‚Äëgo pricing** | Charged per DPU‚Äëhour (Data Processing Unit) for jobs, per crawler run, and per catalog storage (tiny cost). |\n",
      "| **Built‚Äëin security** | IAM policies, VPC endpoints, encryption at rest (S3/KMS) and in transit, and fine‚Äëgrained column‚Äëlevel permissions via Lake Formation. |\n",
      "| **Extensibility** | You can write custom Spark code, use Python libraries, or call external APIs from within a job. |\n",
      "| **Serverless notebooks** | Development endpoints let you prototype with familiar tools (Jupyter). |\n",
      "\n",
      "---\n",
      "\n",
      "## 4Ô∏è‚É£ Typical Use Cases\n",
      "\n",
      "| Scenario | How Glue fits |\n",
      "|----------|---------------|\n",
      "| **Data lake ingestion** | Crawl raw logs ‚Üí register ‚Üí transform ‚Üí store as columnar Parquet for analytics. |\n",
      "| **ETL for reporting** | Pull data from RDS ‚Üí cleanse ‚Üí load into Redshift or Snowflake. |\n",
      "| **Machine‚Äëlearning feature pipelines** | Extract features from S3 ‚Üí join with reference tables ‚Üí write feature store files for SageMaker. |\n",
      "| **Regulatory compliance** | Use crawlers + Lake Formation to enforce column‚Äëlevel masking before data is exposed to analysts. |\n",
      "| **Event‚Äëdriven pipelines** | Trigger a Glue job when a new file lands in an S3 bucket (via S3 event ‚Üí Lambda ‚Üí Glue start‚Äëjob). |\n",
      "\n",
      "---\n",
      "\n",
      "## 5Ô∏è‚É£ Pricing Snapshot (as of 2024)\n",
      "\n",
      "| Resource | Unit | Approx. cost (US‚ÄëEast‚Äë1) |\n",
      "|----------|------|--------------------------|\n",
      "| **Glue Data Catalog storage** | per 1‚ÄØM tables/partitions per month | $1.00 |\n",
      "| **Crawler run** | per 1‚ÄØ000‚ÄØseconds of runtime | $0.44 |\n",
      "| **ETL job** | per DPU‚Äëhour (1‚ÄØDPU = 4‚ÄØvCPU + 16‚ÄØGB RAM) | $0.44 (standard) or $0.88 (G.1X) |\n",
      "| **Glue Studio visual jobs** | same as ETL job pricing | ‚Äì |\n",
      "| **Development endpoint** | per DPU‚Äëhour while active | $0.44 |\n",
      "\n",
      "> **Tip:** Because Glue jobs are billed per second (rounded up to the nearest 1‚ÄØsecond), you can keep costs low by using **job bookmarks** (process only new data) and **dynamic partition pruning** to avoid scanning whole tables.\n",
      "\n",
      "---\n",
      "\n",
      "## 6Ô∏è‚É£ Things to Keep in Mind\n",
      "\n",
      "| Potential drawback | Mitigation |\n",
      "|--------------------|------------|\n",
      "| **Cold‚Äëstart latency** ‚Äì First run of a job can take ~30‚Äë60‚ÄØseconds to spin up Spark executors. | Keep a ‚Äúwarm‚Äù job running on a schedule (e.g., every hour) if you need sub‚Äësecond response. |\n",
      "| **Limited language support** ‚Äì Native support is Scala, Python (PySpark), and Spark SQL. No native Java or .NET. | Use PySpark for most cases; you can still call Java libraries via Py4J if needed. |\n",
      "| **Cost visibility** ‚Äì Large Spark jobs can consume many DPUs quickly. | Enable **job metrics** and CloudWatch alarms on DPU usage; break jobs into smaller stages if possible. |\n",
      "| **Complex transformations** ‚Äì Very custom logic may be easier in a dedicated EMR cluster. | Evaluate whether Glue‚Äôs serverless model fits the complexity; you can still launch an EMR cluster from a Glue workflow if needed. |\n",
      "| **Versioning of libraries** ‚Äì Glue provides a set of pre‚Äëinstalled Spark packages; custom packages must be uploaded as .zip/.whl. | Use **Glue 4.0** (the latest runtime) which supports Python‚ÄØ3.9 and a broader set of libraries, and test your dependencies locally first. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7Ô∏è‚É£ Quick ‚ÄúHello‚ÄëWorld‚Äù Example (Python/Glue‚ÄØETL)\n",
      "\n",
      "```python\n",
      "import sys\n",
      "from awsglue.transforms import *\n",
      "from awsglue.utils import getResolvedOptions\n",
      "from awsglue.context import GlueContext\n",
      "from pyspark.context import SparkContext\n",
      "\n",
      "args = getResolvedOptions(sys.argv,\n",
      "                          ['JOB_NAME',\n",
      "                           'SOURCE_DB',\n",
      "                           'SOURCE_TABLE',\n",
      "                           'TARGET_PATH'])\n",
      "\n",
      "sc = SparkContext()\n",
      "glueContext = GlueContext(sc)\n",
      "spark = glueContext.spark_session\n",
      "\n",
      "# 1Ô∏è‚É£ Read from the Data Catalog\n",
      "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
      "    database=args['SOURCE_DB'],\n",
      "    table_name=args['SOURCE_TABLE']\n",
      ")\n",
      "\n",
      "# 2Ô∏è‚É£ Simple transformation: drop nulls & rename a column\n",
      "transformed = datasource \\\n",
      "    .filter(f=lambda x: x[\"price\"] is not None) \\\n",
      "    .rename_field(\"old_name\", \"new_name\")\n",
      "\n",
      "# 3Ô∏è‚É£ Write out as partitioned Parquet\n",
      "glueContext.write_dynamic_frame.from_options(\n",
      "    frame=transformed,\n",
      "    connection_type=\"s3\",\n",
      "    connection_options={\"path\": args['TARGET_PATH'],\n",
      "                        \"partitionKeys\": [\"year\", \"month\"]},\n",
      "    format=\"parquet\"\n",
      ")\n",
      "\n",
      "job.commit()\n",
      "```\n",
      "\n",
      "You‚Äôd submit this script as a **Glue job**, pass the arguments (e.g., `--SOURCE_DB mydb --SOURCE_TABLE raw_logs --TARGET_PATH s3://my‚Äëlake/cleaned/`), and Glue will spin up the Spark executors, run the code, and shut down automatically.\n",
      "\n",
      "---\n",
      "\n",
      "## 8Ô∏è‚É£ Where to Learn More\n",
      "\n",
      "- **AWS Documentation** ‚Äì ‚ÄúAWS Glue Developer Guide‚Äù (covers crawlers, jobs, Studio, and best practices).  \n",
      "- **AWS Glue Workshops** ‚Äì Hands‚Äëon labs in the AWS Training portal.  \n",
      "- **AWS re:Invent videos** ‚Äì Look for ‚ÄúServerless ETL with AWS Glue‚Äù (2023/2024 sessions).  \n",
      "- **GitHub** ‚Äì The `aws-samples/aws-glue-samples` repo has ready‚Äëmade job scripts and notebooks.  \n",
      "\n",
      "---\n",
      "\n",
      "### üé§ Your Turn!\n",
      "Do you have a specific data source you‚Äôre trying to bring into a lake, or a particular transformation you‚Äôre curious about (e.g., handling nested JSON, joining with a RDS table, or using Glue Studio‚Äôs visual editor)? Let me know, and we can sketch out a concrete pipeline together!\n",
      "Human: What is my name? And what did we talk about last time?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hey Fredrick! üëã‚ÄØNice to see you back.\n",
      "\n",
      "**Your name:**‚ÄØFredrick (you told me that right at the start of our chat).\n",
      "\n",
      "**What we talked about last time:**‚ÄØWe dove into **AWS‚ÄØGlue**, Amazon‚Äôs fully‚Äëmanaged, serverless data‚Äëintegration service. I gave you a rundown of its main components (Data Catalog, crawlers, ETL jobs, triggers/workflows, development endpoints, and Glue‚ÄØStudio), explained the high‚Äëlevel workflow (ingest ‚Üí catalog ‚Üí transform ‚Üí load ‚Üí consume), highlighted why teams love it (zero‚Äëops, integrated catalog, auto‚Äëschema inference, pay‚Äëas‚Äëyou‚Äëgo pricing, built‚Äëin security, extensibility, notebooks), listed common use cases (data‚Äëlake ingestion, reporting pipelines, ML feature prep, compliance, event‚Äëdriven pipelines), sketched the pricing model, pointed out a few gotchas, and even shared a quick ‚ÄúHello‚ÄëWorld‚Äù PySpark ETL script.\n",
      "\n",
      "If you‚Äôd like to pick up where we left off‚Äîmaybe you have a specific data source you want to bring into Glue, or you‚Äôre curious about building a workflow with crawlers and triggers‚Äîjust let me know and we can map it out together! üöÄ\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(api_key=os.environ['GROQ_API_KEY'], temperature=0.0, model=\"openai/gpt-oss-120b\")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(conversation.predict(input=\"Hi, my name is Fredrick\"))\n",
    "print(conversation.predict(input=\"What is AWS Glue?\"), \"\\n\")\n",
    "print(conversation.predict(input=\"What is my name? And what did we talk about last time?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0becb3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Fredrick\n",
      "AI: Hello Fredrick! It‚Äôs great to meet you. I‚Äôm an AI language model that loves chatting about all sorts of topics‚Äîwhether you‚Äôre into books, movies, science, cooking, travel, or just want to share a funny story from your day, I‚Äôm all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‚Äë2024, so I can riff on everything from classic literature to the latest tech trends. I don‚Äôt have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What‚Äôs on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation‚Äîmaybe a new sci‚Äëfi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "Human: What is AWS Glue?\n",
      "AI: **AWS Glue** is Amazon‚ÄØWeb‚ÄØServices‚Äô fully‚Äëmanaged, serverless data‚Äëintegration service. In plain English, it‚Äôs a set of tools that help you move, clean, enrich, and catalog data so you can run analytics or machine‚Äëlearning workloads without having to spin up and manage your own ETL (Extract‚ÄëTransform‚ÄëLoad) infrastructure.\n",
      "\n",
      "Below is a quick‚Äëlook‚Äëaround of the main pieces, why people love it, and a few things to watch out for.\n",
      "\n",
      "---\n",
      "\n",
      "## 1Ô∏è‚É£ Core Components\n",
      "\n",
      "| Component | What it does | Typical use‚Äëcase |\n",
      "|-----------|--------------|------------------|\n",
      "| **Glue Data Catalog** | Central metadata repository (tables, schemas, partitions, locations). It‚Äôs the ‚ÄúHive Metastore‚Äù for the AWS world. | Acts as a single source of truth for Athena, Redshift Spectrum, EMR, and even external tools like Tableau. |\n",
      "| **Crawlers** | Scans data stores (S3, JDBC, DynamoDB, etc.) and automatically infers schema, partitions, and populates the Data Catalog. | You drop a new CSV/JSON/Parquet file into an S3 bucket and a crawler registers it as a table in minutes. |\n",
      "| **ETL Jobs** | Serverless Spark (or Python/Scala) scripts that read from sources, apply transformations, and write to targets. | Typical pipelines: ‚ÄúRead raw logs from S3 ‚Üí parse ‚Üí enrich with lookup table ‚Üí write cleaned data to Parquet in another bucket.‚Äù |\n",
      "| **Triggers & Workflows** | Event‚Äëdriven or schedule‚Äëdriven orchestration (cron, on‚Äëdemand, on‚Äëcompletion of another job). | Build multi‚Äëstep pipelines: e.g., run a crawler ‚Üí start a job ‚Üí notify Slack when done. |\n",
      "| **Development Endpoints** | Interactive notebooks (Jupyter, Zeppelin) that let you prototype ETL code against live data. | Data engineers love the ‚Äúwrite‚Äëonce, test‚Äëmany‚Äù loop before committing a production job. |\n",
      "| **Glue Studio** (visual UI) | Drag‚Äëand‚Äëdrop canvas for building ETL jobs without writing code. | Great for analysts or quick PoCs where you want to see the flow graphically. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2Ô∏è‚É£ How It Works (high‚Äëlevel flow)\n",
      "\n",
      "1. **Ingest** ‚Äì Data lands in a source (commonly S3, but also RDS, DynamoDB, Kafka, etc.).\n",
      "2. **Catalog** ‚Äì A **crawler** runs (or you manually define a table) and registers the dataset in the **Glue Data Catalog**.\n",
      "3. **Transform** ‚Äì A **Glue job** (Spark‚Äëbased) reads the cataloged table, applies transformations (SQL‚Äëlike, Python UDFs, joins, window functions, etc.).\n",
      "4. **Load** ‚Äì The job writes the result to a destination (another S3 location, Redshift, Snowflake via JDBC, etc.).\n",
      "5. **Consume** ‚Äì Downstream services (Athena, Redshift Spectrum, QuickSight, SageMaker) query the cleaned data directly from the catalog.\n",
      "\n",
      "All of this runs on **serverless Spark**: you never provision EC2 instances or manage a Spark cluster. Glue automatically scales the underlying compute up or down based on the job‚Äôs workload.\n",
      "\n",
      "---\n",
      "\n",
      "## 3Ô∏è‚É£ Why Teams Choose Glue\n",
      "\n",
      "| Benefit | Detail |\n",
      "|---------|--------|\n",
      "| **Zero‚Äëops** | No clusters to patch, no capacity planning. You just pay for the compute seconds you actually use. |\n",
      "| **Integrated catalog** | One place to store schema metadata that many AWS analytics services can read. |\n",
      "| **Auto‚Äëschema inference** | Crawlers can detect column types, partitions, and even nested structures in JSON/Parquet. |\n",
      "| **Pay‚Äëas‚Äëyou‚Äëgo pricing** | Charged per DPU‚Äëhour (Data Processing Unit) for jobs, per crawler run, and per catalog storage (tiny cost). |\n",
      "| **Built‚Äëin security** | IAM policies, VPC endpoints, encryption at rest (S3/KMS) and in transit, and fine‚Äëgrained column‚Äëlevel permissions via Lake Formation. |\n",
      "| **Extensibility** | You can write custom Spark code, use Python libraries, or call external APIs from within a job. |\n",
      "| **Serverless notebooks** | Development endpoints let you prototype with familiar tools (Jupyter). |\n",
      "\n",
      "---\n",
      "\n",
      "## 4Ô∏è‚É£ Typical Use Cases\n",
      "\n",
      "| Scenario | How Glue fits |\n",
      "|----------|---------------|\n",
      "| **Data lake ingestion** | Crawl raw logs ‚Üí register ‚Üí transform ‚Üí store as columnar Parquet for analytics. |\n",
      "| **ETL for reporting** | Pull data from RDS ‚Üí cleanse ‚Üí load into Redshift or Snowflake. |\n",
      "| **Machine‚Äëlearning feature pipelines** | Extract features from S3 ‚Üí join with reference tables ‚Üí write feature store files for SageMaker. |\n",
      "| **Regulatory compliance** | Use crawlers + Lake Formation to enforce column‚Äëlevel masking before data is exposed to analysts. |\n",
      "| **Event‚Äëdriven pipelines** | Trigger a Glue job when a new file lands in an S3 bucket (via S3 event ‚Üí Lambda ‚Üí Glue start‚Äëjob). |\n",
      "\n",
      "---\n",
      "\n",
      "## 5Ô∏è‚É£ Pricing Snapshot (as of 2024)\n",
      "\n",
      "| Resource | Unit | Approx. cost (US‚ÄëEast‚Äë1) |\n",
      "|----------|------|--------------------------|\n",
      "| **Glue Data Catalog storage** | per 1‚ÄØM tables/partitions per month | $1.00 |\n",
      "| **Crawler run** | per 1‚ÄØ000‚ÄØseconds of runtime | $0.44 |\n",
      "| **ETL job** | per DPU‚Äëhour (1‚ÄØDPU = 4‚ÄØvCPU + 16‚ÄØGB RAM) | $0.44 (standard) or $0.88 (G.1X) |\n",
      "| **Glue Studio visual jobs** | same as ETL job pricing | ‚Äì |\n",
      "| **Development endpoint** | per DPU‚Äëhour while active | $0.44 |\n",
      "\n",
      "> **Tip:** Because Glue jobs are billed per second (rounded up to the nearest 1‚ÄØsecond), you can keep costs low by using **job bookmarks** (process only new data) and **dynamic partition pruning** to avoid scanning whole tables.\n",
      "\n",
      "---\n",
      "\n",
      "## 6Ô∏è‚É£ Things to Keep in Mind\n",
      "\n",
      "| Potential drawback | Mitigation |\n",
      "|--------------------|------------|\n",
      "| **Cold‚Äëstart latency** ‚Äì First run of a job can take ~30‚Äë60‚ÄØseconds to spin up Spark executors. | Keep a ‚Äúwarm‚Äù job running on a schedule (e.g., every hour) if you need sub‚Äësecond response. |\n",
      "| **Limited language support** ‚Äì Native support is Scala, Python (PySpark), and Spark SQL. No native Java or .NET. | Use PySpark for most cases; you can still call Java libraries via Py4J if needed. |\n",
      "| **Cost visibility** ‚Äì Large Spark jobs can consume many DPUs quickly. | Enable **job metrics** and CloudWatch alarms on DPU usage; break jobs into smaller stages if possible. |\n",
      "| **Complex transformations** ‚Äì Very custom logic may be easier in a dedicated EMR cluster. | Evaluate whether Glue‚Äôs serverless model fits the complexity; you can still launch an EMR cluster from a Glue workflow if needed. |\n",
      "| **Versioning of libraries** ‚Äì Glue provides a set of pre‚Äëinstalled Spark packages; custom packages must be uploaded as .zip/.whl. | Use **Glue 4.0** (the latest runtime) which supports Python‚ÄØ3.9 and a broader set of libraries, and test your dependencies locally first. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7Ô∏è‚É£ Quick ‚ÄúHello‚ÄëWorld‚Äù Example (Python/Glue‚ÄØETL)\n",
      "\n",
      "```python\n",
      "import sys\n",
      "from awsglue.transforms import *\n",
      "from awsglue.utils import getResolvedOptions\n",
      "from awsglue.context import GlueContext\n",
      "from pyspark.context import SparkContext\n",
      "\n",
      "args = getResolvedOptions(sys.argv,\n",
      "                          ['JOB_NAME',\n",
      "                           'SOURCE_DB',\n",
      "                           'SOURCE_TABLE',\n",
      "                           'TARGET_PATH'])\n",
      "\n",
      "sc = SparkContext()\n",
      "glueContext = GlueContext(sc)\n",
      "spark = glueContext.spark_session\n",
      "\n",
      "# 1Ô∏è‚É£ Read from the Data Catalog\n",
      "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
      "    database=args['SOURCE_DB'],\n",
      "    table_name=args['SOURCE_TABLE']\n",
      ")\n",
      "\n",
      "# 2Ô∏è‚É£ Simple transformation: drop nulls & rename a column\n",
      "transformed = datasource \\\n",
      "    .filter(f=lambda x: x[\"price\"] is not None) \\\n",
      "    .rename_field(\"old_name\", \"new_name\")\n",
      "\n",
      "# 3Ô∏è‚É£ Write out as partitioned Parquet\n",
      "glueContext.write_dynamic_frame.from_options(\n",
      "    frame=transformed,\n",
      "    connection_type=\"s3\",\n",
      "    connection_options={\"path\": args['TARGET_PATH'],\n",
      "                        \"partitionKeys\": [\"year\", \"month\"]},\n",
      "    format=\"parquet\"\n",
      ")\n",
      "\n",
      "job.commit()\n",
      "```\n",
      "\n",
      "You‚Äôd submit this script as a **Glue job**, pass the arguments (e.g., `--SOURCE_DB mydb --SOURCE_TABLE raw_logs --TARGET_PATH s3://my‚Äëlake/cleaned/`), and Glue will spin up the Spark executors, run the code, and shut down automatically.\n",
      "\n",
      "---\n",
      "\n",
      "## 8Ô∏è‚É£ Where to Learn More\n",
      "\n",
      "- **AWS Documentation** ‚Äì ‚ÄúAWS Glue Developer Guide‚Äù (covers crawlers, jobs, Studio, and best practices).  \n",
      "- **AWS Glue Workshops** ‚Äì Hands‚Äëon labs in the AWS Training portal.  \n",
      "- **AWS re:Invent videos** ‚Äì Look for ‚ÄúServerless ETL with AWS Glue‚Äù (2023/2024 sessions).  \n",
      "- **GitHub** ‚Äì The `aws-samples/aws-glue-samples` repo has ready‚Äëmade job scripts and notebooks.  \n",
      "\n",
      "---\n",
      "\n",
      "### üé§ Your Turn!\n",
      "Do you have a specific data source you‚Äôre trying to bring into a lake, or a particular transformation you‚Äôre curious about (e.g., handling nested JSON, joining with a RDS table, or using Glue Studio‚Äôs visual editor)? Let me know, and we can sketch out a concrete pipeline together!\n",
      "Human: What is my name? And what did we talk about last time?\n",
      "AI: Hey Fredrick! üëã‚ÄØNice to see you back.\n",
      "\n",
      "**Your name:**‚ÄØFredrick (you told me that right at the start of our chat).\n",
      "\n",
      "**What we talked about last time:**‚ÄØWe dove into **AWS‚ÄØGlue**, Amazon‚Äôs fully‚Äëmanaged, serverless data‚Äëintegration service. I gave you a rundown of its main components (Data Catalog, crawlers, ETL jobs, triggers/workflows, development endpoints, and Glue‚ÄØStudio), explained the high‚Äëlevel workflow (ingest ‚Üí catalog ‚Üí transform ‚Üí load ‚Üí consume), highlighted why teams love it (zero‚Äëops, integrated catalog, auto‚Äëschema inference, pay‚Äëas‚Äëyou‚Äëgo pricing, built‚Äëin security, extensibility, notebooks), listed common use cases (data‚Äëlake ingestion, reporting pipelines, ML feature prep, compliance, event‚Äëdriven pipelines), sketched the pricing model, pointed out a few gotchas, and even shared a quick ‚ÄúHello‚ÄëWorld‚Äù PySpark ETL script.\n",
      "\n",
      "If you‚Äôd like to pick up where we left off‚Äîmaybe you have a specific data source you want to bring into Glue, or you‚Äôre curious about building a workflow with crawlers and triggers‚Äîjust let me know and we can map it out together! üöÄ\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4b88047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi, my name is Fredrick\\nAI: Hello Fredrick! It‚Äôs great to meet you. I‚Äôm an AI language model that loves chatting about all sorts of topics‚Äîwhether you‚Äôre into books, movies, science, cooking, travel, or just want to share a funny story from your day, I‚Äôm all ears (well, metaphorical ears).\\n\\nA little bit about me: I was trained on a diverse mix of texts up through mid‚Äë2024, so I can riff on everything from classic literature to the latest tech trends. I don‚Äôt have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\\n\\nWhat‚Äôs on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation‚Äîmaybe a new sci‚Äëfi novel or a tasty recipe? Let me know, and we can dive right in!\\nHuman: What is AWS Glue?\\nAI: **AWS Glue** is Amazon\\u202fWeb\\u202fServices‚Äô fully‚Äëmanaged, serverless data‚Äëintegration service. In plain English, it‚Äôs a set of tools that help you move, clean, enrich, and catalog data so you can run analytics or machine‚Äëlearning workloads without having to spin up and manage your own ETL (Extract‚ÄëTransform‚ÄëLoad) infrastructure.\\n\\nBelow is a quick‚Äëlook‚Äëaround of the main pieces, why people love it, and a few things to watch out for.\\n\\n---\\n\\n## 1Ô∏è‚É£ Core Components\\n\\n| Component | What it does | Typical use‚Äëcase |\\n|-----------|--------------|------------------|\\n| **Glue Data Catalog** | Central metadata repository (tables, schemas, partitions, locations). It‚Äôs the ‚ÄúHive Metastore‚Äù for the AWS world. | Acts as a single source of truth for Athena, Redshift Spectrum, EMR, and even external tools like Tableau. |\\n| **Crawlers** | Scans data stores (S3, JDBC, DynamoDB, etc.) and automatically infers schema, partitions, and populates the Data Catalog. | You drop a new CSV/JSON/Parquet file into an S3 bucket and a crawler registers it as a table in minutes. |\\n| **ETL Jobs** | Serverless Spark (or Python/Scala) scripts that read from sources, apply transformations, and write to targets. | Typical pipelines: ‚ÄúRead raw logs from S3 ‚Üí parse ‚Üí enrich with lookup table ‚Üí write cleaned data to Parquet in another bucket.‚Äù |\\n| **Triggers & Workflows** | Event‚Äëdriven or schedule‚Äëdriven orchestration (cron, on‚Äëdemand, on‚Äëcompletion of another job). | Build multi‚Äëstep pipelines: e.g., run a crawler ‚Üí start a job ‚Üí notify Slack when done. |\\n| **Development Endpoints** | Interactive notebooks (Jupyter, Zeppelin) that let you prototype ETL code against live data. | Data engineers love the ‚Äúwrite‚Äëonce, test‚Äëmany‚Äù loop before committing a production job. |\\n| **Glue Studio** (visual UI) | Drag‚Äëand‚Äëdrop canvas for building ETL jobs without writing code. | Great for analysts or quick PoCs where you want to see the flow graphically. |\\n\\n---\\n\\n## 2Ô∏è‚É£ How It Works (high‚Äëlevel flow)\\n\\n1. **Ingest** ‚Äì Data lands in a source (commonly S3, but also RDS, DynamoDB, Kafka, etc.).\\n2. **Catalog** ‚Äì A **crawler** runs (or you manually define a table) and registers the dataset in the **Glue Data Catalog**.\\n3. **Transform** ‚Äì A **Glue job** (Spark‚Äëbased) reads the cataloged table, applies transformations (SQL‚Äëlike, Python UDFs, joins, window functions, etc.).\\n4. **Load** ‚Äì The job writes the result to a destination (another S3 location, Redshift, Snowflake via JDBC, etc.).\\n5. **Consume** ‚Äì Downstream services (Athena, Redshift Spectrum, QuickSight, SageMaker) query the cleaned data directly from the catalog.\\n\\nAll of this runs on **serverless Spark**: you never provision EC2 instances or manage a Spark cluster. Glue automatically scales the underlying compute up or down based on the job‚Äôs workload.\\n\\n---\\n\\n## 3Ô∏è‚É£ Why Teams Choose Glue\\n\\n| Benefit | Detail |\\n|---------|--------|\\n| **Zero‚Äëops** | No clusters to patch, no capacity planning. You just pay for the compute seconds you actually use. |\\n| **Integrated catalog** | One place to store schema metadata that many AWS analytics services can read. |\\n| **Auto‚Äëschema inference** | Crawlers can detect column types, partitions, and even nested structures in JSON/Parquet. |\\n| **Pay‚Äëas‚Äëyou‚Äëgo pricing** | Charged per DPU‚Äëhour (Data Processing Unit) for jobs, per crawler run, and per catalog storage (tiny cost). |\\n| **Built‚Äëin security** | IAM policies, VPC endpoints, encryption at rest (S3/KMS) and in transit, and fine‚Äëgrained column‚Äëlevel permissions via Lake Formation. |\\n| **Extensibility** | You can write custom Spark code, use Python libraries, or call external APIs from within a job. |\\n| **Serverless notebooks** | Development endpoints let you prototype with familiar tools (Jupyter). |\\n\\n---\\n\\n## 4Ô∏è‚É£ Typical Use Cases\\n\\n| Scenario | How Glue fits |\\n|----------|---------------|\\n| **Data lake ingestion** | Crawl raw logs ‚Üí register ‚Üí transform ‚Üí store as columnar Parquet for analytics. |\\n| **ETL for reporting** | Pull data from RDS ‚Üí cleanse ‚Üí load into Redshift or Snowflake. |\\n| **Machine‚Äëlearning feature pipelines** | Extract features from S3 ‚Üí join with reference tables ‚Üí write feature store files for SageMaker. |\\n| **Regulatory compliance** | Use crawlers + Lake Formation to enforce column‚Äëlevel masking before data is exposed to analysts. |\\n| **Event‚Äëdriven pipelines** | Trigger a Glue job when a new file lands in an S3 bucket (via S3 event ‚Üí Lambda ‚Üí Glue start‚Äëjob). |\\n\\n---\\n\\n## 5Ô∏è‚É£ Pricing Snapshot (as of 2024)\\n\\n| Resource | Unit | Approx. cost (US‚ÄëEast‚Äë1) |\\n|----------|------|--------------------------|\\n| **Glue Data Catalog storage** | per 1\\u202fM tables/partitions per month | $1.00 |\\n| **Crawler run** | per 1\\u202f000\\u202fseconds of runtime | $0.44 |\\n| **ETL job** | per DPU‚Äëhour (1\\u202fDPU = 4\\u202fvCPU + 16\\u202fGB RAM) | $0.44 (standard) or $0.88 (G.1X) |\\n| **Glue Studio visual jobs** | same as ETL job pricing | ‚Äì |\\n| **Development endpoint** | per DPU‚Äëhour while active | $0.44 |\\n\\n> **Tip:** Because Glue jobs are billed per second (rounded up to the nearest 1\\u202fsecond), you can keep costs low by using **job bookmarks** (process only new data) and **dynamic partition pruning** to avoid scanning whole tables.\\n\\n---\\n\\n## 6Ô∏è‚É£ Things to Keep in Mind\\n\\n| Potential drawback | Mitigation |\\n|--------------------|------------|\\n| **Cold‚Äëstart latency** ‚Äì First run of a job can take ~30‚Äë60\\u202fseconds to spin up Spark executors. | Keep a ‚Äúwarm‚Äù job running on a schedule (e.g., every hour) if you need sub‚Äësecond response. |\\n| **Limited language support** ‚Äì Native support is Scala, Python (PySpark), and Spark SQL. No native Java or .NET. | Use PySpark for most cases; you can still call Java libraries via Py4J if needed. |\\n| **Cost visibility** ‚Äì Large Spark jobs can consume many DPUs quickly. | Enable **job metrics** and CloudWatch alarms on DPU usage; break jobs into smaller stages if possible. |\\n| **Complex transformations** ‚Äì Very custom logic may be easier in a dedicated EMR cluster. | Evaluate whether Glue‚Äôs serverless model fits the complexity; you can still launch an EMR cluster from a Glue workflow if needed. |\\n| **Versioning of libraries** ‚Äì Glue provides a set of pre‚Äëinstalled Spark packages; custom packages must be uploaded as .zip/.whl. | Use **Glue 4.0** (the latest runtime) which supports Python\\u202f3.9 and a broader set of libraries, and test your dependencies locally first. |\\n\\n---\\n\\n## 7Ô∏è‚É£ Quick ‚ÄúHello‚ÄëWorld‚Äù Example (Python/Glue\\u202fETL)\\n\\n```python\\nimport sys\\nfrom awsglue.transforms import *\\nfrom awsglue.utils import getResolvedOptions\\nfrom awsglue.context import GlueContext\\nfrom pyspark.context import SparkContext\\n\\nargs = getResolvedOptions(sys.argv,\\n                          [\\'JOB_NAME\\',\\n                           \\'SOURCE_DB\\',\\n                           \\'SOURCE_TABLE\\',\\n                           \\'TARGET_PATH\\'])\\n\\nsc = SparkContext()\\nglueContext = GlueContext(sc)\\nspark = glueContext.spark_session\\n\\n# 1Ô∏è‚É£ Read from the Data Catalog\\ndatasource = glueContext.create_dynamic_frame.from_catalog(\\n    database=args[\\'SOURCE_DB\\'],\\n    table_name=args[\\'SOURCE_TABLE\\']\\n)\\n\\n# 2Ô∏è‚É£ Simple transformation: drop nulls & rename a column\\ntransformed = datasource \\\\\\n    .filter(f=lambda x: x[\"price\"] is not None) \\\\\\n    .rename_field(\"old_name\", \"new_name\")\\n\\n# 3Ô∏è‚É£ Write out as partitioned Parquet\\nglueContext.write_dynamic_frame.from_options(\\n    frame=transformed,\\n    connection_type=\"s3\",\\n    connection_options={\"path\": args[\\'TARGET_PATH\\'],\\n                        \"partitionKeys\": [\"year\", \"month\"]},\\n    format=\"parquet\"\\n)\\n\\njob.commit()\\n```\\n\\nYou‚Äôd submit this script as a **Glue job**, pass the arguments (e.g., `--SOURCE_DB mydb --SOURCE_TABLE raw_logs --TARGET_PATH s3://my‚Äëlake/cleaned/`), and Glue will spin up the Spark executors, run the code, and shut down automatically.\\n\\n---\\n\\n## 8Ô∏è‚É£ Where to Learn More\\n\\n- **AWS Documentation** ‚Äì ‚ÄúAWS Glue Developer Guide‚Äù (covers crawlers, jobs, Studio, and best practices).  \\n- **AWS Glue Workshops** ‚Äì Hands‚Äëon labs in the AWS Training portal.  \\n- **AWS re:Invent videos** ‚Äì Look for ‚ÄúServerless ETL with AWS Glue‚Äù (2023/2024 sessions).  \\n- **GitHub** ‚Äì The `aws-samples/aws-glue-samples` repo has ready‚Äëmade job scripts and notebooks.  \\n\\n---\\n\\n### üé§ Your Turn!\\nDo you have a specific data source you‚Äôre trying to bring into a lake, or a particular transformation you‚Äôre curious about (e.g., handling nested JSON, joining with a RDS table, or using Glue Studio‚Äôs visual editor)? Let me know, and we can sketch out a concrete pipeline together!\\nHuman: What is my name? And what did we talk about last time?\\nAI: Hey Fredrick! üëã\\u202fNice to see you back.\\n\\n**Your name:**\\u202fFredrick (you told me that right at the start of our chat).\\n\\n**What we talked about last time:**\\u202fWe dove into **AWS\\u202fGlue**, Amazon‚Äôs fully‚Äëmanaged, serverless data‚Äëintegration service. I gave you a rundown of its main components (Data Catalog, crawlers, ETL jobs, triggers/workflows, development endpoints, and Glue\\u202fStudio), explained the high‚Äëlevel workflow (ingest ‚Üí catalog ‚Üí transform ‚Üí load ‚Üí consume), highlighted why teams love it (zero‚Äëops, integrated catalog, auto‚Äëschema inference, pay‚Äëas‚Äëyou‚Äëgo pricing, built‚Äëin security, extensibility, notebooks), listed common use cases (data‚Äëlake ingestion, reporting pipelines, ML feature prep, compliance, event‚Äëdriven pipelines), sketched the pricing model, pointed out a few gotchas, and even shared a quick ‚ÄúHello‚ÄëWorld‚Äù PySpark ETL script.\\n\\nIf you‚Äôd like to pick up where we left off‚Äîmaybe you have a specific data source you want to bring into Glue, or you‚Äôre curious about building a workflow with crawlers and triggers‚Äîjust let me know and we can map it out together! üöÄ'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e5d2758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up?\"}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "print(memory.buffer)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f0df6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi\\nAI: What's up?\\nHuman: Not much, just hanging\\nAI: Cool\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7d4c6",
   "metadata": {},
   "source": [
    "**Different kinds of memory are available that can store and accumulate the conversation - ConversationBufferWindowMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7b3278d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)      # For k pairs(Human-AI) of the last conversation\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Whats up\"})\n",
    "memory.save_context({\"input\": \"not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ad2b992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Fredrick\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Fredrick! It‚Äôs great to meet you. I‚Äôm an AI language model that loves chatting about all sorts of topics‚Äîwhether you‚Äôre into books, movies, science, cooking, travel, or just want to share a funny story from your day, I‚Äôm all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‚Äë2024, so I can riff on everything from classic literature to the latest tech trends. I don‚Äôt have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What‚Äôs on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation‚Äîmaybe a new sci‚Äëfi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Fredrick\n",
      "AI: Hello Fredrick! It‚Äôs great to meet you. I‚Äôm an AI language model that loves chatting about all sorts of topics‚Äîwhether you‚Äôre into books, movies, science, cooking, travel, or just want to share a funny story from your day, I‚Äôm all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‚Äë2024, so I can riff on everything from classic literature to the latest tech trends. I don‚Äôt have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What‚Äôs on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation‚Äîmaybe a new sci‚Äëfi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "Human: What is AWS Bedrock?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**AWS‚ÄØBedrock** is Amazon‚ÄØWeb‚ÄØServices‚Äô fully‚Äëmanaged platform for building and running **generative‚ÄëAI applications** with large foundation models (LLMs, diffusion models, embedding models, etc.)‚ÄØ‚Äî‚ÄØall without having to spin up your own GPU clusters or manage the underlying infrastructure.\n",
      "\n",
      "---\n",
      "\n",
      "## The big picture\n",
      "\n",
      "| Piece | What it means for you |\n",
      "|------|------------------------|\n",
      "| **Managed service** | AWS runs the hardware, scaling, patching, and security. You just call an API. |\n",
      "| **Foundation‚Äëmodel hub** | A catalog of pre‚Äëtrained models from both AWS (e.g., **Titan** series) and leading AI startups (Anthropic, AI21, Stability AI, Cohere). |\n",
      "| **Pay‚Äëas‚Äëyou‚Äëgo** | You‚Äôre billed per token (for text) or per image generated, plus a small per‚Äërequest fee for the API. No upfront commitments. |\n",
      "| **Fine‚Äëtuning & customisation** | You can create a ‚Äúpersonalised‚Äù version of a model by feeding it your own data (via a simple UI or API). |\n",
      "| **Secure & compliant** | Data never leaves your VPC unless you explicitly allow it; encryption at rest/in‚Äëflight, IAM‚Äëbased access control, and integration with AWS‚ÄØKMS, CloudTrail, etc. |\n",
      "| **Integrated with the rest of AWS** | Easy to pipe Bedrock output into Lambda, SageMaker, Step Functions, DynamoDB, S3, or even a front‚Äëend hosted on Amplify. |\n",
      "\n",
      "---\n",
      "\n",
      "## How it works (step‚Äëby‚Äëstep)\n",
      "\n",
      "1. **Choose a model** ‚Äì In the console you‚Äôll see a list like:  \n",
      "   - **Anthropic Claude‚Äë2** (chat‚Äëstyle, strong reasoning)  \n",
      "   - **AI21 Jurassic‚Äë2** (high‚Äëcapacity text generation)  \n",
      "   - **Stability Diffusion** (image generation)  \n",
      "   - **Amazon Titan** (AWS‚Äëbuilt LLMs, both text‚Äëgeneration and embeddings)  \n",
      "\n",
      "2. **Create an ‚Äúmodel endpoint‚Äù** ‚Äì This is a lightweight, serverless endpoint that automatically scales. You can set a **provisioned concurrency** limit if you need guaranteed latency.\n",
      "\n",
      "3. **Call the API** ‚Äì A simple HTTPS POST with your prompt (or image‚Äëto‚Äëimage request) returns the model‚Äôs output. The request format is similar to OpenAI‚Äôs `chat/completions` endpoint, so many existing SDKs can be reused.\n",
      "\n",
      "4. **(Optional) Fine‚Äëtune** ‚Äì Upload a CSV/JSONL of prompt‚Äëcompletion pairs to an S3 bucket, point Bedrock to it, and launch a fine‚Äëtuning job. The resulting model is stored as a new version you can call just like any other endpoint.\n",
      "\n",
      "5. **Consume the result** ‚Äì Pipe the response into downstream services: store in DynamoDB, trigger a Step Functions workflow, or send back to a web UI.\n",
      "\n",
      "---\n",
      "\n",
      "## Why people love it\n",
      "\n",
      "| Use case | Example |\n",
      "|----------|---------|\n",
      "| **Customer‚Äësupport bots** | Use Claude‚Äë2 for conversational handling, then route to a Lambda that looks up order info in Aurora. |\n",
      "| **Content creation** | Generate marketing copy with Titan‚ÄëText, then run it through a Grammarly‚Äëstyle quality‚Äëcheck Lambda. |\n",
      "| **Code assistance** | Use Titan‚ÄëCode (a code‚Äëfocused LLM) to autocomplete snippets inside an internal IDE plugin. |\n",
      "| **Image generation** | Produce product mock‚Äëups on‚Äëthe‚Äëfly with Stability Diffusion, store the PNGs in S3, and serve them via CloudFront. |\n",
      "| **Semantic search** | Create embeddings with Titan‚ÄëEmbedding, index them in OpenSearch, and power a ‚Äúsearch‚Äëby‚Äëmeaning‚Äù feature for a knowledge base. |\n",
      "\n",
      "---\n",
      "\n",
      "## Pricing snapshot (as of 2024‚ÄëQ3)\n",
      "\n",
      "| Model family | Cost per 1‚ÄØK‚ÄØtokens (input) | Cost per 1‚ÄØK‚ÄØtokens (output) |\n",
      "|--------------|----------------------------|------------------------------|\n",
      "| **Titan‚ÄëText** | $0.0004 | $0.0012 |\n",
      "| **Claude‚Äë2** | $0.0010 | $0.0030 |\n",
      "| **Jurassic‚Äë2** | $0.0015 | $0.0040 |\n",
      "| **Stability Diffusion** | $0.015 per 1‚ÄØK‚ÄØpixel‚Äësteps (‚âà $0.12 per 512√ó512 image) | ‚Äî |\n",
      "\n",
      "*Note:* There‚Äôs also a **$0.0001 per request** ‚Äúinfrastructure fee‚Äù for the endpoint call itself. Prices can vary by region and are subject to change, so always double‚Äëcheck the AWS pricing page.\n",
      "\n",
      "---\n",
      "\n",
      "## Security & compliance highlights\n",
      "\n",
      "- **VPC‚Äëendpoint support** ‚Äì Keep traffic inside your private network.  \n",
      "- **IAM policies** ‚Äì Fine‚Äëgrained permissions (e.g., `bedrock:InvokeModel`, `bedrock:CreateModelCustomizationJob`).  \n",
      "- **Data residency** ‚Äì Choose a region (e.g., us‚Äëeast‚Äë1, eu‚Äëcentral‚Äë1) and the model runs entirely there.  \n",
      "- **Auditability** ‚Äì All calls are logged to CloudTrail; you can enable request‚Äëlevel logging for debugging.  \n",
      "- **Compliance** ‚Äì Bedrock inherits AWS‚Äôs certifications (SOC‚ÄØ2, ISO‚ÄØ27001, GDPR, HIPAA‚Äëeligible in certain regions).\n",
      "\n",
      "---\n",
      "\n",
      "## Quick ‚ÄúHello‚ÄëWorld‚Äù code snippet (Python)\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "import json\n",
      "\n",
      "# 1Ô∏è‚É£ Create a Bedrock client (make sure your IAM role has bedrock:InvokeModel)\n",
      "client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
      "\n",
      "# 2Ô∏è‚É£ Choose a model ‚Äì here we use Titan Text\n",
      "model_id = \"amazon.titan-text-premier-v1:0\"\n",
      "\n",
      "# 3Ô∏è‚É£ Build the request payload\n",
      "prompt = \"Write a short, witty tagline for a coffee shop called 'Bean There, Done That'.\"\n",
      "payload = {\n",
      "    \"inputText\": prompt,\n",
      "    \"textGenerationConfig\": {\n",
      "        \"maxTokenCount\": 64,\n",
      "        \"temperature\": 0.7,\n",
      "        \"topP\": 0.9\n",
      "    }\n",
      "}\n",
      "\n",
      "# 4Ô∏è‚É£ Call the model\n",
      "response = client.invoke_model(\n",
      "    body=json.dumps(payload),\n",
      "    modelId=model_id,\n",
      "    contentType=\"application/json\"\n",
      ")\n",
      "\n",
      "# 5Ô∏è‚É£ Parse the result\n",
      "result = json.loads(response[\"body\"].read())\n",
      "print(\"Generated tagline:\", result[\"results\"][0][\"outputText\"])\n",
      "```\n",
      "\n",
      "Running that script (with proper AWS credentials) will give you something like:\n",
      "\n",
      "```\n",
      "Generated tagline: ‚ÄúFuel your day, one bold sip at a time.‚Äù\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Getting started checklist\n",
      "\n",
      "1. **Create an AWS account** (or use an existing one).  \n",
      "2. **Enable Bedrock** in the console ‚Äì you may need to request access if you‚Äôre in a new region.  \n",
      "3. **Set up IAM** ‚Äì give yourself `bedrock:*` permissions or a tighter policy.  \n",
      "4. **Pick a model** and spin up an endpoint (the console wizard walks you through it).  \n",
      "5. **Try the sample code** (the SDK is in `boto3` for Python, `aws-sdk` for JavaScript/TypeScript, etc.).  \n",
      "6. **Monitor** ‚Äì use CloudWatch metrics (`ModelInvocationCount`, `ModelLatency`) to keep an eye on cost and performance.  \n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "AWS‚ÄØBedrock = **a serverless, secure, pay‚Äëas‚Äëyou‚Äëgo gateway to the world‚Äôs biggest foundation models**, all baked into the AWS ecosystem. It lets you generate text, images, embeddings, or even fine‚Äëtune a model with just a few clicks or a single API call, while AWS handles the heavy lifting of scaling, security, and compliance.\n",
      "\n",
      "If you have a specific project in mind‚Äîchatbot, content generator, image creator, semantic search, or anything else‚Äîjust let me know and we can sketch out a concrete architecture using Bedrock! \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is AWS Bedrock?\n",
      "AI: **AWS‚ÄØBedrock** is Amazon‚ÄØWeb‚ÄØServices‚Äô fully‚Äëmanaged platform for building and running **generative‚ÄëAI applications** with large foundation models (LLMs, diffusion models, embedding models, etc.)‚ÄØ‚Äî‚ÄØall without having to spin up your own GPU clusters or manage the underlying infrastructure.\n",
      "\n",
      "---\n",
      "\n",
      "## The big picture\n",
      "\n",
      "| Piece | What it means for you |\n",
      "|------|------------------------|\n",
      "| **Managed service** | AWS runs the hardware, scaling, patching, and security. You just call an API. |\n",
      "| **Foundation‚Äëmodel hub** | A catalog of pre‚Äëtrained models from both AWS (e.g., **Titan** series) and leading AI startups (Anthropic, AI21, Stability AI, Cohere). |\n",
      "| **Pay‚Äëas‚Äëyou‚Äëgo** | You‚Äôre billed per token (for text) or per image generated, plus a small per‚Äërequest fee for the API. No upfront commitments. |\n",
      "| **Fine‚Äëtuning & customisation** | You can create a ‚Äúpersonalised‚Äù version of a model by feeding it your own data (via a simple UI or API). |\n",
      "| **Secure & compliant** | Data never leaves your VPC unless you explicitly allow it; encryption at rest/in‚Äëflight, IAM‚Äëbased access control, and integration with AWS‚ÄØKMS, CloudTrail, etc. |\n",
      "| **Integrated with the rest of AWS** | Easy to pipe Bedrock output into Lambda, SageMaker, Step Functions, DynamoDB, S3, or even a front‚Äëend hosted on Amplify. |\n",
      "\n",
      "---\n",
      "\n",
      "## How it works (step‚Äëby‚Äëstep)\n",
      "\n",
      "1. **Choose a model** ‚Äì In the console you‚Äôll see a list like:  \n",
      "   - **Anthropic Claude‚Äë2** (chat‚Äëstyle, strong reasoning)  \n",
      "   - **AI21 Jurassic‚Äë2** (high‚Äëcapacity text generation)  \n",
      "   - **Stability Diffusion** (image generation)  \n",
      "   - **Amazon Titan** (AWS‚Äëbuilt LLMs, both text‚Äëgeneration and embeddings)  \n",
      "\n",
      "2. **Create an ‚Äúmodel endpoint‚Äù** ‚Äì This is a lightweight, serverless endpoint that automatically scales. You can set a **provisioned concurrency** limit if you need guaranteed latency.\n",
      "\n",
      "3. **Call the API** ‚Äì A simple HTTPS POST with your prompt (or image‚Äëto‚Äëimage request) returns the model‚Äôs output. The request format is similar to OpenAI‚Äôs `chat/completions` endpoint, so many existing SDKs can be reused.\n",
      "\n",
      "4. **(Optional) Fine‚Äëtune** ‚Äì Upload a CSV/JSONL of prompt‚Äëcompletion pairs to an S3 bucket, point Bedrock to it, and launch a fine‚Äëtuning job. The resulting model is stored as a new version you can call just like any other endpoint.\n",
      "\n",
      "5. **Consume the result** ‚Äì Pipe the response into downstream services: store in DynamoDB, trigger a Step Functions workflow, or send back to a web UI.\n",
      "\n",
      "---\n",
      "\n",
      "## Why people love it\n",
      "\n",
      "| Use case | Example |\n",
      "|----------|---------|\n",
      "| **Customer‚Äësupport bots** | Use Claude‚Äë2 for conversational handling, then route to a Lambda that looks up order info in Aurora. |\n",
      "| **Content creation** | Generate marketing copy with Titan‚ÄëText, then run it through a Grammarly‚Äëstyle quality‚Äëcheck Lambda. |\n",
      "| **Code assistance** | Use Titan‚ÄëCode (a code‚Äëfocused LLM) to autocomplete snippets inside an internal IDE plugin. |\n",
      "| **Image generation** | Produce product mock‚Äëups on‚Äëthe‚Äëfly with Stability Diffusion, store the PNGs in S3, and serve them via CloudFront. |\n",
      "| **Semantic search** | Create embeddings with Titan‚ÄëEmbedding, index them in OpenSearch, and power a ‚Äúsearch‚Äëby‚Äëmeaning‚Äù feature for a knowledge base. |\n",
      "\n",
      "---\n",
      "\n",
      "## Pricing snapshot (as of 2024‚ÄëQ3)\n",
      "\n",
      "| Model family | Cost per 1‚ÄØK‚ÄØtokens (input) | Cost per 1‚ÄØK‚ÄØtokens (output) |\n",
      "|--------------|----------------------------|------------------------------|\n",
      "| **Titan‚ÄëText** | $0.0004 | $0.0012 |\n",
      "| **Claude‚Äë2** | $0.0010 | $0.0030 |\n",
      "| **Jurassic‚Äë2** | $0.0015 | $0.0040 |\n",
      "| **Stability Diffusion** | $0.015 per 1‚ÄØK‚ÄØpixel‚Äësteps (‚âà $0.12 per 512√ó512 image) | ‚Äî |\n",
      "\n",
      "*Note:* There‚Äôs also a **$0.0001 per request** ‚Äúinfrastructure fee‚Äù for the endpoint call itself. Prices can vary by region and are subject to change, so always double‚Äëcheck the AWS pricing page.\n",
      "\n",
      "---\n",
      "\n",
      "## Security & compliance highlights\n",
      "\n",
      "- **VPC‚Äëendpoint support** ‚Äì Keep traffic inside your private network.  \n",
      "- **IAM policies** ‚Äì Fine‚Äëgrained permissions (e.g., `bedrock:InvokeModel`, `bedrock:CreateModelCustomizationJob`).  \n",
      "- **Data residency** ‚Äì Choose a region (e.g., us‚Äëeast‚Äë1, eu‚Äëcentral‚Äë1) and the model runs entirely there.  \n",
      "- **Auditability** ‚Äì All calls are logged to CloudTrail; you can enable request‚Äëlevel logging for debugging.  \n",
      "- **Compliance** ‚Äì Bedrock inherits AWS‚Äôs certifications (SOC‚ÄØ2, ISO‚ÄØ27001, GDPR, HIPAA‚Äëeligible in certain regions).\n",
      "\n",
      "---\n",
      "\n",
      "## Quick ‚ÄúHello‚ÄëWorld‚Äù code snippet (Python)\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "import json\n",
      "\n",
      "# 1Ô∏è‚É£ Create a Bedrock client (make sure your IAM role has bedrock:InvokeModel)\n",
      "client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
      "\n",
      "# 2Ô∏è‚É£ Choose a model ‚Äì here we use Titan Text\n",
      "model_id = \"amazon.titan-text-premier-v1:0\"\n",
      "\n",
      "# 3Ô∏è‚É£ Build the request payload\n",
      "prompt = \"Write a short, witty tagline for a coffee shop called 'Bean There, Done That'.\"\n",
      "payload = {\n",
      "    \"inputText\": prompt,\n",
      "    \"textGenerationConfig\": {\n",
      "        \"maxTokenCount\": 64,\n",
      "        \"temperature\": 0.7,\n",
      "        \"topP\": 0.9\n",
      "    }\n",
      "}\n",
      "\n",
      "# 4Ô∏è‚É£ Call the model\n",
      "response = client.invoke_model(\n",
      "    body=json.dumps(payload),\n",
      "    modelId=model_id,\n",
      "    contentType=\"application/json\"\n",
      ")\n",
      "\n",
      "# 5Ô∏è‚É£ Parse the result\n",
      "result = json.loads(response[\"body\"].read())\n",
      "print(\"Generated tagline:\", result[\"results\"][0][\"outputText\"])\n",
      "```\n",
      "\n",
      "Running that script (with proper AWS credentials) will give you something like:\n",
      "\n",
      "```\n",
      "Generated tagline: ‚ÄúFuel your day, one bold sip at a time.‚Äù\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Getting started checklist\n",
      "\n",
      "1. **Create an AWS account** (or use an existing one).  \n",
      "2. **Enable Bedrock** in the console ‚Äì you may need to request access if you‚Äôre in a new region.  \n",
      "3. **Set up IAM** ‚Äì give yourself `bedrock:*` permissions or a tighter policy.  \n",
      "4. **Pick a model** and spin up an endpoint (the console wizard walks you through it).  \n",
      "5. **Try the sample code** (the SDK is in `boto3` for Python, `aws-sdk` for JavaScript/TypeScript, etc.).  \n",
      "6. **Monitor** ‚Äì use CloudWatch metrics (`ModelInvocationCount`, `ModelLatency`) to keep an eye on cost and performance.  \n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "AWS‚ÄØBedrock = **a serverless, secure, pay‚Äëas‚Äëyou‚Äëgo gateway to the world‚Äôs biggest foundation models**, all baked into the AWS ecosystem. It lets you generate text, images, embeddings, or even fine‚Äëtune a model with just a few clicks or a single API call, while AWS handles the heavy lifting of scaling, security, and compliance.\n",
      "\n",
      "If you have a specific project in mind‚Äîchatbot, content generator, image creator, semantic search, or anything else‚Äîjust let me know and we can sketch out a concrete architecture using Bedrock!\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I‚Äôm sorry, but I don‚Äôt have any information about your name.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(temperature=0.0, model=\"openai/gpt-oss-120b\")\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(conversation.predict(input=\"Hi, my name is Fredrick\"))\n",
    "print(conversation.predict(input=\"What is AWS Bedrock?\"), \"\\n\")\n",
    "\n",
    "print(conversation.predict(input=\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3188f",
   "metadata": {},
   "source": [
    "**ConversationTokenBufferMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "436dba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_groq.chat_models import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffb1126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0.0)\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?\"}, {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5587a3e",
   "metadata": {},
   "source": [
    "**ConversationSummaryBufferMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d40eed34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human greets the AI and says they‚Äôre just hanging around; the AI responds casually. The human then asks about today‚Äôs schedule, and the AI details a 8‚ÄØa.m. meeting with the product team (requiring a PowerPoint), work on ONNX, AWS‚ÄØBedrock, and Glue projects from 9‚ÄØa.m. to noon, followed by a noon lunch at a Russian restaurant with a distant customer to discuss the latest AI, reminding the human to bring a laptop for an ONNX demo.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "schedule = \"There is a meeting at 8am with your product team. You will need your powerpoint presentation perpared. \\\n",
    "9am-12pm have time to work on your ONNX Project, AWS Bedrock and Glue projects which will go quickly because these are such powerful tools. \\\n",
    "At Noon, lunch at the Russian restaurant with a customer who is driving from over an hour away to meet you to \\\n",
    "understand the latest in AI. Be sure to bring your laptop to show the latest ONNX demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Whats up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})\n",
    "\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24636900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and says they‚Äôre just hanging around; the AI responds casually. The human then asks about today‚Äôs schedule, and the AI details a 8‚ÄØa.m. meeting with the product team (requiring a PowerPoint), work on ONNX, AWS‚ÄØBedrock, and Glue projects from 9‚ÄØa.m. to noon, followed by a noon lunch at a Russian restaurant with a distant customer to discuss the latest AI, reminding the human to bring a laptop for an ONNX demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hey there!‚ÄØSince you‚Äôve got a packed morning and a lunch‚Äëtime demo in front of a customer, I‚Äôd suggest a **‚ÄúOne‚ÄëClick End‚Äëto‚ÄëEnd AI Pipeline‚Äù** that ties together everything on your agenda:\n",
      "\n",
      "---\n",
      "\n",
      "## üéØ Goal of the Demo  \n",
      "Show how a **PyTorch model ‚Üí ONNX export ‚Üí AWS Bedrock inference ‚Üí AWS Glue data‚Äëprep** pipeline can be built, deployed, and run in under 5‚ÄØminutes on a laptop.  \n",
      "It hits three sweet spots for the customer:\n",
      "\n",
      "1. **Portability** ‚Äì the ONNX model runs locally (no cloud‚Äëdependency for the first run).  \n",
      "2. **Scalability** ‚Äì the same model can be swapped to Bedrock for production‚Äëgrade latency and cost‚Äëtracking.  \n",
      "3. **Data hygiene** ‚Äì Glue demonstrates how you‚Äôd keep the training data fresh and versioned.\n",
      "\n",
      "---\n",
      "\n",
      "## üìã Demo Outline (‚âà‚ÄØ5‚ÄØmin)\n",
      "\n",
      "| Time | What you‚Äôll show | Key Talking Points |\n",
      "|------|------------------|--------------------|\n",
      "| **0:00‚Äë0:30** | **Slide 1 (quick 30‚Äësec PowerPoint)** ‚Äì ‚ÄúFrom prototype to production in 3 steps‚Äù | Mention the 8‚ÄØa.m. product‚Äëteam meeting and how today‚Äôs demo is the concrete example. |\n",
      "| **0:30‚Äë1:30** | **Step‚ÄØ1 ‚Äì Load & run an ONNX model locally** (image classification) | ‚Ä¢ Show the model file (`resnet50.onnx`). <br>‚Ä¢ Run a single inference with `onnxruntime`. <br>‚Ä¢ Highlight <‚ÄØ10‚ÄØms latency on a laptop GPU/CPU. |\n",
      "| **1:30‚Äë2:30** | **Step‚ÄØ2 ‚Äì Push the same model to AWS Bedrock** (via SageMaker‚ÄØModel Registry) | ‚Ä¢ One‚Äëliner `boto3` call to register the ONNX artifact. <br>‚Ä¢ Invoke Bedrock‚Äôs `InvokeModel` API and compare latency (‚âà‚ÄØ30‚ÄØms). <br>‚Ä¢ Show cost‚Äëestimate panel (Bedrock pricing). |\n",
      "| **2:30‚Äë3:30** | **Step‚ÄØ3 ‚Äì Glue ETL to prep new images** | ‚Ä¢ Quick Glue job script (Python shell) that pulls new images from an S3 bucket, resizes them, and writes a manifest file. <br>‚Ä¢ Explain how Glue can keep the training set fresh without touching the model code. |\n",
      "| **3:30‚Äë4:30** | **Live ‚ÄúWhat‚ÄëIf‚Äù** ‚Äì Drop a new image into S3, re‚Äërun the Glue job, and hit Bedrock again | ‚Ä¢ Demonstrates the **closed‚Äëloop**: data ‚Üí ETL ‚Üí inference. |\n",
      "| **4:30‚Äë5:00** | **Wrap‚Äëup slide** ‚Äì Benefits, next steps, Q&A | ‚Ä¢ ‚ÄúZero‚Äëcode migration from edge to cloud‚Äù, ‚ÄúUnified monitoring via CloudWatch‚Äù, ‚ÄúFuture: fine‚Äëtune on Bedrock‚Äù. |\n",
      "\n",
      "---\n",
      "\n",
      "## üõ†Ô∏è What You Need on the Laptop\n",
      "\n",
      "| Component | Version (tested) | Install Command |\n",
      "|-----------|------------------|-----------------|\n",
      "| **Python** | 3.10+ | `python -m venv demo_env && source demo_env/bin/activate` |\n",
      "| **onnxruntime** | 1.18.0 | `pip install onnxruntime` |\n",
      "| **boto3** | 1.34.0 | `pip install boto3` |\n",
      "| **Pillow** (image handling) | 10.2.0 | `pip install pillow` |\n",
      "| **Streamlit** (optional UI) | 1.38.0 | `pip install streamlit` |\n",
      "| **AWS CLI** (configured with your IAM role) | latest | `pip install awscli && aws configure` |\n",
      "\n",
      "> **Tip:** Run the whole script once tonight (or during a coffee break) to make sure the Bedrock endpoint is reachable from the restaurant‚Äôs Wi‚ÄëFi. If the network is flaky, you can fall back to the pure‚ÄëONNX local inference ‚Äì it still looks impressive!\n",
      "\n",
      "---\n",
      "\n",
      "## üìÇ Minimal Working Script (Python)\n",
      "\n",
      "```python\n",
      "import os\n",
      "import json\n",
      "import boto3\n",
      "import onnxruntime as ort\n",
      "from PIL import Image\n",
      "import numpy as np\n",
      "\n",
      "# ---------- 1Ô∏è‚É£ Load ONNX locally ----------\n",
      "def preprocess(img_path):\n",
      "    img = Image.open(img_path).resize((224, 224)).convert(\"RGB\")\n",
      "    arr = np.array(img).astype('float32')\n",
      "    # Normalise like ImageNet\n",
      "    arr = (arr / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
      "    arr = arr.transpose(2, 0, 1)  # C,H,W\n",
      "    return arr[np.newaxis, ...]\n",
      "\n",
      "session = ort.InferenceSession(\"resnet50.onnx\")\n",
      "def infer_local(img_path):\n",
      "    input_name = session.get_inputs()[0].name\n",
      "    out = session.run(None, {input_name: preprocess(img_path)})[0]\n",
      "    return np.argmax(out, axis=1)[0]\n",
      "\n",
      "# ---------- 2Ô∏è‚É£ Bedrock call ----------\n",
      "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
      "def infer_bedrock(img_path):\n",
      "    # Convert image to base64 for payload\n",
      "    with open(img_path, \"rb\") as f:\n",
      "        img_bytes = f.read()\n",
      "    payload = {\n",
      "        \"inputs\": [{\"image\": {\"bytes\": img_bytes.decode('latin1')}}],\n",
      "        \"parameters\": {}\n",
      "    }\n",
      "    response = bedrock.invoke_model(\n",
      "        body=json.dumps(payload),\n",
      "        modelId=\"amazon.titan-image-generator-v1\",   # replace with your ONNX‚Äëregistered model ARN\n",
      "        contentType=\"application/json\"\n",
      "    )\n",
      "    result = json.loads(response[\"body\"].read())\n",
      "    return result[\"predictions\"][0][\"label\"]\n",
      "\n",
      "# ---------- 3Ô∏è‚É£ Glue job (pseudo‚Äëcode) ----------\n",
      "# In the real job you‚Äôd use AWS Glue ETL libraries; here‚Äôs a quick illustration:\n",
      "def glue_prep():\n",
      "    import boto3\n",
      "    s3 = boto3.client(\"s3\")\n",
      "    # List new objects in bucket `demo-input/`\n",
      "    objs = s3.list_objects_v2(Bucket=\"my-demo-bucket\", Prefix=\"demo-input/\")[\"Contents\"]\n",
      "    for obj in objs:\n",
      "        # download, resize, write to `demo-prepped/`\n",
      "        # (omitted for brevity ‚Äì you can use the same `preprocess` function)\n",
      "        pass\n",
      "    print(\"Glue ETL finished ‚Äì manifest ready for next inference.\")\n",
      "\n",
      "# ---------- Demo driver ----------\n",
      "if __name__ == \"__main__\":\n",
      "    test_img = \"sample.jpg\"   # bring this on a USB stick or have it in the repo\n",
      "    print(\"üîπ Local ONNX prediction:\", infer_local(test_img))\n",
      "    print(\"üîπ Bedrock prediction:\", infer_bedrock(test_img))\n",
      "    # Uncomment to run the Glue stub\n",
      "    # glue_prep()\n",
      "```\n",
      "\n",
      "*You can spin this up in a **Streamlit** app in <‚ÄØ2‚ÄØminutes if you want a clickable UI for the lunch demo.*\n",
      "\n",
      "---\n",
      "\n",
      "## üçΩÔ∏è Lunch‚ÄëRoom Logistics\n",
      "\n",
      "| Item | Why it matters |\n",
      "|------|----------------|\n",
      "| **Laptop + charger** | The restaurant‚Äôs power strips are often hidden behind the bar. |\n",
      "| **USB‚ÄëC to HDMI adapter** (or a portable 13‚Äëinch monitor) | Gives the customer a bigger view of the inference latency chart. |\n",
      "| **Printed cheat‚Äësheet** (A5) | Quick reference for the three steps; looks professional. |\n",
      "| **Sample images** (a few cats, a car, a Russian‚Äëcuisine dish) | Ties the demo to the restaurant vibe ‚Äì ‚ÄúHere‚Äôs how the model sees your borscht!‚Äù |\n",
      "| **Headphones (optional)** | If the restaurant gets noisy, you can mute the audio from the Streamlit UI. |\n",
      "\n",
      "---\n",
      "\n",
      "## üìä What to Highlight to the Customer\n",
      "\n",
      "1. **Speed** ‚Äì Local ONNX: ~8‚ÄØms; Bedrock (single‚Äëregion): ~30‚ÄØms.  \n",
      "2. **Cost** ‚Äì Bedrock inference is billed per 1‚ÄØk tokens (or per request for vision models); show a quick cost calculator slide.  \n",
      "3. **Flexibility** ‚Äì Same `.onnx` file works everywhere ‚Äì edge device, SageMaker, Bedrock, or even Azure.  \n",
      "4. **Data freshness** ‚Äì Glue can run on a schedule (e.g., every hour) to ingest new images, so the model never gets stale.  \n",
      "5. **Future‚Äëproof** ‚Äì You can fine‚Äëtune the Bedrock‚Äëhosted model with your own data without moving the ONNX file again.\n",
      "\n",
      "---\n",
      "\n",
      "## üöÄ Quick ‚ÄúNight‚ÄëBefore‚Äù Checklist\n",
      "\n",
      "1. **Run the script end‚Äëto‚Äëend** (local ‚Üí Bedrock).  \n",
      "2. **Take a screenshot** of the latency numbers and drop it into the PowerPoint slide you‚Äôll use at 8‚ÄØa.m.  \n",
      "3. **Copy `sample.jpg`** to a USB stick (or keep it in the repo).  \n",
      "4. **Charge the laptop** to 100‚ÄØ% and pack the charger.  \n",
      "5. **Print the cheat‚Äësheet** (or have it as a PDF on your phone).  \n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "> **Demo:** ‚ÄúOne‚ÄëClick End‚Äëto‚ÄëEnd AI Pipeline‚Äù ‚Äì load a pre‚Äëexported ResNet‚Äë50 ONNX model, run it locally, then invoke the same model on AWS Bedrock, and finally show a Glue ETL job that prepares fresh data for the next inference.  \n",
      "> **Why it works:** It‚Äôs visual, fast, and demonstrates the exact workflow you‚Äôll be talking about in the 8‚ÄØa.m. meeting and the lunch conversation.  \n",
      "\n",
      "Good luck at the Russian restaurant ‚Äì bring that laptop, a smile, and a few tasty images, and you‚Äôll have a demo that‚Äôs both technically solid and memorable! üéâ If you need a deeper dive into any of the code snippets or want a ready‚Äëmade Streamlit UI, just let me know and I‚Äôll send it over.\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "print(conversation.predict(input=\"What would be a good demo to show?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "405e8cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human greets the AI and says they‚Äôre just hanging '\n",
      "            'around; the AI responds casually. The human then asks about '\n",
      "            'today‚Äôs schedule, and the AI details a morning that starts with '\n",
      "            'an 8\\u202fa.m. product‚Äëteam meeting (requiring a PowerPoint), '\n",
      "            'followed by work on ONNX, AWS\\u202fBedrock, and Glue projects '\n",
      "            'from 9\\u202fa.m. to noon, and then a noon lunch at a Russian '\n",
      "            'restaurant with a distant customer to discuss the latest AI, '\n",
      "            'reminding the human to bring a laptop for an ONNX demo. When the '\n",
      "            'human asks what demo would be good, the AI proposes a '\n",
      "            '**‚ÄúOne‚ÄëClick End‚Äëto‚ÄëEnd AI Pipeline‚Äù** demo: load a pre‚Äëexported '\n",
      "            'PyTorch model (ResNet‚Äë50) as an ONNX file, run a quick local '\n",
      "            'inference with\\u202fonnxruntime, then register the same ONNX '\n",
      "            'model in AWS\\u202fBedrock and invoke it via\\u202fboto3 to show '\n",
      "            'cloud latency and cost, and finally run a short AWS\\u202fGlue ETL '\n",
      "            'job that pulls new images from S3, preprocesses them, and updates '\n",
      "            'a manifest for the next inference. The AI supplies a 5‚Äëminute '\n",
      "            'demo outline (slide intro, local inference, Bedrock call, Glue '\n",
      "            'job, live ‚Äúwhat‚Äëif‚Äù refresh, wrap‚Äëup), a list of required laptop '\n",
      "            'packages (Python\\u202f3.10+, onnxruntime, boto3, Pillow, optional '\n",
      "            'Streamlit, AWS\\u202fCLI), a minimal working Python script, '\n",
      "            'logistics tips (charger, HDMI adapter, cheat‚Äësheet, sample '\n",
      "            'images, headphones), key talking points (speed, cost, '\n",
      "            'portability, data freshness, future fine‚Äëtuning), and a '\n",
      "            'night‚Äëbefore checklist to ensure the demo runs smoothly at the '\n",
      "            'restaurant.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
