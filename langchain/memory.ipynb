{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8d91c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pprint import pprint\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70bfed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Fredrick\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Fredrick! It’s great to meet you. I’m an AI language model that loves chatting about all sorts of topics—whether you’re into books, movies, science, cooking, travel, or just want to share a funny story from your day, I’m all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‑2024, so I can riff on everything from classic literature to the latest tech trends. I don’t have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What’s on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation—maybe a new sci‑fi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Fredrick\n",
      "AI: Hello Fredrick! It’s great to meet you. I’m an AI language model that loves chatting about all sorts of topics—whether you’re into books, movies, science, cooking, travel, or just want to share a funny story from your day, I’m all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‑2024, so I can riff on everything from classic literature to the latest tech trends. I don’t have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What’s on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation—maybe a new sci‑fi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "Human: What is AWS Glue?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**AWS Glue** is Amazon Web Services’ fully‑managed, serverless data‑integration service. In plain English, it’s a set of tools that help you move, clean, enrich, and catalog data so you can run analytics or machine‑learning workloads without having to spin up and manage your own ETL (Extract‑Transform‑Load) infrastructure.\n",
      "\n",
      "Below is a quick‑look‑around of the main pieces, why people love it, and a few things to watch out for.\n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣ Core Components\n",
      "\n",
      "| Component | What it does | Typical use‑case |\n",
      "|-----------|--------------|------------------|\n",
      "| **Glue Data Catalog** | Central metadata repository (tables, schemas, partitions, locations). It’s the “Hive Metastore” for the AWS world. | Acts as a single source of truth for Athena, Redshift Spectrum, EMR, and even external tools like Tableau. |\n",
      "| **Crawlers** | Scans data stores (S3, JDBC, DynamoDB, etc.) and automatically infers schema, partitions, and populates the Data Catalog. | You drop a new CSV/JSON/Parquet file into an S3 bucket and a crawler registers it as a table in minutes. |\n",
      "| **ETL Jobs** | Serverless Spark (or Python/Scala) scripts that read from sources, apply transformations, and write to targets. | Typical pipelines: “Read raw logs from S3 → parse → enrich with lookup table → write cleaned data to Parquet in another bucket.” |\n",
      "| **Triggers & Workflows** | Event‑driven or schedule‑driven orchestration (cron, on‑demand, on‑completion of another job). | Build multi‑step pipelines: e.g., run a crawler → start a job → notify Slack when done. |\n",
      "| **Development Endpoints** | Interactive notebooks (Jupyter, Zeppelin) that let you prototype ETL code against live data. | Data engineers love the “write‑once, test‑many” loop before committing a production job. |\n",
      "| **Glue Studio** (visual UI) | Drag‑and‑drop canvas for building ETL jobs without writing code. | Great for analysts or quick PoCs where you want to see the flow graphically. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣ How It Works (high‑level flow)\n",
      "\n",
      "1. **Ingest** – Data lands in a source (commonly S3, but also RDS, DynamoDB, Kafka, etc.).\n",
      "2. **Catalog** – A **crawler** runs (or you manually define a table) and registers the dataset in the **Glue Data Catalog**.\n",
      "3. **Transform** – A **Glue job** (Spark‑based) reads the cataloged table, applies transformations (SQL‑like, Python UDFs, joins, window functions, etc.).\n",
      "4. **Load** – The job writes the result to a destination (another S3 location, Redshift, Snowflake via JDBC, etc.).\n",
      "5. **Consume** – Downstream services (Athena, Redshift Spectrum, QuickSight, SageMaker) query the cleaned data directly from the catalog.\n",
      "\n",
      "All of this runs on **serverless Spark**: you never provision EC2 instances or manage a Spark cluster. Glue automatically scales the underlying compute up or down based on the job’s workload.\n",
      "\n",
      "---\n",
      "\n",
      "## 3️⃣ Why Teams Choose Glue\n",
      "\n",
      "| Benefit | Detail |\n",
      "|---------|--------|\n",
      "| **Zero‑ops** | No clusters to patch, no capacity planning. You just pay for the compute seconds you actually use. |\n",
      "| **Integrated catalog** | One place to store schema metadata that many AWS analytics services can read. |\n",
      "| **Auto‑schema inference** | Crawlers can detect column types, partitions, and even nested structures in JSON/Parquet. |\n",
      "| **Pay‑as‑you‑go pricing** | Charged per DPU‑hour (Data Processing Unit) for jobs, per crawler run, and per catalog storage (tiny cost). |\n",
      "| **Built‑in security** | IAM policies, VPC endpoints, encryption at rest (S3/KMS) and in transit, and fine‑grained column‑level permissions via Lake Formation. |\n",
      "| **Extensibility** | You can write custom Spark code, use Python libraries, or call external APIs from within a job. |\n",
      "| **Serverless notebooks** | Development endpoints let you prototype with familiar tools (Jupyter). |\n",
      "\n",
      "---\n",
      "\n",
      "## 4️⃣ Typical Use Cases\n",
      "\n",
      "| Scenario | How Glue fits |\n",
      "|----------|---------------|\n",
      "| **Data lake ingestion** | Crawl raw logs → register → transform → store as columnar Parquet for analytics. |\n",
      "| **ETL for reporting** | Pull data from RDS → cleanse → load into Redshift or Snowflake. |\n",
      "| **Machine‑learning feature pipelines** | Extract features from S3 → join with reference tables → write feature store files for SageMaker. |\n",
      "| **Regulatory compliance** | Use crawlers + Lake Formation to enforce column‑level masking before data is exposed to analysts. |\n",
      "| **Event‑driven pipelines** | Trigger a Glue job when a new file lands in an S3 bucket (via S3 event → Lambda → Glue start‑job). |\n",
      "\n",
      "---\n",
      "\n",
      "## 5️⃣ Pricing Snapshot (as of 2024)\n",
      "\n",
      "| Resource | Unit | Approx. cost (US‑East‑1) |\n",
      "|----------|------|--------------------------|\n",
      "| **Glue Data Catalog storage** | per 1 M tables/partitions per month | $1.00 |\n",
      "| **Crawler run** | per 1 000 seconds of runtime | $0.44 |\n",
      "| **ETL job** | per DPU‑hour (1 DPU = 4 vCPU + 16 GB RAM) | $0.44 (standard) or $0.88 (G.1X) |\n",
      "| **Glue Studio visual jobs** | same as ETL job pricing | – |\n",
      "| **Development endpoint** | per DPU‑hour while active | $0.44 |\n",
      "\n",
      "> **Tip:** Because Glue jobs are billed per second (rounded up to the nearest 1 second), you can keep costs low by using **job bookmarks** (process only new data) and **dynamic partition pruning** to avoid scanning whole tables.\n",
      "\n",
      "---\n",
      "\n",
      "## 6️⃣ Things to Keep in Mind\n",
      "\n",
      "| Potential drawback | Mitigation |\n",
      "|--------------------|------------|\n",
      "| **Cold‑start latency** – First run of a job can take ~30‑60 seconds to spin up Spark executors. | Keep a “warm” job running on a schedule (e.g., every hour) if you need sub‑second response. |\n",
      "| **Limited language support** – Native support is Scala, Python (PySpark), and Spark SQL. No native Java or .NET. | Use PySpark for most cases; you can still call Java libraries via Py4J if needed. |\n",
      "| **Cost visibility** – Large Spark jobs can consume many DPUs quickly. | Enable **job metrics** and CloudWatch alarms on DPU usage; break jobs into smaller stages if possible. |\n",
      "| **Complex transformations** – Very custom logic may be easier in a dedicated EMR cluster. | Evaluate whether Glue’s serverless model fits the complexity; you can still launch an EMR cluster from a Glue workflow if needed. |\n",
      "| **Versioning of libraries** – Glue provides a set of pre‑installed Spark packages; custom packages must be uploaded as .zip/.whl. | Use **Glue 4.0** (the latest runtime) which supports Python 3.9 and a broader set of libraries, and test your dependencies locally first. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7️⃣ Quick “Hello‑World” Example (Python/Glue ETL)\n",
      "\n",
      "```python\n",
      "import sys\n",
      "from awsglue.transforms import *\n",
      "from awsglue.utils import getResolvedOptions\n",
      "from awsglue.context import GlueContext\n",
      "from pyspark.context import SparkContext\n",
      "\n",
      "args = getResolvedOptions(sys.argv,\n",
      "                          ['JOB_NAME',\n",
      "                           'SOURCE_DB',\n",
      "                           'SOURCE_TABLE',\n",
      "                           'TARGET_PATH'])\n",
      "\n",
      "sc = SparkContext()\n",
      "glueContext = GlueContext(sc)\n",
      "spark = glueContext.spark_session\n",
      "\n",
      "# 1️⃣ Read from the Data Catalog\n",
      "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
      "    database=args['SOURCE_DB'],\n",
      "    table_name=args['SOURCE_TABLE']\n",
      ")\n",
      "\n",
      "# 2️⃣ Simple transformation: drop nulls & rename a column\n",
      "transformed = datasource \\\n",
      "    .filter(f=lambda x: x[\"price\"] is not None) \\\n",
      "    .rename_field(\"old_name\", \"new_name\")\n",
      "\n",
      "# 3️⃣ Write out as partitioned Parquet\n",
      "glueContext.write_dynamic_frame.from_options(\n",
      "    frame=transformed,\n",
      "    connection_type=\"s3\",\n",
      "    connection_options={\"path\": args['TARGET_PATH'],\n",
      "                        \"partitionKeys\": [\"year\", \"month\"]},\n",
      "    format=\"parquet\"\n",
      ")\n",
      "\n",
      "job.commit()\n",
      "```\n",
      "\n",
      "You’d submit this script as a **Glue job**, pass the arguments (e.g., `--SOURCE_DB mydb --SOURCE_TABLE raw_logs --TARGET_PATH s3://my‑lake/cleaned/`), and Glue will spin up the Spark executors, run the code, and shut down automatically.\n",
      "\n",
      "---\n",
      "\n",
      "## 8️⃣ Where to Learn More\n",
      "\n",
      "- **AWS Documentation** – “AWS Glue Developer Guide” (covers crawlers, jobs, Studio, and best practices).  \n",
      "- **AWS Glue Workshops** – Hands‑on labs in the AWS Training portal.  \n",
      "- **AWS re:Invent videos** – Look for “Serverless ETL with AWS Glue” (2023/2024 sessions).  \n",
      "- **GitHub** – The `aws-samples/aws-glue-samples` repo has ready‑made job scripts and notebooks.  \n",
      "\n",
      "---\n",
      "\n",
      "### 🎤 Your Turn!\n",
      "Do you have a specific data source you’re trying to bring into a lake, or a particular transformation you’re curious about (e.g., handling nested JSON, joining with a RDS table, or using Glue Studio’s visual editor)? Let me know, and we can sketch out a concrete pipeline together! \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Fredrick\n",
      "AI: Hello Fredrick! It’s great to meet you. I’m an AI language model that loves chatting about all sorts of topics—whether you’re into books, movies, science, cooking, travel, or just want to share a funny story from your day, I’m all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‑2024, so I can riff on everything from classic literature to the latest tech trends. I don’t have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What’s on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation—maybe a new sci‑fi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "Human: What is AWS Glue?\n",
      "AI: **AWS Glue** is Amazon Web Services’ fully‑managed, serverless data‑integration service. In plain English, it’s a set of tools that help you move, clean, enrich, and catalog data so you can run analytics or machine‑learning workloads without having to spin up and manage your own ETL (Extract‑Transform‑Load) infrastructure.\n",
      "\n",
      "Below is a quick‑look‑around of the main pieces, why people love it, and a few things to watch out for.\n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣ Core Components\n",
      "\n",
      "| Component | What it does | Typical use‑case |\n",
      "|-----------|--------------|------------------|\n",
      "| **Glue Data Catalog** | Central metadata repository (tables, schemas, partitions, locations). It’s the “Hive Metastore” for the AWS world. | Acts as a single source of truth for Athena, Redshift Spectrum, EMR, and even external tools like Tableau. |\n",
      "| **Crawlers** | Scans data stores (S3, JDBC, DynamoDB, etc.) and automatically infers schema, partitions, and populates the Data Catalog. | You drop a new CSV/JSON/Parquet file into an S3 bucket and a crawler registers it as a table in minutes. |\n",
      "| **ETL Jobs** | Serverless Spark (or Python/Scala) scripts that read from sources, apply transformations, and write to targets. | Typical pipelines: “Read raw logs from S3 → parse → enrich with lookup table → write cleaned data to Parquet in another bucket.” |\n",
      "| **Triggers & Workflows** | Event‑driven or schedule‑driven orchestration (cron, on‑demand, on‑completion of another job). | Build multi‑step pipelines: e.g., run a crawler → start a job → notify Slack when done. |\n",
      "| **Development Endpoints** | Interactive notebooks (Jupyter, Zeppelin) that let you prototype ETL code against live data. | Data engineers love the “write‑once, test‑many” loop before committing a production job. |\n",
      "| **Glue Studio** (visual UI) | Drag‑and‑drop canvas for building ETL jobs without writing code. | Great for analysts or quick PoCs where you want to see the flow graphically. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣ How It Works (high‑level flow)\n",
      "\n",
      "1. **Ingest** – Data lands in a source (commonly S3, but also RDS, DynamoDB, Kafka, etc.).\n",
      "2. **Catalog** – A **crawler** runs (or you manually define a table) and registers the dataset in the **Glue Data Catalog**.\n",
      "3. **Transform** – A **Glue job** (Spark‑based) reads the cataloged table, applies transformations (SQL‑like, Python UDFs, joins, window functions, etc.).\n",
      "4. **Load** – The job writes the result to a destination (another S3 location, Redshift, Snowflake via JDBC, etc.).\n",
      "5. **Consume** – Downstream services (Athena, Redshift Spectrum, QuickSight, SageMaker) query the cleaned data directly from the catalog.\n",
      "\n",
      "All of this runs on **serverless Spark**: you never provision EC2 instances or manage a Spark cluster. Glue automatically scales the underlying compute up or down based on the job’s workload.\n",
      "\n",
      "---\n",
      "\n",
      "## 3️⃣ Why Teams Choose Glue\n",
      "\n",
      "| Benefit | Detail |\n",
      "|---------|--------|\n",
      "| **Zero‑ops** | No clusters to patch, no capacity planning. You just pay for the compute seconds you actually use. |\n",
      "| **Integrated catalog** | One place to store schema metadata that many AWS analytics services can read. |\n",
      "| **Auto‑schema inference** | Crawlers can detect column types, partitions, and even nested structures in JSON/Parquet. |\n",
      "| **Pay‑as‑you‑go pricing** | Charged per DPU‑hour (Data Processing Unit) for jobs, per crawler run, and per catalog storage (tiny cost). |\n",
      "| **Built‑in security** | IAM policies, VPC endpoints, encryption at rest (S3/KMS) and in transit, and fine‑grained column‑level permissions via Lake Formation. |\n",
      "| **Extensibility** | You can write custom Spark code, use Python libraries, or call external APIs from within a job. |\n",
      "| **Serverless notebooks** | Development endpoints let you prototype with familiar tools (Jupyter). |\n",
      "\n",
      "---\n",
      "\n",
      "## 4️⃣ Typical Use Cases\n",
      "\n",
      "| Scenario | How Glue fits |\n",
      "|----------|---------------|\n",
      "| **Data lake ingestion** | Crawl raw logs → register → transform → store as columnar Parquet for analytics. |\n",
      "| **ETL for reporting** | Pull data from RDS → cleanse → load into Redshift or Snowflake. |\n",
      "| **Machine‑learning feature pipelines** | Extract features from S3 → join with reference tables → write feature store files for SageMaker. |\n",
      "| **Regulatory compliance** | Use crawlers + Lake Formation to enforce column‑level masking before data is exposed to analysts. |\n",
      "| **Event‑driven pipelines** | Trigger a Glue job when a new file lands in an S3 bucket (via S3 event → Lambda → Glue start‑job). |\n",
      "\n",
      "---\n",
      "\n",
      "## 5️⃣ Pricing Snapshot (as of 2024)\n",
      "\n",
      "| Resource | Unit | Approx. cost (US‑East‑1) |\n",
      "|----------|------|--------------------------|\n",
      "| **Glue Data Catalog storage** | per 1 M tables/partitions per month | $1.00 |\n",
      "| **Crawler run** | per 1 000 seconds of runtime | $0.44 |\n",
      "| **ETL job** | per DPU‑hour (1 DPU = 4 vCPU + 16 GB RAM) | $0.44 (standard) or $0.88 (G.1X) |\n",
      "| **Glue Studio visual jobs** | same as ETL job pricing | – |\n",
      "| **Development endpoint** | per DPU‑hour while active | $0.44 |\n",
      "\n",
      "> **Tip:** Because Glue jobs are billed per second (rounded up to the nearest 1 second), you can keep costs low by using **job bookmarks** (process only new data) and **dynamic partition pruning** to avoid scanning whole tables.\n",
      "\n",
      "---\n",
      "\n",
      "## 6️⃣ Things to Keep in Mind\n",
      "\n",
      "| Potential drawback | Mitigation |\n",
      "|--------------------|------------|\n",
      "| **Cold‑start latency** – First run of a job can take ~30‑60 seconds to spin up Spark executors. | Keep a “warm” job running on a schedule (e.g., every hour) if you need sub‑second response. |\n",
      "| **Limited language support** – Native support is Scala, Python (PySpark), and Spark SQL. No native Java or .NET. | Use PySpark for most cases; you can still call Java libraries via Py4J if needed. |\n",
      "| **Cost visibility** – Large Spark jobs can consume many DPUs quickly. | Enable **job metrics** and CloudWatch alarms on DPU usage; break jobs into smaller stages if possible. |\n",
      "| **Complex transformations** – Very custom logic may be easier in a dedicated EMR cluster. | Evaluate whether Glue’s serverless model fits the complexity; you can still launch an EMR cluster from a Glue workflow if needed. |\n",
      "| **Versioning of libraries** – Glue provides a set of pre‑installed Spark packages; custom packages must be uploaded as .zip/.whl. | Use **Glue 4.0** (the latest runtime) which supports Python 3.9 and a broader set of libraries, and test your dependencies locally first. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7️⃣ Quick “Hello‑World” Example (Python/Glue ETL)\n",
      "\n",
      "```python\n",
      "import sys\n",
      "from awsglue.transforms import *\n",
      "from awsglue.utils import getResolvedOptions\n",
      "from awsglue.context import GlueContext\n",
      "from pyspark.context import SparkContext\n",
      "\n",
      "args = getResolvedOptions(sys.argv,\n",
      "                          ['JOB_NAME',\n",
      "                           'SOURCE_DB',\n",
      "                           'SOURCE_TABLE',\n",
      "                           'TARGET_PATH'])\n",
      "\n",
      "sc = SparkContext()\n",
      "glueContext = GlueContext(sc)\n",
      "spark = glueContext.spark_session\n",
      "\n",
      "# 1️⃣ Read from the Data Catalog\n",
      "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
      "    database=args['SOURCE_DB'],\n",
      "    table_name=args['SOURCE_TABLE']\n",
      ")\n",
      "\n",
      "# 2️⃣ Simple transformation: drop nulls & rename a column\n",
      "transformed = datasource \\\n",
      "    .filter(f=lambda x: x[\"price\"] is not None) \\\n",
      "    .rename_field(\"old_name\", \"new_name\")\n",
      "\n",
      "# 3️⃣ Write out as partitioned Parquet\n",
      "glueContext.write_dynamic_frame.from_options(\n",
      "    frame=transformed,\n",
      "    connection_type=\"s3\",\n",
      "    connection_options={\"path\": args['TARGET_PATH'],\n",
      "                        \"partitionKeys\": [\"year\", \"month\"]},\n",
      "    format=\"parquet\"\n",
      ")\n",
      "\n",
      "job.commit()\n",
      "```\n",
      "\n",
      "You’d submit this script as a **Glue job**, pass the arguments (e.g., `--SOURCE_DB mydb --SOURCE_TABLE raw_logs --TARGET_PATH s3://my‑lake/cleaned/`), and Glue will spin up the Spark executors, run the code, and shut down automatically.\n",
      "\n",
      "---\n",
      "\n",
      "## 8️⃣ Where to Learn More\n",
      "\n",
      "- **AWS Documentation** – “AWS Glue Developer Guide” (covers crawlers, jobs, Studio, and best practices).  \n",
      "- **AWS Glue Workshops** – Hands‑on labs in the AWS Training portal.  \n",
      "- **AWS re:Invent videos** – Look for “Serverless ETL with AWS Glue” (2023/2024 sessions).  \n",
      "- **GitHub** – The `aws-samples/aws-glue-samples` repo has ready‑made job scripts and notebooks.  \n",
      "\n",
      "---\n",
      "\n",
      "### 🎤 Your Turn!\n",
      "Do you have a specific data source you’re trying to bring into a lake, or a particular transformation you’re curious about (e.g., handling nested JSON, joining with a RDS table, or using Glue Studio’s visual editor)? Let me know, and we can sketch out a concrete pipeline together!\n",
      "Human: What is my name? And what did we talk about last time?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hey Fredrick! 👋 Nice to see you back.\n",
      "\n",
      "**Your name:** Fredrick (you told me that right at the start of our chat).\n",
      "\n",
      "**What we talked about last time:** We dove into **AWS Glue**, Amazon’s fully‑managed, serverless data‑integration service. I gave you a rundown of its main components (Data Catalog, crawlers, ETL jobs, triggers/workflows, development endpoints, and Glue Studio), explained the high‑level workflow (ingest → catalog → transform → load → consume), highlighted why teams love it (zero‑ops, integrated catalog, auto‑schema inference, pay‑as‑you‑go pricing, built‑in security, extensibility, notebooks), listed common use cases (data‑lake ingestion, reporting pipelines, ML feature prep, compliance, event‑driven pipelines), sketched the pricing model, pointed out a few gotchas, and even shared a quick “Hello‑World” PySpark ETL script.\n",
      "\n",
      "If you’d like to pick up where we left off—maybe you have a specific data source you want to bring into Glue, or you’re curious about building a workflow with crawlers and triggers—just let me know and we can map it out together! 🚀\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(api_key=os.environ['GROQ_API_KEY'], temperature=0.0, model=\"openai/gpt-oss-120b\")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(conversation.predict(input=\"Hi, my name is Fredrick\"))\n",
    "print(conversation.predict(input=\"What is AWS Glue?\"), \"\\n\")\n",
    "print(conversation.predict(input=\"What is my name? And what did we talk about last time?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0becb3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Fredrick\n",
      "AI: Hello Fredrick! It’s great to meet you. I’m an AI language model that loves chatting about all sorts of topics—whether you’re into books, movies, science, cooking, travel, or just want to share a funny story from your day, I’m all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‑2024, so I can riff on everything from classic literature to the latest tech trends. I don’t have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What’s on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation—maybe a new sci‑fi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "Human: What is AWS Glue?\n",
      "AI: **AWS Glue** is Amazon Web Services’ fully‑managed, serverless data‑integration service. In plain English, it’s a set of tools that help you move, clean, enrich, and catalog data so you can run analytics or machine‑learning workloads without having to spin up and manage your own ETL (Extract‑Transform‑Load) infrastructure.\n",
      "\n",
      "Below is a quick‑look‑around of the main pieces, why people love it, and a few things to watch out for.\n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣ Core Components\n",
      "\n",
      "| Component | What it does | Typical use‑case |\n",
      "|-----------|--------------|------------------|\n",
      "| **Glue Data Catalog** | Central metadata repository (tables, schemas, partitions, locations). It’s the “Hive Metastore” for the AWS world. | Acts as a single source of truth for Athena, Redshift Spectrum, EMR, and even external tools like Tableau. |\n",
      "| **Crawlers** | Scans data stores (S3, JDBC, DynamoDB, etc.) and automatically infers schema, partitions, and populates the Data Catalog. | You drop a new CSV/JSON/Parquet file into an S3 bucket and a crawler registers it as a table in minutes. |\n",
      "| **ETL Jobs** | Serverless Spark (or Python/Scala) scripts that read from sources, apply transformations, and write to targets. | Typical pipelines: “Read raw logs from S3 → parse → enrich with lookup table → write cleaned data to Parquet in another bucket.” |\n",
      "| **Triggers & Workflows** | Event‑driven or schedule‑driven orchestration (cron, on‑demand, on‑completion of another job). | Build multi‑step pipelines: e.g., run a crawler → start a job → notify Slack when done. |\n",
      "| **Development Endpoints** | Interactive notebooks (Jupyter, Zeppelin) that let you prototype ETL code against live data. | Data engineers love the “write‑once, test‑many” loop before committing a production job. |\n",
      "| **Glue Studio** (visual UI) | Drag‑and‑drop canvas for building ETL jobs without writing code. | Great for analysts or quick PoCs where you want to see the flow graphically. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣ How It Works (high‑level flow)\n",
      "\n",
      "1. **Ingest** – Data lands in a source (commonly S3, but also RDS, DynamoDB, Kafka, etc.).\n",
      "2. **Catalog** – A **crawler** runs (or you manually define a table) and registers the dataset in the **Glue Data Catalog**.\n",
      "3. **Transform** – A **Glue job** (Spark‑based) reads the cataloged table, applies transformations (SQL‑like, Python UDFs, joins, window functions, etc.).\n",
      "4. **Load** – The job writes the result to a destination (another S3 location, Redshift, Snowflake via JDBC, etc.).\n",
      "5. **Consume** – Downstream services (Athena, Redshift Spectrum, QuickSight, SageMaker) query the cleaned data directly from the catalog.\n",
      "\n",
      "All of this runs on **serverless Spark**: you never provision EC2 instances or manage a Spark cluster. Glue automatically scales the underlying compute up or down based on the job’s workload.\n",
      "\n",
      "---\n",
      "\n",
      "## 3️⃣ Why Teams Choose Glue\n",
      "\n",
      "| Benefit | Detail |\n",
      "|---------|--------|\n",
      "| **Zero‑ops** | No clusters to patch, no capacity planning. You just pay for the compute seconds you actually use. |\n",
      "| **Integrated catalog** | One place to store schema metadata that many AWS analytics services can read. |\n",
      "| **Auto‑schema inference** | Crawlers can detect column types, partitions, and even nested structures in JSON/Parquet. |\n",
      "| **Pay‑as‑you‑go pricing** | Charged per DPU‑hour (Data Processing Unit) for jobs, per crawler run, and per catalog storage (tiny cost). |\n",
      "| **Built‑in security** | IAM policies, VPC endpoints, encryption at rest (S3/KMS) and in transit, and fine‑grained column‑level permissions via Lake Formation. |\n",
      "| **Extensibility** | You can write custom Spark code, use Python libraries, or call external APIs from within a job. |\n",
      "| **Serverless notebooks** | Development endpoints let you prototype with familiar tools (Jupyter). |\n",
      "\n",
      "---\n",
      "\n",
      "## 4️⃣ Typical Use Cases\n",
      "\n",
      "| Scenario | How Glue fits |\n",
      "|----------|---------------|\n",
      "| **Data lake ingestion** | Crawl raw logs → register → transform → store as columnar Parquet for analytics. |\n",
      "| **ETL for reporting** | Pull data from RDS → cleanse → load into Redshift or Snowflake. |\n",
      "| **Machine‑learning feature pipelines** | Extract features from S3 → join with reference tables → write feature store files for SageMaker. |\n",
      "| **Regulatory compliance** | Use crawlers + Lake Formation to enforce column‑level masking before data is exposed to analysts. |\n",
      "| **Event‑driven pipelines** | Trigger a Glue job when a new file lands in an S3 bucket (via S3 event → Lambda → Glue start‑job). |\n",
      "\n",
      "---\n",
      "\n",
      "## 5️⃣ Pricing Snapshot (as of 2024)\n",
      "\n",
      "| Resource | Unit | Approx. cost (US‑East‑1) |\n",
      "|----------|------|--------------------------|\n",
      "| **Glue Data Catalog storage** | per 1 M tables/partitions per month | $1.00 |\n",
      "| **Crawler run** | per 1 000 seconds of runtime | $0.44 |\n",
      "| **ETL job** | per DPU‑hour (1 DPU = 4 vCPU + 16 GB RAM) | $0.44 (standard) or $0.88 (G.1X) |\n",
      "| **Glue Studio visual jobs** | same as ETL job pricing | – |\n",
      "| **Development endpoint** | per DPU‑hour while active | $0.44 |\n",
      "\n",
      "> **Tip:** Because Glue jobs are billed per second (rounded up to the nearest 1 second), you can keep costs low by using **job bookmarks** (process only new data) and **dynamic partition pruning** to avoid scanning whole tables.\n",
      "\n",
      "---\n",
      "\n",
      "## 6️⃣ Things to Keep in Mind\n",
      "\n",
      "| Potential drawback | Mitigation |\n",
      "|--------------------|------------|\n",
      "| **Cold‑start latency** – First run of a job can take ~30‑60 seconds to spin up Spark executors. | Keep a “warm” job running on a schedule (e.g., every hour) if you need sub‑second response. |\n",
      "| **Limited language support** – Native support is Scala, Python (PySpark), and Spark SQL. No native Java or .NET. | Use PySpark for most cases; you can still call Java libraries via Py4J if needed. |\n",
      "| **Cost visibility** – Large Spark jobs can consume many DPUs quickly. | Enable **job metrics** and CloudWatch alarms on DPU usage; break jobs into smaller stages if possible. |\n",
      "| **Complex transformations** – Very custom logic may be easier in a dedicated EMR cluster. | Evaluate whether Glue’s serverless model fits the complexity; you can still launch an EMR cluster from a Glue workflow if needed. |\n",
      "| **Versioning of libraries** – Glue provides a set of pre‑installed Spark packages; custom packages must be uploaded as .zip/.whl. | Use **Glue 4.0** (the latest runtime) which supports Python 3.9 and a broader set of libraries, and test your dependencies locally first. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7️⃣ Quick “Hello‑World” Example (Python/Glue ETL)\n",
      "\n",
      "```python\n",
      "import sys\n",
      "from awsglue.transforms import *\n",
      "from awsglue.utils import getResolvedOptions\n",
      "from awsglue.context import GlueContext\n",
      "from pyspark.context import SparkContext\n",
      "\n",
      "args = getResolvedOptions(sys.argv,\n",
      "                          ['JOB_NAME',\n",
      "                           'SOURCE_DB',\n",
      "                           'SOURCE_TABLE',\n",
      "                           'TARGET_PATH'])\n",
      "\n",
      "sc = SparkContext()\n",
      "glueContext = GlueContext(sc)\n",
      "spark = glueContext.spark_session\n",
      "\n",
      "# 1️⃣ Read from the Data Catalog\n",
      "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
      "    database=args['SOURCE_DB'],\n",
      "    table_name=args['SOURCE_TABLE']\n",
      ")\n",
      "\n",
      "# 2️⃣ Simple transformation: drop nulls & rename a column\n",
      "transformed = datasource \\\n",
      "    .filter(f=lambda x: x[\"price\"] is not None) \\\n",
      "    .rename_field(\"old_name\", \"new_name\")\n",
      "\n",
      "# 3️⃣ Write out as partitioned Parquet\n",
      "glueContext.write_dynamic_frame.from_options(\n",
      "    frame=transformed,\n",
      "    connection_type=\"s3\",\n",
      "    connection_options={\"path\": args['TARGET_PATH'],\n",
      "                        \"partitionKeys\": [\"year\", \"month\"]},\n",
      "    format=\"parquet\"\n",
      ")\n",
      "\n",
      "job.commit()\n",
      "```\n",
      "\n",
      "You’d submit this script as a **Glue job**, pass the arguments (e.g., `--SOURCE_DB mydb --SOURCE_TABLE raw_logs --TARGET_PATH s3://my‑lake/cleaned/`), and Glue will spin up the Spark executors, run the code, and shut down automatically.\n",
      "\n",
      "---\n",
      "\n",
      "## 8️⃣ Where to Learn More\n",
      "\n",
      "- **AWS Documentation** – “AWS Glue Developer Guide” (covers crawlers, jobs, Studio, and best practices).  \n",
      "- **AWS Glue Workshops** – Hands‑on labs in the AWS Training portal.  \n",
      "- **AWS re:Invent videos** – Look for “Serverless ETL with AWS Glue” (2023/2024 sessions).  \n",
      "- **GitHub** – The `aws-samples/aws-glue-samples` repo has ready‑made job scripts and notebooks.  \n",
      "\n",
      "---\n",
      "\n",
      "### 🎤 Your Turn!\n",
      "Do you have a specific data source you’re trying to bring into a lake, or a particular transformation you’re curious about (e.g., handling nested JSON, joining with a RDS table, or using Glue Studio’s visual editor)? Let me know, and we can sketch out a concrete pipeline together!\n",
      "Human: What is my name? And what did we talk about last time?\n",
      "AI: Hey Fredrick! 👋 Nice to see you back.\n",
      "\n",
      "**Your name:** Fredrick (you told me that right at the start of our chat).\n",
      "\n",
      "**What we talked about last time:** We dove into **AWS Glue**, Amazon’s fully‑managed, serverless data‑integration service. I gave you a rundown of its main components (Data Catalog, crawlers, ETL jobs, triggers/workflows, development endpoints, and Glue Studio), explained the high‑level workflow (ingest → catalog → transform → load → consume), highlighted why teams love it (zero‑ops, integrated catalog, auto‑schema inference, pay‑as‑you‑go pricing, built‑in security, extensibility, notebooks), listed common use cases (data‑lake ingestion, reporting pipelines, ML feature prep, compliance, event‑driven pipelines), sketched the pricing model, pointed out a few gotchas, and even shared a quick “Hello‑World” PySpark ETL script.\n",
      "\n",
      "If you’d like to pick up where we left off—maybe you have a specific data source you want to bring into Glue, or you’re curious about building a workflow with crawlers and triggers—just let me know and we can map it out together! 🚀\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4b88047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi, my name is Fredrick\\nAI: Hello Fredrick! It’s great to meet you. I’m an AI language model that loves chatting about all sorts of topics—whether you’re into books, movies, science, cooking, travel, or just want to share a funny story from your day, I’m all ears (well, metaphorical ears).\\n\\nA little bit about me: I was trained on a diverse mix of texts up through mid‑2024, so I can riff on everything from classic literature to the latest tech trends. I don’t have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\\n\\nWhat’s on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation—maybe a new sci‑fi novel or a tasty recipe? Let me know, and we can dive right in!\\nHuman: What is AWS Glue?\\nAI: **AWS Glue** is Amazon\\u202fWeb\\u202fServices’ fully‑managed, serverless data‑integration service. In plain English, it’s a set of tools that help you move, clean, enrich, and catalog data so you can run analytics or machine‑learning workloads without having to spin up and manage your own ETL (Extract‑Transform‑Load) infrastructure.\\n\\nBelow is a quick‑look‑around of the main pieces, why people love it, and a few things to watch out for.\\n\\n---\\n\\n## 1️⃣ Core Components\\n\\n| Component | What it does | Typical use‑case |\\n|-----------|--------------|------------------|\\n| **Glue Data Catalog** | Central metadata repository (tables, schemas, partitions, locations). It’s the “Hive Metastore” for the AWS world. | Acts as a single source of truth for Athena, Redshift Spectrum, EMR, and even external tools like Tableau. |\\n| **Crawlers** | Scans data stores (S3, JDBC, DynamoDB, etc.) and automatically infers schema, partitions, and populates the Data Catalog. | You drop a new CSV/JSON/Parquet file into an S3 bucket and a crawler registers it as a table in minutes. |\\n| **ETL Jobs** | Serverless Spark (or Python/Scala) scripts that read from sources, apply transformations, and write to targets. | Typical pipelines: “Read raw logs from S3 → parse → enrich with lookup table → write cleaned data to Parquet in another bucket.” |\\n| **Triggers & Workflows** | Event‑driven or schedule‑driven orchestration (cron, on‑demand, on‑completion of another job). | Build multi‑step pipelines: e.g., run a crawler → start a job → notify Slack when done. |\\n| **Development Endpoints** | Interactive notebooks (Jupyter, Zeppelin) that let you prototype ETL code against live data. | Data engineers love the “write‑once, test‑many” loop before committing a production job. |\\n| **Glue Studio** (visual UI) | Drag‑and‑drop canvas for building ETL jobs without writing code. | Great for analysts or quick PoCs where you want to see the flow graphically. |\\n\\n---\\n\\n## 2️⃣ How It Works (high‑level flow)\\n\\n1. **Ingest** – Data lands in a source (commonly S3, but also RDS, DynamoDB, Kafka, etc.).\\n2. **Catalog** – A **crawler** runs (or you manually define a table) and registers the dataset in the **Glue Data Catalog**.\\n3. **Transform** – A **Glue job** (Spark‑based) reads the cataloged table, applies transformations (SQL‑like, Python UDFs, joins, window functions, etc.).\\n4. **Load** – The job writes the result to a destination (another S3 location, Redshift, Snowflake via JDBC, etc.).\\n5. **Consume** – Downstream services (Athena, Redshift Spectrum, QuickSight, SageMaker) query the cleaned data directly from the catalog.\\n\\nAll of this runs on **serverless Spark**: you never provision EC2 instances or manage a Spark cluster. Glue automatically scales the underlying compute up or down based on the job’s workload.\\n\\n---\\n\\n## 3️⃣ Why Teams Choose Glue\\n\\n| Benefit | Detail |\\n|---------|--------|\\n| **Zero‑ops** | No clusters to patch, no capacity planning. You just pay for the compute seconds you actually use. |\\n| **Integrated catalog** | One place to store schema metadata that many AWS analytics services can read. |\\n| **Auto‑schema inference** | Crawlers can detect column types, partitions, and even nested structures in JSON/Parquet. |\\n| **Pay‑as‑you‑go pricing** | Charged per DPU‑hour (Data Processing Unit) for jobs, per crawler run, and per catalog storage (tiny cost). |\\n| **Built‑in security** | IAM policies, VPC endpoints, encryption at rest (S3/KMS) and in transit, and fine‑grained column‑level permissions via Lake Formation. |\\n| **Extensibility** | You can write custom Spark code, use Python libraries, or call external APIs from within a job. |\\n| **Serverless notebooks** | Development endpoints let you prototype with familiar tools (Jupyter). |\\n\\n---\\n\\n## 4️⃣ Typical Use Cases\\n\\n| Scenario | How Glue fits |\\n|----------|---------------|\\n| **Data lake ingestion** | Crawl raw logs → register → transform → store as columnar Parquet for analytics. |\\n| **ETL for reporting** | Pull data from RDS → cleanse → load into Redshift or Snowflake. |\\n| **Machine‑learning feature pipelines** | Extract features from S3 → join with reference tables → write feature store files for SageMaker. |\\n| **Regulatory compliance** | Use crawlers + Lake Formation to enforce column‑level masking before data is exposed to analysts. |\\n| **Event‑driven pipelines** | Trigger a Glue job when a new file lands in an S3 bucket (via S3 event → Lambda → Glue start‑job). |\\n\\n---\\n\\n## 5️⃣ Pricing Snapshot (as of 2024)\\n\\n| Resource | Unit | Approx. cost (US‑East‑1) |\\n|----------|------|--------------------------|\\n| **Glue Data Catalog storage** | per 1\\u202fM tables/partitions per month | $1.00 |\\n| **Crawler run** | per 1\\u202f000\\u202fseconds of runtime | $0.44 |\\n| **ETL job** | per DPU‑hour (1\\u202fDPU = 4\\u202fvCPU + 16\\u202fGB RAM) | $0.44 (standard) or $0.88 (G.1X) |\\n| **Glue Studio visual jobs** | same as ETL job pricing | – |\\n| **Development endpoint** | per DPU‑hour while active | $0.44 |\\n\\n> **Tip:** Because Glue jobs are billed per second (rounded up to the nearest 1\\u202fsecond), you can keep costs low by using **job bookmarks** (process only new data) and **dynamic partition pruning** to avoid scanning whole tables.\\n\\n---\\n\\n## 6️⃣ Things to Keep in Mind\\n\\n| Potential drawback | Mitigation |\\n|--------------------|------------|\\n| **Cold‑start latency** – First run of a job can take ~30‑60\\u202fseconds to spin up Spark executors. | Keep a “warm” job running on a schedule (e.g., every hour) if you need sub‑second response. |\\n| **Limited language support** – Native support is Scala, Python (PySpark), and Spark SQL. No native Java or .NET. | Use PySpark for most cases; you can still call Java libraries via Py4J if needed. |\\n| **Cost visibility** – Large Spark jobs can consume many DPUs quickly. | Enable **job metrics** and CloudWatch alarms on DPU usage; break jobs into smaller stages if possible. |\\n| **Complex transformations** – Very custom logic may be easier in a dedicated EMR cluster. | Evaluate whether Glue’s serverless model fits the complexity; you can still launch an EMR cluster from a Glue workflow if needed. |\\n| **Versioning of libraries** – Glue provides a set of pre‑installed Spark packages; custom packages must be uploaded as .zip/.whl. | Use **Glue 4.0** (the latest runtime) which supports Python\\u202f3.9 and a broader set of libraries, and test your dependencies locally first. |\\n\\n---\\n\\n## 7️⃣ Quick “Hello‑World” Example (Python/Glue\\u202fETL)\\n\\n```python\\nimport sys\\nfrom awsglue.transforms import *\\nfrom awsglue.utils import getResolvedOptions\\nfrom awsglue.context import GlueContext\\nfrom pyspark.context import SparkContext\\n\\nargs = getResolvedOptions(sys.argv,\\n                          [\\'JOB_NAME\\',\\n                           \\'SOURCE_DB\\',\\n                           \\'SOURCE_TABLE\\',\\n                           \\'TARGET_PATH\\'])\\n\\nsc = SparkContext()\\nglueContext = GlueContext(sc)\\nspark = glueContext.spark_session\\n\\n# 1️⃣ Read from the Data Catalog\\ndatasource = glueContext.create_dynamic_frame.from_catalog(\\n    database=args[\\'SOURCE_DB\\'],\\n    table_name=args[\\'SOURCE_TABLE\\']\\n)\\n\\n# 2️⃣ Simple transformation: drop nulls & rename a column\\ntransformed = datasource \\\\\\n    .filter(f=lambda x: x[\"price\"] is not None) \\\\\\n    .rename_field(\"old_name\", \"new_name\")\\n\\n# 3️⃣ Write out as partitioned Parquet\\nglueContext.write_dynamic_frame.from_options(\\n    frame=transformed,\\n    connection_type=\"s3\",\\n    connection_options={\"path\": args[\\'TARGET_PATH\\'],\\n                        \"partitionKeys\": [\"year\", \"month\"]},\\n    format=\"parquet\"\\n)\\n\\njob.commit()\\n```\\n\\nYou’d submit this script as a **Glue job**, pass the arguments (e.g., `--SOURCE_DB mydb --SOURCE_TABLE raw_logs --TARGET_PATH s3://my‑lake/cleaned/`), and Glue will spin up the Spark executors, run the code, and shut down automatically.\\n\\n---\\n\\n## 8️⃣ Where to Learn More\\n\\n- **AWS Documentation** – “AWS Glue Developer Guide” (covers crawlers, jobs, Studio, and best practices).  \\n- **AWS Glue Workshops** – Hands‑on labs in the AWS Training portal.  \\n- **AWS re:Invent videos** – Look for “Serverless ETL with AWS Glue” (2023/2024 sessions).  \\n- **GitHub** – The `aws-samples/aws-glue-samples` repo has ready‑made job scripts and notebooks.  \\n\\n---\\n\\n### 🎤 Your Turn!\\nDo you have a specific data source you’re trying to bring into a lake, or a particular transformation you’re curious about (e.g., handling nested JSON, joining with a RDS table, or using Glue Studio’s visual editor)? Let me know, and we can sketch out a concrete pipeline together!\\nHuman: What is my name? And what did we talk about last time?\\nAI: Hey Fredrick! 👋\\u202fNice to see you back.\\n\\n**Your name:**\\u202fFredrick (you told me that right at the start of our chat).\\n\\n**What we talked about last time:**\\u202fWe dove into **AWS\\u202fGlue**, Amazon’s fully‑managed, serverless data‑integration service. I gave you a rundown of its main components (Data Catalog, crawlers, ETL jobs, triggers/workflows, development endpoints, and Glue\\u202fStudio), explained the high‑level workflow (ingest → catalog → transform → load → consume), highlighted why teams love it (zero‑ops, integrated catalog, auto‑schema inference, pay‑as‑you‑go pricing, built‑in security, extensibility, notebooks), listed common use cases (data‑lake ingestion, reporting pipelines, ML feature prep, compliance, event‑driven pipelines), sketched the pricing model, pointed out a few gotchas, and even shared a quick “Hello‑World” PySpark ETL script.\\n\\nIf you’d like to pick up where we left off—maybe you have a specific data source you want to bring into Glue, or you’re curious about building a workflow with crawlers and triggers—just let me know and we can map it out together! 🚀'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e5d2758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up?\"}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "print(memory.buffer)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f0df6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: Hi\\nAI: What's up?\\nHuman: Not much, just hanging\\nAI: Cool\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})['history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7d4c6",
   "metadata": {},
   "source": [
    "**Different kinds of memory are available that can store and accumulate the conversation - ConversationBufferWindowMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7b3278d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)      # For k pairs(Human-AI) of the last conversation\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Whats up\"})\n",
    "memory.save_context({\"input\": \"not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ad2b992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Fredrick\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Fredrick! It’s great to meet you. I’m an AI language model that loves chatting about all sorts of topics—whether you’re into books, movies, science, cooking, travel, or just want to share a funny story from your day, I’m all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‑2024, so I can riff on everything from classic literature to the latest tech trends. I don’t have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What’s on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation—maybe a new sci‑fi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Fredrick\n",
      "AI: Hello Fredrick! It’s great to meet you. I’m an AI language model that loves chatting about all sorts of topics—whether you’re into books, movies, science, cooking, travel, or just want to share a funny story from your day, I’m all ears (well, metaphorical ears).\n",
      "\n",
      "A little bit about me: I was trained on a diverse mix of texts up through mid‑2024, so I can riff on everything from classic literature to the latest tech trends. I don’t have personal experiences, but I can pull together details from history, pop culture, and even niche hobbies to keep our conversation lively.\n",
      "\n",
      "What’s on your mind today? Are you working on any projects, planning a trip, or just looking for a good recommendation—maybe a new sci‑fi novel or a tasty recipe? Let me know, and we can dive right in!\n",
      "Human: What is AWS Bedrock?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**AWS Bedrock** is Amazon Web Services’ fully‑managed platform for building and running **generative‑AI applications** with large foundation models (LLMs, diffusion models, embedding models, etc.) — all without having to spin up your own GPU clusters or manage the underlying infrastructure.\n",
      "\n",
      "---\n",
      "\n",
      "## The big picture\n",
      "\n",
      "| Piece | What it means for you |\n",
      "|------|------------------------|\n",
      "| **Managed service** | AWS runs the hardware, scaling, patching, and security. You just call an API. |\n",
      "| **Foundation‑model hub** | A catalog of pre‑trained models from both AWS (e.g., **Titan** series) and leading AI startups (Anthropic, AI21, Stability AI, Cohere). |\n",
      "| **Pay‑as‑you‑go** | You’re billed per token (for text) or per image generated, plus a small per‑request fee for the API. No upfront commitments. |\n",
      "| **Fine‑tuning & customisation** | You can create a “personalised” version of a model by feeding it your own data (via a simple UI or API). |\n",
      "| **Secure & compliant** | Data never leaves your VPC unless you explicitly allow it; encryption at rest/in‑flight, IAM‑based access control, and integration with AWS KMS, CloudTrail, etc. |\n",
      "| **Integrated with the rest of AWS** | Easy to pipe Bedrock output into Lambda, SageMaker, Step Functions, DynamoDB, S3, or even a front‑end hosted on Amplify. |\n",
      "\n",
      "---\n",
      "\n",
      "## How it works (step‑by‑step)\n",
      "\n",
      "1. **Choose a model** – In the console you’ll see a list like:  \n",
      "   - **Anthropic Claude‑2** (chat‑style, strong reasoning)  \n",
      "   - **AI21 Jurassic‑2** (high‑capacity text generation)  \n",
      "   - **Stability Diffusion** (image generation)  \n",
      "   - **Amazon Titan** (AWS‑built LLMs, both text‑generation and embeddings)  \n",
      "\n",
      "2. **Create an “model endpoint”** – This is a lightweight, serverless endpoint that automatically scales. You can set a **provisioned concurrency** limit if you need guaranteed latency.\n",
      "\n",
      "3. **Call the API** – A simple HTTPS POST with your prompt (or image‑to‑image request) returns the model’s output. The request format is similar to OpenAI’s `chat/completions` endpoint, so many existing SDKs can be reused.\n",
      "\n",
      "4. **(Optional) Fine‑tune** – Upload a CSV/JSONL of prompt‑completion pairs to an S3 bucket, point Bedrock to it, and launch a fine‑tuning job. The resulting model is stored as a new version you can call just like any other endpoint.\n",
      "\n",
      "5. **Consume the result** – Pipe the response into downstream services: store in DynamoDB, trigger a Step Functions workflow, or send back to a web UI.\n",
      "\n",
      "---\n",
      "\n",
      "## Why people love it\n",
      "\n",
      "| Use case | Example |\n",
      "|----------|---------|\n",
      "| **Customer‑support bots** | Use Claude‑2 for conversational handling, then route to a Lambda that looks up order info in Aurora. |\n",
      "| **Content creation** | Generate marketing copy with Titan‑Text, then run it through a Grammarly‑style quality‑check Lambda. |\n",
      "| **Code assistance** | Use Titan‑Code (a code‑focused LLM) to autocomplete snippets inside an internal IDE plugin. |\n",
      "| **Image generation** | Produce product mock‑ups on‑the‑fly with Stability Diffusion, store the PNGs in S3, and serve them via CloudFront. |\n",
      "| **Semantic search** | Create embeddings with Titan‑Embedding, index them in OpenSearch, and power a “search‑by‑meaning” feature for a knowledge base. |\n",
      "\n",
      "---\n",
      "\n",
      "## Pricing snapshot (as of 2024‑Q3)\n",
      "\n",
      "| Model family | Cost per 1 K tokens (input) | Cost per 1 K tokens (output) |\n",
      "|--------------|----------------------------|------------------------------|\n",
      "| **Titan‑Text** | $0.0004 | $0.0012 |\n",
      "| **Claude‑2** | $0.0010 | $0.0030 |\n",
      "| **Jurassic‑2** | $0.0015 | $0.0040 |\n",
      "| **Stability Diffusion** | $0.015 per 1 K pixel‑steps (≈ $0.12 per 512×512 image) | — |\n",
      "\n",
      "*Note:* There’s also a **$0.0001 per request** “infrastructure fee” for the endpoint call itself. Prices can vary by region and are subject to change, so always double‑check the AWS pricing page.\n",
      "\n",
      "---\n",
      "\n",
      "## Security & compliance highlights\n",
      "\n",
      "- **VPC‑endpoint support** – Keep traffic inside your private network.  \n",
      "- **IAM policies** – Fine‑grained permissions (e.g., `bedrock:InvokeModel`, `bedrock:CreateModelCustomizationJob`).  \n",
      "- **Data residency** – Choose a region (e.g., us‑east‑1, eu‑central‑1) and the model runs entirely there.  \n",
      "- **Auditability** – All calls are logged to CloudTrail; you can enable request‑level logging for debugging.  \n",
      "- **Compliance** – Bedrock inherits AWS’s certifications (SOC 2, ISO 27001, GDPR, HIPAA‑eligible in certain regions).\n",
      "\n",
      "---\n",
      "\n",
      "## Quick “Hello‑World” code snippet (Python)\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "import json\n",
      "\n",
      "# 1️⃣ Create a Bedrock client (make sure your IAM role has bedrock:InvokeModel)\n",
      "client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
      "\n",
      "# 2️⃣ Choose a model – here we use Titan Text\n",
      "model_id = \"amazon.titan-text-premier-v1:0\"\n",
      "\n",
      "# 3️⃣ Build the request payload\n",
      "prompt = \"Write a short, witty tagline for a coffee shop called 'Bean There, Done That'.\"\n",
      "payload = {\n",
      "    \"inputText\": prompt,\n",
      "    \"textGenerationConfig\": {\n",
      "        \"maxTokenCount\": 64,\n",
      "        \"temperature\": 0.7,\n",
      "        \"topP\": 0.9\n",
      "    }\n",
      "}\n",
      "\n",
      "# 4️⃣ Call the model\n",
      "response = client.invoke_model(\n",
      "    body=json.dumps(payload),\n",
      "    modelId=model_id,\n",
      "    contentType=\"application/json\"\n",
      ")\n",
      "\n",
      "# 5️⃣ Parse the result\n",
      "result = json.loads(response[\"body\"].read())\n",
      "print(\"Generated tagline:\", result[\"results\"][0][\"outputText\"])\n",
      "```\n",
      "\n",
      "Running that script (with proper AWS credentials) will give you something like:\n",
      "\n",
      "```\n",
      "Generated tagline: “Fuel your day, one bold sip at a time.”\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Getting started checklist\n",
      "\n",
      "1. **Create an AWS account** (or use an existing one).  \n",
      "2. **Enable Bedrock** in the console – you may need to request access if you’re in a new region.  \n",
      "3. **Set up IAM** – give yourself `bedrock:*` permissions or a tighter policy.  \n",
      "4. **Pick a model** and spin up an endpoint (the console wizard walks you through it).  \n",
      "5. **Try the sample code** (the SDK is in `boto3` for Python, `aws-sdk` for JavaScript/TypeScript, etc.).  \n",
      "6. **Monitor** – use CloudWatch metrics (`ModelInvocationCount`, `ModelLatency`) to keep an eye on cost and performance.  \n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "AWS Bedrock = **a serverless, secure, pay‑as‑you‑go gateway to the world’s biggest foundation models**, all baked into the AWS ecosystem. It lets you generate text, images, embeddings, or even fine‑tune a model with just a few clicks or a single API call, while AWS handles the heavy lifting of scaling, security, and compliance.\n",
      "\n",
      "If you have a specific project in mind—chatbot, content generator, image creator, semantic search, or anything else—just let me know and we can sketch out a concrete architecture using Bedrock! \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is AWS Bedrock?\n",
      "AI: **AWS Bedrock** is Amazon Web Services’ fully‑managed platform for building and running **generative‑AI applications** with large foundation models (LLMs, diffusion models, embedding models, etc.) — all without having to spin up your own GPU clusters or manage the underlying infrastructure.\n",
      "\n",
      "---\n",
      "\n",
      "## The big picture\n",
      "\n",
      "| Piece | What it means for you |\n",
      "|------|------------------------|\n",
      "| **Managed service** | AWS runs the hardware, scaling, patching, and security. You just call an API. |\n",
      "| **Foundation‑model hub** | A catalog of pre‑trained models from both AWS (e.g., **Titan** series) and leading AI startups (Anthropic, AI21, Stability AI, Cohere). |\n",
      "| **Pay‑as‑you‑go** | You’re billed per token (for text) or per image generated, plus a small per‑request fee for the API. No upfront commitments. |\n",
      "| **Fine‑tuning & customisation** | You can create a “personalised” version of a model by feeding it your own data (via a simple UI or API). |\n",
      "| **Secure & compliant** | Data never leaves your VPC unless you explicitly allow it; encryption at rest/in‑flight, IAM‑based access control, and integration with AWS KMS, CloudTrail, etc. |\n",
      "| **Integrated with the rest of AWS** | Easy to pipe Bedrock output into Lambda, SageMaker, Step Functions, DynamoDB, S3, or even a front‑end hosted on Amplify. |\n",
      "\n",
      "---\n",
      "\n",
      "## How it works (step‑by‑step)\n",
      "\n",
      "1. **Choose a model** – In the console you’ll see a list like:  \n",
      "   - **Anthropic Claude‑2** (chat‑style, strong reasoning)  \n",
      "   - **AI21 Jurassic‑2** (high‑capacity text generation)  \n",
      "   - **Stability Diffusion** (image generation)  \n",
      "   - **Amazon Titan** (AWS‑built LLMs, both text‑generation and embeddings)  \n",
      "\n",
      "2. **Create an “model endpoint”** – This is a lightweight, serverless endpoint that automatically scales. You can set a **provisioned concurrency** limit if you need guaranteed latency.\n",
      "\n",
      "3. **Call the API** – A simple HTTPS POST with your prompt (or image‑to‑image request) returns the model’s output. The request format is similar to OpenAI’s `chat/completions` endpoint, so many existing SDKs can be reused.\n",
      "\n",
      "4. **(Optional) Fine‑tune** – Upload a CSV/JSONL of prompt‑completion pairs to an S3 bucket, point Bedrock to it, and launch a fine‑tuning job. The resulting model is stored as a new version you can call just like any other endpoint.\n",
      "\n",
      "5. **Consume the result** – Pipe the response into downstream services: store in DynamoDB, trigger a Step Functions workflow, or send back to a web UI.\n",
      "\n",
      "---\n",
      "\n",
      "## Why people love it\n",
      "\n",
      "| Use case | Example |\n",
      "|----------|---------|\n",
      "| **Customer‑support bots** | Use Claude‑2 for conversational handling, then route to a Lambda that looks up order info in Aurora. |\n",
      "| **Content creation** | Generate marketing copy with Titan‑Text, then run it through a Grammarly‑style quality‑check Lambda. |\n",
      "| **Code assistance** | Use Titan‑Code (a code‑focused LLM) to autocomplete snippets inside an internal IDE plugin. |\n",
      "| **Image generation** | Produce product mock‑ups on‑the‑fly with Stability Diffusion, store the PNGs in S3, and serve them via CloudFront. |\n",
      "| **Semantic search** | Create embeddings with Titan‑Embedding, index them in OpenSearch, and power a “search‑by‑meaning” feature for a knowledge base. |\n",
      "\n",
      "---\n",
      "\n",
      "## Pricing snapshot (as of 2024‑Q3)\n",
      "\n",
      "| Model family | Cost per 1 K tokens (input) | Cost per 1 K tokens (output) |\n",
      "|--------------|----------------------------|------------------------------|\n",
      "| **Titan‑Text** | $0.0004 | $0.0012 |\n",
      "| **Claude‑2** | $0.0010 | $0.0030 |\n",
      "| **Jurassic‑2** | $0.0015 | $0.0040 |\n",
      "| **Stability Diffusion** | $0.015 per 1 K pixel‑steps (≈ $0.12 per 512×512 image) | — |\n",
      "\n",
      "*Note:* There’s also a **$0.0001 per request** “infrastructure fee” for the endpoint call itself. Prices can vary by region and are subject to change, so always double‑check the AWS pricing page.\n",
      "\n",
      "---\n",
      "\n",
      "## Security & compliance highlights\n",
      "\n",
      "- **VPC‑endpoint support** – Keep traffic inside your private network.  \n",
      "- **IAM policies** – Fine‑grained permissions (e.g., `bedrock:InvokeModel`, `bedrock:CreateModelCustomizationJob`).  \n",
      "- **Data residency** – Choose a region (e.g., us‑east‑1, eu‑central‑1) and the model runs entirely there.  \n",
      "- **Auditability** – All calls are logged to CloudTrail; you can enable request‑level logging for debugging.  \n",
      "- **Compliance** – Bedrock inherits AWS’s certifications (SOC 2, ISO 27001, GDPR, HIPAA‑eligible in certain regions).\n",
      "\n",
      "---\n",
      "\n",
      "## Quick “Hello‑World” code snippet (Python)\n",
      "\n",
      "```python\n",
      "import boto3\n",
      "import json\n",
      "\n",
      "# 1️⃣ Create a Bedrock client (make sure your IAM role has bedrock:InvokeModel)\n",
      "client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
      "\n",
      "# 2️⃣ Choose a model – here we use Titan Text\n",
      "model_id = \"amazon.titan-text-premier-v1:0\"\n",
      "\n",
      "# 3️⃣ Build the request payload\n",
      "prompt = \"Write a short, witty tagline for a coffee shop called 'Bean There, Done That'.\"\n",
      "payload = {\n",
      "    \"inputText\": prompt,\n",
      "    \"textGenerationConfig\": {\n",
      "        \"maxTokenCount\": 64,\n",
      "        \"temperature\": 0.7,\n",
      "        \"topP\": 0.9\n",
      "    }\n",
      "}\n",
      "\n",
      "# 4️⃣ Call the model\n",
      "response = client.invoke_model(\n",
      "    body=json.dumps(payload),\n",
      "    modelId=model_id,\n",
      "    contentType=\"application/json\"\n",
      ")\n",
      "\n",
      "# 5️⃣ Parse the result\n",
      "result = json.loads(response[\"body\"].read())\n",
      "print(\"Generated tagline:\", result[\"results\"][0][\"outputText\"])\n",
      "```\n",
      "\n",
      "Running that script (with proper AWS credentials) will give you something like:\n",
      "\n",
      "```\n",
      "Generated tagline: “Fuel your day, one bold sip at a time.”\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Getting started checklist\n",
      "\n",
      "1. **Create an AWS account** (or use an existing one).  \n",
      "2. **Enable Bedrock** in the console – you may need to request access if you’re in a new region.  \n",
      "3. **Set up IAM** – give yourself `bedrock:*` permissions or a tighter policy.  \n",
      "4. **Pick a model** and spin up an endpoint (the console wizard walks you through it).  \n",
      "5. **Try the sample code** (the SDK is in `boto3` for Python, `aws-sdk` for JavaScript/TypeScript, etc.).  \n",
      "6. **Monitor** – use CloudWatch metrics (`ModelInvocationCount`, `ModelLatency`) to keep an eye on cost and performance.  \n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "AWS Bedrock = **a serverless, secure, pay‑as‑you‑go gateway to the world’s biggest foundation models**, all baked into the AWS ecosystem. It lets you generate text, images, embeddings, or even fine‑tune a model with just a few clicks or a single API call, while AWS handles the heavy lifting of scaling, security, and compliance.\n",
      "\n",
      "If you have a specific project in mind—chatbot, content generator, image creator, semantic search, or anything else—just let me know and we can sketch out a concrete architecture using Bedrock!\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I’m sorry, but I don’t have any information about your name.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(temperature=0.0, model=\"openai/gpt-oss-120b\")\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(conversation.predict(input=\"Hi, my name is Fredrick\"))\n",
    "print(conversation.predict(input=\"What is AWS Bedrock?\"), \"\\n\")\n",
    "\n",
    "print(conversation.predict(input=\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3188f",
   "metadata": {},
   "source": [
    "**ConversationTokenBufferMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "436dba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_groq.chat_models import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffb1126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\", temperature=0.0)\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?\"}, {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5587a3e",
   "metadata": {},
   "source": [
    "**ConversationSummaryBufferMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d40eed34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human greets the AI and says they’re just hanging around; the AI responds casually. The human then asks about today’s schedule, and the AI details a 8 a.m. meeting with the product team (requiring a PowerPoint), work on ONNX, AWS Bedrock, and Glue projects from 9 a.m. to noon, followed by a noon lunch at a Russian restaurant with a distant customer to discuss the latest AI, reminding the human to bring a laptop for an ONNX demo.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "schedule = \"There is a meeting at 8am with your product team. You will need your powerpoint presentation perpared. \\\n",
    "9am-12pm have time to work on your ONNX Project, AWS Bedrock and Glue projects which will go quickly because these are such powerful tools. \\\n",
    "At Noon, lunch at the Russian restaurant with a customer who is driving from over an hour away to meet you to \\\n",
    "understand the latest in AI. Be sure to bring your laptop to show the latest ONNX demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"Whats up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})\n",
    "\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24636900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and says they’re just hanging around; the AI responds casually. The human then asks about today’s schedule, and the AI details a 8 a.m. meeting with the product team (requiring a PowerPoint), work on ONNX, AWS Bedrock, and Glue projects from 9 a.m. to noon, followed by a noon lunch at a Russian restaurant with a distant customer to discuss the latest AI, reminding the human to bring a laptop for an ONNX demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hey there! Since you’ve got a packed morning and a lunch‑time demo in front of a customer, I’d suggest a **“One‑Click End‑to‑End AI Pipeline”** that ties together everything on your agenda:\n",
      "\n",
      "---\n",
      "\n",
      "## 🎯 Goal of the Demo  \n",
      "Show how a **PyTorch model → ONNX export → AWS Bedrock inference → AWS Glue data‑prep** pipeline can be built, deployed, and run in under 5 minutes on a laptop.  \n",
      "It hits three sweet spots for the customer:\n",
      "\n",
      "1. **Portability** – the ONNX model runs locally (no cloud‑dependency for the first run).  \n",
      "2. **Scalability** – the same model can be swapped to Bedrock for production‑grade latency and cost‑tracking.  \n",
      "3. **Data hygiene** – Glue demonstrates how you’d keep the training data fresh and versioned.\n",
      "\n",
      "---\n",
      "\n",
      "## 📋 Demo Outline (≈ 5 min)\n",
      "\n",
      "| Time | What you’ll show | Key Talking Points |\n",
      "|------|------------------|--------------------|\n",
      "| **0:00‑0:30** | **Slide 1 (quick 30‑sec PowerPoint)** – “From prototype to production in 3 steps” | Mention the 8 a.m. product‑team meeting and how today’s demo is the concrete example. |\n",
      "| **0:30‑1:30** | **Step 1 – Load & run an ONNX model locally** (image classification) | • Show the model file (`resnet50.onnx`). <br>• Run a single inference with `onnxruntime`. <br>• Highlight < 10 ms latency on a laptop GPU/CPU. |\n",
      "| **1:30‑2:30** | **Step 2 – Push the same model to AWS Bedrock** (via SageMaker Model Registry) | • One‑liner `boto3` call to register the ONNX artifact. <br>• Invoke Bedrock’s `InvokeModel` API and compare latency (≈ 30 ms). <br>• Show cost‑estimate panel (Bedrock pricing). |\n",
      "| **2:30‑3:30** | **Step 3 – Glue ETL to prep new images** | • Quick Glue job script (Python shell) that pulls new images from an S3 bucket, resizes them, and writes a manifest file. <br>• Explain how Glue can keep the training set fresh without touching the model code. |\n",
      "| **3:30‑4:30** | **Live “What‑If”** – Drop a new image into S3, re‑run the Glue job, and hit Bedrock again | • Demonstrates the **closed‑loop**: data → ETL → inference. |\n",
      "| **4:30‑5:00** | **Wrap‑up slide** – Benefits, next steps, Q&A | • “Zero‑code migration from edge to cloud”, “Unified monitoring via CloudWatch”, “Future: fine‑tune on Bedrock”. |\n",
      "\n",
      "---\n",
      "\n",
      "## 🛠️ What You Need on the Laptop\n",
      "\n",
      "| Component | Version (tested) | Install Command |\n",
      "|-----------|------------------|-----------------|\n",
      "| **Python** | 3.10+ | `python -m venv demo_env && source demo_env/bin/activate` |\n",
      "| **onnxruntime** | 1.18.0 | `pip install onnxruntime` |\n",
      "| **boto3** | 1.34.0 | `pip install boto3` |\n",
      "| **Pillow** (image handling) | 10.2.0 | `pip install pillow` |\n",
      "| **Streamlit** (optional UI) | 1.38.0 | `pip install streamlit` |\n",
      "| **AWS CLI** (configured with your IAM role) | latest | `pip install awscli && aws configure` |\n",
      "\n",
      "> **Tip:** Run the whole script once tonight (or during a coffee break) to make sure the Bedrock endpoint is reachable from the restaurant’s Wi‑Fi. If the network is flaky, you can fall back to the pure‑ONNX local inference – it still looks impressive!\n",
      "\n",
      "---\n",
      "\n",
      "## 📂 Minimal Working Script (Python)\n",
      "\n",
      "```python\n",
      "import os\n",
      "import json\n",
      "import boto3\n",
      "import onnxruntime as ort\n",
      "from PIL import Image\n",
      "import numpy as np\n",
      "\n",
      "# ---------- 1️⃣ Load ONNX locally ----------\n",
      "def preprocess(img_path):\n",
      "    img = Image.open(img_path).resize((224, 224)).convert(\"RGB\")\n",
      "    arr = np.array(img).astype('float32')\n",
      "    # Normalise like ImageNet\n",
      "    arr = (arr / 255.0 - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
      "    arr = arr.transpose(2, 0, 1)  # C,H,W\n",
      "    return arr[np.newaxis, ...]\n",
      "\n",
      "session = ort.InferenceSession(\"resnet50.onnx\")\n",
      "def infer_local(img_path):\n",
      "    input_name = session.get_inputs()[0].name\n",
      "    out = session.run(None, {input_name: preprocess(img_path)})[0]\n",
      "    return np.argmax(out, axis=1)[0]\n",
      "\n",
      "# ---------- 2️⃣ Bedrock call ----------\n",
      "bedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
      "def infer_bedrock(img_path):\n",
      "    # Convert image to base64 for payload\n",
      "    with open(img_path, \"rb\") as f:\n",
      "        img_bytes = f.read()\n",
      "    payload = {\n",
      "        \"inputs\": [{\"image\": {\"bytes\": img_bytes.decode('latin1')}}],\n",
      "        \"parameters\": {}\n",
      "    }\n",
      "    response = bedrock.invoke_model(\n",
      "        body=json.dumps(payload),\n",
      "        modelId=\"amazon.titan-image-generator-v1\",   # replace with your ONNX‑registered model ARN\n",
      "        contentType=\"application/json\"\n",
      "    )\n",
      "    result = json.loads(response[\"body\"].read())\n",
      "    return result[\"predictions\"][0][\"label\"]\n",
      "\n",
      "# ---------- 3️⃣ Glue job (pseudo‑code) ----------\n",
      "# In the real job you’d use AWS Glue ETL libraries; here’s a quick illustration:\n",
      "def glue_prep():\n",
      "    import boto3\n",
      "    s3 = boto3.client(\"s3\")\n",
      "    # List new objects in bucket `demo-input/`\n",
      "    objs = s3.list_objects_v2(Bucket=\"my-demo-bucket\", Prefix=\"demo-input/\")[\"Contents\"]\n",
      "    for obj in objs:\n",
      "        # download, resize, write to `demo-prepped/`\n",
      "        # (omitted for brevity – you can use the same `preprocess` function)\n",
      "        pass\n",
      "    print(\"Glue ETL finished – manifest ready for next inference.\")\n",
      "\n",
      "# ---------- Demo driver ----------\n",
      "if __name__ == \"__main__\":\n",
      "    test_img = \"sample.jpg\"   # bring this on a USB stick or have it in the repo\n",
      "    print(\"🔹 Local ONNX prediction:\", infer_local(test_img))\n",
      "    print(\"🔹 Bedrock prediction:\", infer_bedrock(test_img))\n",
      "    # Uncomment to run the Glue stub\n",
      "    # glue_prep()\n",
      "```\n",
      "\n",
      "*You can spin this up in a **Streamlit** app in < 2 minutes if you want a clickable UI for the lunch demo.*\n",
      "\n",
      "---\n",
      "\n",
      "## 🍽️ Lunch‑Room Logistics\n",
      "\n",
      "| Item | Why it matters |\n",
      "|------|----------------|\n",
      "| **Laptop + charger** | The restaurant’s power strips are often hidden behind the bar. |\n",
      "| **USB‑C to HDMI adapter** (or a portable 13‑inch monitor) | Gives the customer a bigger view of the inference latency chart. |\n",
      "| **Printed cheat‑sheet** (A5) | Quick reference for the three steps; looks professional. |\n",
      "| **Sample images** (a few cats, a car, a Russian‑cuisine dish) | Ties the demo to the restaurant vibe – “Here’s how the model sees your borscht!” |\n",
      "| **Headphones (optional)** | If the restaurant gets noisy, you can mute the audio from the Streamlit UI. |\n",
      "\n",
      "---\n",
      "\n",
      "## 📊 What to Highlight to the Customer\n",
      "\n",
      "1. **Speed** – Local ONNX: ~8 ms; Bedrock (single‑region): ~30 ms.  \n",
      "2. **Cost** – Bedrock inference is billed per 1 k tokens (or per request for vision models); show a quick cost calculator slide.  \n",
      "3. **Flexibility** – Same `.onnx` file works everywhere – edge device, SageMaker, Bedrock, or even Azure.  \n",
      "4. **Data freshness** – Glue can run on a schedule (e.g., every hour) to ingest new images, so the model never gets stale.  \n",
      "5. **Future‑proof** – You can fine‑tune the Bedrock‑hosted model with your own data without moving the ONNX file again.\n",
      "\n",
      "---\n",
      "\n",
      "## 🚀 Quick “Night‑Before” Checklist\n",
      "\n",
      "1. **Run the script end‑to‑end** (local → Bedrock).  \n",
      "2. **Take a screenshot** of the latency numbers and drop it into the PowerPoint slide you’ll use at 8 a.m.  \n",
      "3. **Copy `sample.jpg`** to a USB stick (or keep it in the repo).  \n",
      "4. **Charge the laptop** to 100 % and pack the charger.  \n",
      "5. **Print the cheat‑sheet** (or have it as a PDF on your phone).  \n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "> **Demo:** “One‑Click End‑to‑End AI Pipeline” – load a pre‑exported ResNet‑50 ONNX model, run it locally, then invoke the same model on AWS Bedrock, and finally show a Glue ETL job that prepares fresh data for the next inference.  \n",
      "> **Why it works:** It’s visual, fast, and demonstrates the exact workflow you’ll be talking about in the 8 a.m. meeting and the lunch conversation.  \n",
      "\n",
      "Good luck at the Russian restaurant – bring that laptop, a smile, and a few tasty images, and you’ll have a demo that’s both technically solid and memorable! 🎉 If you need a deeper dive into any of the code snippets or want a ready‑made Streamlit UI, just let me know and I’ll send it over.\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "print(conversation.predict(input=\"What would be a good demo to show?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "405e8cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'System: The human greets the AI and says they’re just hanging '\n",
      "            'around; the AI responds casually. The human then asks about '\n",
      "            'today’s schedule, and the AI details a morning that starts with '\n",
      "            'an 8\\u202fa.m. product‑team meeting (requiring a PowerPoint), '\n",
      "            'followed by work on ONNX, AWS\\u202fBedrock, and Glue projects '\n",
      "            'from 9\\u202fa.m. to noon, and then a noon lunch at a Russian '\n",
      "            'restaurant with a distant customer to discuss the latest AI, '\n",
      "            'reminding the human to bring a laptop for an ONNX demo. When the '\n",
      "            'human asks what demo would be good, the AI proposes a '\n",
      "            '**“One‑Click End‑to‑End AI Pipeline”** demo: load a pre‑exported '\n",
      "            'PyTorch model (ResNet‑50) as an ONNX file, run a quick local '\n",
      "            'inference with\\u202fonnxruntime, then register the same ONNX '\n",
      "            'model in AWS\\u202fBedrock and invoke it via\\u202fboto3 to show '\n",
      "            'cloud latency and cost, and finally run a short AWS\\u202fGlue ETL '\n",
      "            'job that pulls new images from S3, preprocesses them, and updates '\n",
      "            'a manifest for the next inference. The AI supplies a 5‑minute '\n",
      "            'demo outline (slide intro, local inference, Bedrock call, Glue '\n",
      "            'job, live “what‑if” refresh, wrap‑up), a list of required laptop '\n",
      "            'packages (Python\\u202f3.10+, onnxruntime, boto3, Pillow, optional '\n",
      "            'Streamlit, AWS\\u202fCLI), a minimal working Python script, '\n",
      "            'logistics tips (charger, HDMI adapter, cheat‑sheet, sample '\n",
      "            'images, headphones), key talking points (speed, cost, '\n",
      "            'portability, data freshness, future fine‑tuning), and a '\n",
      "            'night‑before checklist to ensure the demo runs smoothly at the '\n",
      "            'restaurant.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.load_memory_variables({}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
