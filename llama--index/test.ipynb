{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73cc257a",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b8b63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 15:36:33,033 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loading files: 100%|██████████| 6/6 [00:05<00:00,  1.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='70769b61-2ceb-4a4a-8b2a-36b43953c6b1', embedding=None, metadata={'page_label': '1', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='You Only Look Once:\\nUniﬁed, Real-Time Object Detection\\nJoseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\\nUniversity of Washington∗, Allen Institute for AI†, Facebook AI Research¶\\nhttp://pjreddie.com/yolo/\\nAbstract\\nWe present YOLO, a new approach to object detection.\\nPrior work on object detection repurposes classiﬁers to per-\\nform detection. Instead, we frame object detection as a re-\\ngression problem to spatially separated bounding boxes and\\nassociated class probabilities. A single neural network pre-\\ndicts bounding boxes and class probabilities directly from\\nfull images in one evaluation. Since the whole detection\\npipeline is a single network, it can be optimized end-to-end\\ndirectly on detection performance.\\nOur uniﬁed architecture is extremely fast. Our base\\nYOLO model processes images in real-time at 45 frames\\nper second. A smaller version of the network, Fast YOLO,\\nprocesses an astounding 155 frames per second while\\nstill achieving double the mAP of other real-time detec-\\ntors. Compared to state-of-the-art detection systems, YOLO\\nmakes more localization errors but is less likely to predict\\nfalse positives on background. Finally, YOLO learns very\\ngeneral representations of objects. It outperforms other de-\\ntection methods, including DPM and R-CNN, when gener-\\nalizing from natural images to other domains like artwork.\\n1. Introduction\\nHumans glance at an image and instantly know what ob-\\njects are in the image, where they are, and how they inter-\\nact. The human visual system is fast and accurate, allow-\\ning us to perform complex tasks like driving with little con-\\nscious thought. Fast, accurate algorithms for object detec-\\ntion would allow computers to drive cars without special-\\nized sensors, enable assistive devices to convey real-time\\nscene information to human users, and unlock the potential\\nfor general purpose, responsive robotic systems.\\nCurrent detection systems repurpose classiﬁers to per-\\nform detection. To detect an object, these systems take a\\nclassiﬁer for that object and evaluate it at various locations\\nand scales in a test image. Systems like deformable parts\\nmodels (DPM) use a sliding window approach where the\\nclassiﬁer is run at evenly spaced locations over the entire\\nimage [10].\\nMore recent approaches like R-CNN use region proposal\\n1. Resize image.\\n2. Run convolutional network.\\n3. Non-max suppression.\\nDog: 0.30\\nPerson: 0.64\\nHorse: 0.28\\nFigure 1: The YOLO Detection System. Processing images\\nwith YOLO is simple and straightforward. Our system (1) resizes\\nthe input image to 448 × 448, (2) runs a single convolutional net-\\nwork on the image, and (3) thresholds the resulting detections by\\nthe model’s conﬁdence.\\nmethods to ﬁrst generate potential bounding boxes in an im-\\nage and then run a classiﬁer on these proposed boxes. After\\nclassiﬁcation, post-processing is used to reﬁne the bound-\\ning boxes, eliminate duplicate detections, and rescore the\\nboxes based on other objects in the scene [13]. These com-\\nplex pipelines are slow and hard to optimize because each\\nindividual component must be trained separately.\\nWe reframe object detection as a single regression prob-\\nlem, straight from image pixels to bounding box coordi-\\nnates and class probabilities. Using our system, you only\\nlook once (YOLO) at an image to predict what objects are\\npresent and where they are.\\nYOLO is refreshingly simple: see Figure 1. A sin-\\ngle convolutional network simultaneously predicts multi-\\nple bounding boxes and class probabilities for those boxes.\\nYOLO trains on full images and directly optimizes detec-\\ntion performance. This uniﬁed model has several beneﬁts\\nover traditional methods of object detection.\\nFirst, YOLO is extremely fast. Since we frame detection\\nas a regression problem we don’t need a complex pipeline.\\nWe simply run our neural network on a new image at test\\ntime to predict detections. Our base network runs at 45\\nframes per second with no batch processing on a Titan X\\nGPU and a fast version runs at more than 150 fps. This\\nmeans we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO\\nachieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running\\nin real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/.\\nSecond, YOLO reasons globally about the image when\\n1\\narXiv:1506.02640v5  [cs.CV]  9 May 2016', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='31212c40-7fa9-4781-a4df-4c5bd6192c73', embedding=None, metadata={'page_label': '2', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='making predictions. Unlike sliding window and region\\nproposal-based techniques, YOLO sees the entire image\\nduring training and test time so it implicitly encodes contex-\\ntual information about classes as well as their appearance.\\nFast R-CNN, a top detection method [14], mistakes back-\\nground patches in an image for objects because it can’t see\\nthe larger context. YOLO makes less than half the number\\nof background errors compared to Fast R-CNN.\\nThird, YOLO learns generalizable representations of ob-\\njects. When trained on natural images and tested on art-\\nwork, YOLO outperforms top detection methods like DPM\\nand R-CNN by a wide margin. Since YOLO is highly gen-\\neralizable it is less likely to break down when applied to\\nnew domains or unexpected inputs.\\nYOLO still lags behind state-of-the-art detection systems\\nin accuracy. While it can quickly identify objects in im-\\nages it struggles to precisely localize some objects, espe-\\ncially small ones. We examine these tradeoffs further in our\\nexperiments.\\nAll of our training and testing code is open source. A\\nvariety of pretrained models are also available to download.\\n2. Uniﬁed Detection\\nWe unify the separate components of object detection\\ninto a single neural network. Our network uses features\\nfrom the entire image to predict each bounding box. It also\\npredicts all bounding boxes across all classes for an im-\\nage simultaneously. This means our network reasons glob-\\nally about the full image and all the objects in the image.\\nThe YOLO design enables end-to-end training and real-\\ntime speeds while maintaining high average precision.\\nOur system divides the input image into an S×S grid.\\nIf the center of an object falls into a grid cell, that grid cell\\nis responsible for detecting that object.\\nEach grid cell predictsBbounding boxes and conﬁdence\\nscores for those boxes. These conﬁdence scores reﬂect how\\nconﬁdent the model is that the box contains an object and\\nalso how accurate it thinks the box is that it predicts. For-\\nmally we deﬁne conﬁdence as Pr(Object) ∗IOUtruth\\npred . If no\\nobject exists in that cell, the conﬁdence scores should be\\nzero. Otherwise we want the conﬁdence score to equal the\\nintersection over union (IOU) between the predicted box\\nand the ground truth.\\nEach bounding box consists of 5 predictions: x, y, w, h,\\nand conﬁdence. The (x,y) coordinates represent the center\\nof the box relative to the bounds of the grid cell. The width\\nand height are predicted relative to the whole image. Finally\\nthe conﬁdence prediction represents the IOU between the\\npredicted box and any ground truth box.\\nEach grid cell also predicts C conditional class proba-\\nbilities, Pr(Classi|Object). These probabilities are condi-\\ntioned on the grid cell containing an object. We only predict\\none set of class probabilities per grid cell, regardless of the\\nnumber of boxes B.\\nAt test time we multiply the conditional class probabili-\\nties and the individual box conﬁdence predictions,\\nPr(Classi|Object) ∗Pr(Object) ∗IOUtruth\\npred = Pr(Classi) ∗IOUtruth\\npred (1)\\nwhich gives us class-speciﬁc conﬁdence scores for each\\nbox. These scores encode both the probability of that class\\nappearing in the box and how well the predicted box ﬁts the\\nobject.\\nS × S grid on input\\nBounding boxes + confidence\\nClass probability map\\nFinal detections\\nFigure 2: The Model. Our system models detection as a regres-\\nsion problem. It divides the image into an S × S grid and for each\\ngrid cell predicts B bounding boxes, conﬁdence for those boxes,\\nand C class probabilities. These predictions are encoded as an\\nS × S × (B ∗ 5 +C) tensor.\\nFor evaluating YOLO on PASCAL VOC, we use S = 7,\\nB = 2. P ASCAL VOC has 20 labelled classes so C = 20.\\nOur ﬁnal prediction is a 7 ×7 ×30 tensor.\\n2.1. Network Design\\nWe implement this model as a convolutional neural net-\\nwork and evaluate it on the PASCAL VOC detection dataset\\n[9]. The initial convolutional layers of the network extract\\nfeatures from the image while the fully connected layers\\npredict the output probabilities and coordinates.\\nOur network architecture is inspired by the GoogLeNet\\nmodel for image classiﬁcation [34]. Our network has 24\\nconvolutional layers followed by 2 fully connected layers.\\nInstead of the inception modules used by GoogLeNet, we\\nsimply use 1 ×1 reduction layers followed by 3 ×3 convo-\\nlutional layers, similar to Lin et al [22]. The full network is\\nshown in Figure 3.\\nWe also train a fast version of YOLO designed to push\\nthe boundaries of fast object detection. Fast YOLO uses a\\nneural network with fewer convolutional layers (9 instead\\nof 24) and fewer ﬁlters in those layers. Other than the size\\nof the network, all training and testing parameters are the\\nsame between YOLO and Fast YOLO.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='571525b0-3f0b-4872-87d2-b7785b78fb59', embedding=None, metadata={'page_label': '3', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='448\\n448\\n3\\n7\\n7\\nConv. Layer\\n7x7x64-s-2\\nMaxpool Layer\\n2x2-s-2\\n3\\n3\\n112\\n112\\n192\\n3\\n3\\n56\\n56\\n256\\nConn. Layer\\n4096\\nConn. LayerConv. Layer\\n3x3x192\\nMaxpool Layer\\n2x2-s-2\\nConv. Layers\\n1x1x128\\n3x3x256\\n1x1x256\\n3x3x512\\nMaxpool Layer\\n2x2-s-2\\n3\\n3\\n28\\n28\\n512\\nConv. Layers\\n1x1x256\\n3x3x512\\n1x1x512\\n3x3x1024\\nMaxpool Layer\\n2x2-s-2\\n3\\n3\\n14\\n14\\n1024\\nConv. Layers\\n1x1x512\\n3x3x1024\\n3x3x1024\\n3x3x1024-s-2\\n3\\n3\\n7\\n7\\n1024\\n7\\n7\\n1024\\n7\\n7\\n30\\n} ×4 } ×2\\nConv. Layers\\n3x3x1024\\n3x3x1024\\nFigure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1 × 1\\nconvolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classiﬁcation\\ntask at half the resolution (224 × 224 input image) and then double the resolution for detection.\\nThe ﬁnal output of our network is the 7 ×7 ×30 tensor\\nof predictions.\\n2.2. Training\\nWe pretrain our convolutional layers on the ImageNet\\n1000-class competition dataset [30]. For pretraining we use\\nthe ﬁrst 20 convolutional layers from Figure 3 followed by a\\naverage-pooling layer and a fully connected layer. We train\\nthis network for approximately a week and achieve a single\\ncrop top-5 accuracy of 88% on the ImageNet 2012 valida-\\ntion set, comparable to the GoogLeNet models in Caffe’s\\nModel Zoo [24]. We use the Darknet framework for all\\ntraining and inference [26].\\nWe then convert the model to perform detection. Ren et\\nal. show that adding both convolutional and connected lay-\\ners to pretrained networks can improve performance [29].\\nFollowing their example, we add four convolutional lay-\\ners and two fully connected layers with randomly initialized\\nweights. Detection often requires ﬁne-grained visual infor-\\nmation so we increase the input resolution of the network\\nfrom 224 ×224 to 448 ×448.\\nOur ﬁnal layer predicts both class probabilities and\\nbounding box coordinates. We normalize the bounding box\\nwidth and height by the image width and height so that they\\nfall between 0 and 1. We parametrize the bounding box x\\nand ycoordinates to be offsets of a particular grid cell loca-\\ntion so they are also bounded between 0 and 1.\\nWe use a linear activation function for the ﬁnal layer and\\nall other layers use the following leaky rectiﬁed linear acti-\\nvation:\\nφ(x) =\\n{\\nx, if x> 0\\n0.1x, otherwise (2)\\nWe optimize for sum-squared error in the output of our\\nmodel. We use sum-squared error because it is easy to op-\\ntimize, however it does not perfectly align with our goal of\\nmaximizing average precision. It weights localization er-\\nror equally with classiﬁcation error which may not be ideal.\\nAlso, in every image many grid cells do not contain any\\nobject. This pushes the “conﬁdence” scores of those cells\\ntowards zero, often overpowering the gradient from cells\\nthat do contain objects. This can lead to model instability,\\ncausing training to diverge early on.\\nTo remedy this, we increase the loss from bounding box\\ncoordinate predictions and decrease the loss from conﬁ-\\ndence predictions for boxes that don’t contain objects. We\\nuse two parameters, λcoord and λnoobj to accomplish this. We\\nset λcoord = 5and λnoobj = .5.\\nSum-squared error also equally weights errors in large\\nboxes and small boxes. Our error metric should reﬂect that\\nsmall deviations in large boxes matter less than in small\\nboxes. To partially address this we predict the square root\\nof the bounding box width and height instead of the width\\nand height directly.\\nYOLO predicts multiple bounding boxes per grid cell.\\nAt training time we only want one bounding box predictor\\nto be responsible for each object. We assign one predictor\\nto be “responsible” for predicting an object based on which\\nprediction has the highest current IOU with the ground\\ntruth. This leads to specialization between the bounding box\\npredictors. Each predictor gets better at predicting certain\\nsizes, aspect ratios, or classes of object, improving overall\\nrecall.\\nDuring training we optimize the following, multi-part', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e178e6ef-5468-417a-bd0d-e1daf75e8d66', embedding=None, metadata={'page_label': '4', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='loss function:\\nλcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n[\\n(xi −ˆxi)2 + (yi −ˆyi)2\\n]\\n+ λcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n[(√wi −\\n√\\nˆwi\\n)2\\n+\\n(√\\nhi −\\n√\\nˆhi\\n)2]\\n+\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 obj\\nij\\n(\\nCi −ˆCi\\n)2\\n+ λnoobj\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n1 noobj\\nij\\n(\\nCi −ˆCi\\n)2\\n+\\nS2\\n∑\\ni=0\\n1 obj\\ni\\n∑\\nc∈classes\\n(pi(c) −ˆpi(c))2 (3)\\nwhere 1 obj\\ni denotes if object appears in cell i and 1 obj\\nij de-\\nnotes that the jth bounding box predictor in cell i is “re-\\nsponsible” for that prediction.\\nNote that the loss function only penalizes classiﬁcation\\nerror if an object is present in that grid cell (hence the con-\\nditional class probability discussed earlier). It also only pe-\\nnalizes bounding box coordinate error if that predictor is\\n“responsible” for the ground truth box (i.e. has the highest\\nIOU of any predictor in that grid cell).\\nWe train the network for about 135 epochs on the train-\\ning and validation data sets from P ASCAL VOC 2007 and\\n2012. When testing on 2012 we also include the VOC 2007\\ntest data for training. Throughout training we use a batch\\nsize of 64, a momentum of 0.9 and a decay of 0.0005.\\nOur learning rate schedule is as follows: For the ﬁrst\\nepochs we slowly raise the learning rate from10−3 to 10−2.\\nIf we start at a high learning rate our model often diverges\\ndue to unstable gradients. We continue training with 10−2\\nfor 75 epochs, then 10−3 for 30 epochs, and ﬁnally 10−4\\nfor 30 epochs.\\nTo avoid overﬁtting we use dropout and extensive data\\naugmentation. A dropout layer with rate = .5 after the ﬁrst\\nconnected layer prevents co-adaptation between layers [18].\\nFor data augmentation we introduce random scaling and\\ntranslations of up to 20% of the original image size. We\\nalso randomly adjust the exposure and saturation of the im-\\nage by up to a factor of 1.5 in the HSV color space.\\n2.3. Inference\\nJust like in training, predicting detections for a test image\\nonly requires one network evaluation. On PASCAL VOC the\\nnetwork predicts 98 bounding boxes per image and class\\nprobabilities for each box. YOLO is extremely fast at test\\ntime since it only requires a single network evaluation, un-\\nlike classiﬁer-based methods.\\nThe grid design enforces spatial diversity in the bound-\\ning box predictions. Often it is clear which grid cell an\\nobject falls in to and the network only predicts one box for\\neach object. However, some large objects or objects near\\nthe border of multiple cells can be well localized by multi-\\nple cells. Non-maximal suppression can be used to ﬁx these\\nmultiple detections. While not critical to performance as it\\nis for R-CNN or DPM, non-maximal suppression adds 2-\\n3% in mAP.\\n2.4. Limitations of YOLO\\nYOLO imposes strong spatial constraints on bounding\\nbox predictions since each grid cell only predicts two boxes\\nand can only have one class. This spatial constraint lim-\\nits the number of nearby objects that our model can pre-\\ndict. Our model struggles with small objects that appear in\\ngroups, such as ﬂocks of birds.\\nSince our model learns to predict bounding boxes from\\ndata, it struggles to generalize to objects in new or unusual\\naspect ratios or conﬁgurations. Our model also uses rela-\\ntively coarse features for predicting bounding boxes since\\nour architecture has multiple downsampling layers from the\\ninput image.\\nFinally, while we train on a loss function that approxi-\\nmates detection performance, our loss function treats errors\\nthe same in small bounding boxes versus large bounding\\nboxes. A small error in a large box is generally benign but a\\nsmall error in a small box has a much greater effect on IOU.\\nOur main source of error is incorrect localizations.\\n3. Comparison to Other Detection Systems\\nObject detection is a core problem in computer vision.\\nDetection pipelines generally start by extracting a set of\\nrobust features from input images (Haar [25], SIFT [23],\\nHOG [4], convolutional features [6]). Then, classiﬁers\\n[36, 21, 13, 10] or localizers [1, 32] are used to identify\\nobjects in the feature space. These classiﬁers or localizers\\nare run either in sliding window fashion over the whole im-\\nage or on some subset of regions in the image [35, 15, 39].\\nWe compare the YOLO detection system to several top de-\\ntection frameworks, highlighting key similarities and differ-\\nences.\\nDeformable parts models. Deformable parts models\\n(DPM) use a sliding window approach to object detection\\n[10]. DPM uses a disjoint pipeline to extract static features,\\nclassify regions, predict bounding boxes for high scoring\\nregions, etc. Our system replaces all of these disparate parts\\nwith a single convolutional neural network. The network\\nperforms feature extraction, bounding box prediction, non-\\nmaximal suppression, and contextual reasoning all concur-\\nrently. Instead of static features, the network trains the fea-\\ntures in-line and optimizes them for the detection task. Our\\nuniﬁed architecture leads to a faster, more accurate model\\nthan DPM.\\nR-CNN. R-CNN and its variants use region proposals in-\\nstead of sliding windows to ﬁnd objects in images. Selective', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6ab3d5c7-4757-4176-a15c-3a4e91ef73ee', embedding=None, metadata={'page_label': '5', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Search [35] generates potential bounding boxes, a convolu-\\ntional network extracts features, an SVM scores the boxes, a\\nlinear model adjusts the bounding boxes, and non-max sup-\\npression eliminates duplicate detections. Each stage of this\\ncomplex pipeline must be precisely tuned independently\\nand the resulting system is very slow, taking more than 40\\nseconds per image at test time [14].\\nYOLO shares some similarities with R-CNN. Each grid\\ncell proposes potential bounding boxes and scores those\\nboxes using convolutional features. However, our system\\nputs spatial constraints on the grid cell proposals which\\nhelps mitigate multiple detections of the same object. Our\\nsystem also proposes far fewer bounding boxes, only 98\\nper image compared to about 2000 from Selective Search.\\nFinally, our system combines these individual components\\ninto a single, jointly optimized model.\\nOther Fast Detectors Fast and Faster R-CNN focus on\\nspeeding up the R-CNN framework by sharing computa-\\ntion and using neural networks to propose regions instead\\nof Selective Search [14] [28]. While they offer speed and\\naccuracy improvements over R-CNN, both still fall short of\\nreal-time performance.\\nMany research efforts focus on speeding up the DPM\\npipeline [31] [38] [5]. They speed up HOG computation,\\nuse cascades, and push computation to GPUs. However,\\nonly 30Hz DPM [31] actually runs in real-time.\\nInstead of trying to optimize individual components of\\na large detection pipeline, YOLO throws out the pipeline\\nentirely and is fast by design.\\nDetectors for single classes like faces or people can be\\nhighly optimized since they have to deal with much less\\nvariation [37]. YOLO is a general purpose detector that\\nlearns to detect a variety of objects simultaneously.\\nDeep MultiBox. Unlike R-CNN, Szegedy et al. train a\\nconvolutional neural network to predict regions of interest\\n[8] instead of using Selective Search. MultiBox can also\\nperform single object detection by replacing the conﬁdence\\nprediction with a single class prediction. However, Multi-\\nBox cannot perform general object detection and is still just\\na piece in a larger detection pipeline, requiring further im-\\nage patch classiﬁcation. Both YOLO and MultiBox use a\\nconvolutional network to predict bounding boxes in an im-\\nage but YOLO is a complete detection system.\\nOverFeat. Sermanet et al. train a convolutional neural\\nnetwork to perform localization and adapt that localizer to\\nperform detection [32]. OverFeat efﬁciently performs slid-\\ning window detection but it is still a disjoint system. Over-\\nFeat optimizes for localization, not detection performance.\\nLike DPM, the localizer only sees local information when\\nmaking a prediction. OverFeat cannot reason about global\\ncontext and thus requires signiﬁcant post-processing to pro-\\nduce coherent detections.\\nMultiGrasp. Our work is similar in design to work on\\ngrasp detection by Redmon et al [27]. Our grid approach to\\nbounding box prediction is based on the MultiGrasp system\\nfor regression to grasps. However, grasp detection is a much\\nsimpler task than object detection. MultiGrasp only needs\\nto predict a single graspable region for an image containing\\none object. It doesn’t have to estimate the size, location,\\nor boundaries of the object or predict it’s class, only ﬁnd a\\nregion suitable for grasping. YOLO predicts both bounding\\nboxes and class probabilities for multiple objects of multi-\\nple classes in an image.\\n4. Experiments\\nFirst we compare YOLO with other real-time detection\\nsystems on PASCAL VOC 2007. To understand the differ-\\nences between YOLO and R-CNN variants we explore the\\nerrors on VOC 2007 made by YOLO and Fast R-CNN, one\\nof the highest performing versions of R-CNN [14]. Based\\non the different error proﬁles we show that YOLO can be\\nused to rescore Fast R-CNN detections and reduce the er-\\nrors from background false positives, giving a signiﬁcant\\nperformance boost. We also present VOC 2012 results and\\ncompare mAP to current state-of-the-art methods. Finally,\\nwe show that YOLO generalizes to new domains better than\\nother detectors on two artwork datasets.\\n4.1. Comparison to Other Real-Time Systems\\nMany research efforts in object detection focus on mak-\\ning standard detection pipelines fast. [5] [38] [31] [14] [17]\\n[28] However, only Sadeghi et al. actually produce a de-\\ntection system that runs in real-time (30 frames per second\\nor better) [31]. We compare YOLO to their GPU imple-\\nmentation of DPM which runs either at 30Hz or 100Hz.\\nWhile the other efforts don’t reach the real-time milestone\\nwe also compare their relative mAP and speed to examine\\nthe accuracy-performance tradeoffs available in object de-\\ntection systems.\\nFast YOLO is the fastest object detection method on\\nPASCAL ; as far as we know, it is the fastest extant object\\ndetector. With 52.7% mAP, it is more than twice as accurate\\nas prior work on real-time detection. YOLO pushes mAP to\\n63.4% while still maintaining real-time performance.\\nWe also train YOLO using VGG-16. This model is more\\naccurate but also signiﬁcantly slower than YOLO. It is use-\\nful for comparison to other detection systems that rely on\\nVGG-16 but since it is slower than real-time the rest of the\\npaper focuses on our faster models.\\nFastest DPM effectively speeds up DPM without sacri-\\nﬁcing much mAP but it still misses real-time performance\\nby a factor of 2 [38]. It also is limited by DPM’s relatively\\nlow accuracy on detection compared to neural network ap-\\nproaches.\\nR-CNN minus R replaces Selective Search with static\\nbounding box proposals [20]. While it is much faster than', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7160b914-010a-42cc-9079-085b1e77c7a2', embedding=None, metadata={'page_label': '6', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Real-Time Detectors Train mAP FPS\\n100Hz DPM [31] 2007 16.0 100\\n30Hz DPM [31] 2007 26.1 30\\nFast YOLO 2007+2012 52.7 155\\nYOLO 2007+2012 63.4 45\\nLess Than Real-Time\\nFastest DPM [38] 2007 30.4 15\\nR-CNN Minus R [20] 2007 53.5 6\\nFast R-CNN [14] 2007+2012 70.0 0.5\\nFaster R-CNN VGG-16[28] 2007+2012 73.2 7\\nFaster R-CNN ZF [28] 2007+2012 62.1 18\\nYOLO VGG-16 2007+2012 66.4 21\\nTable 1: Real-Time Systems on PASCAL VOC 2007. Compar-\\ning the performance and speed of fast detectors. Fast YOLO is\\nthe fastest detector on record for P ASCAL VOC detection and is\\nstill twice as accurate as any other real-time detector. YOLO is\\n10 mAP more accurate than the fast version while still well above\\nreal-time in speed.\\nR-CNN, it still falls short of real-time and takes a signiﬁcant\\naccuracy hit from not having good proposals.\\nFast R-CNN speeds up the classiﬁcation stage of R-CNN\\nbut it still relies on selective search which can take around\\n2 seconds per image to generate bounding box proposals.\\nThus it has high mAP but at 0.5 fps it is still far from real-\\ntime.\\nThe recent Faster R-CNN replaces selective search with\\na neural network to propose bounding boxes, similar to\\nSzegedy et al. [8] In our tests, their most accurate model\\nachieves 7 fps while a smaller, less accurate one runs at\\n18 fps. The VGG-16 version of Faster R-CNN is 10 mAP\\nhigher but is also 6 times slower than YOLO. The Zeiler-\\nFergus Faster R-CNN is only 2.5 times slower than YOLO\\nbut is also less accurate.\\n4.2. VOC 2007 Error Analysis\\nTo further examine the differences between YOLO and\\nstate-of-the-art detectors, we look at a detailed breakdown\\nof results on VOC 2007. We compare YOLO to Fast R-\\nCNN since Fast R-CNN is one of the highest performing\\ndetectors on PASCAL and it’s detections are publicly avail-\\nable.\\nWe use the methodology and tools of Hoiem et al. [19]\\nFor each category at test time we look at the top N predic-\\ntions for that category. Each prediction is either correct or\\nit is classiﬁed based on the type of error:\\n•Correct: correct class and IOU >.5\\n•Localization: correct class, .1 <IOU <.5\\n•Similar: class is similar, IOU >.1\\nCorrect: 71.6% Correct: 65.5%\\nLoc: 8.6%\\nSim: 4.3%\\nOther: 1.9%\\nBackground: 13.6%\\nLoc: 19.0%\\nSim: 6.75%\\nOther: 4.0%\\nBackground: 4.75%\\nFast R-CNN YOLO\\nFigure 4: Error Analysis: Fast R-CNN vs. YOLO These\\ncharts show the percentage of localization and background errors\\nin the top N detections for various categories (N = # objects in that\\ncategory).\\n•Other: class is wrong, IOU >.1\\n•Background: IOU <.1 for any object\\nFigure 4 shows the breakdown of each error type aver-\\naged across all 20 classes.\\nYOLO struggles to localize objects correctly. Localiza-\\ntion errors account for more of YOLO’s errors than all other\\nsources combined. Fast R-CNN makes much fewer local-\\nization errors but far more background errors. 13.6% of\\nit’s top detections are false positives that don’t contain any\\nobjects. Fast R-CNN is almost 3x more likely to predict\\nbackground detections than YOLO.\\n4.3. Combining Fast R-CNN and YOLO\\nYOLO makes far fewer background mistakes than Fast\\nR-CNN. By using YOLO to eliminate background detec-\\ntions from Fast R-CNN we get a signiﬁcant boost in perfor-\\nmance. For every bounding box that R-CNN predicts we\\ncheck to see if YOLO predicts a similar box. If it does, we\\ngive that prediction a boost based on the probability pre-\\ndicted by YOLO and the overlap between the two boxes.\\nThe best Fast R-CNN model achieves a mAP of 71.8%\\non the VOC 2007 test set. When combined with YOLO, its\\nmAP Combined Gain\\nFast R-CNN 71.8 - -\\nFast R-CNN (2007 data) 66.9 72.4 .6\\nFast R-CNN (VGG-M) 59.2 72.4 .6\\nFast R-CNN (CaffeNet) 57.1 72.1 .3\\nYOLO 63.4 75.0 3.2\\nTable 2: Model combination experiments on VOC 2007. We\\nexamine the effect of combining various models with the best ver-\\nsion of Fast R-CNN. Other versions of Fast R-CNN provide only\\na small beneﬁt while YOLO provides a signiﬁcant performance\\nboost.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='15b1ac24-9f51-4ead-9443-cd3d2eab0b56', embedding=None, metadata={'page_label': '7', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='VOC 2012 test mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike personplant sheep sofa train tv\\nMR CNN MORE DATA [11] 73.9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0\\nHyperNet VGG 71.4 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7\\nHyperNet SP 71.3 84.1 78.3 73.3 55.5 53.6 78.6 79.6 87.5 49.5 74.9 52.1 85.6 81.6 83.2 81.6 48.4 73.2 59.3 79.7 65.6\\nFast R-CNN + YOLO 70.7 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2\\nMR CNN S CNN [11] 70.7 85.0 79.6 71.5 55.3 57.7 76.0 73.9 84.6 50.5 74.3 61.7 85.5 79.9 81.7 76.4 41.0 69.0 61.2 77.7 72.1\\nFaster R-CNN [28] 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\\nDEEP ENS COCO 70.1 84.0 79.4 71.6 51.9 51.1 74.1 72.1 88.6 48.3 73.4 57.8 86.1 80.0 80.7 70.4 46.6 69.6 68.8 75.9 71.4\\nNoC [29] 68.8 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1\\nFast R-CNN [14] 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2\\nUMICH FGS STRUCT 66.4 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2\\nNUS NIN C2000 [7] 63.8 80.2 73.8 61.9 43.7 43.0 70.3 67.6 80.7 41.9 69.7 51.7 78.2 75.2 76.9 65.1 38.6 68.3 58.0 68.7 63.3\\nBabyLearning [7] 63.2 78.0 74.2 61.3 45.7 42.7 68.2 66.8 80.2 40.6 70.0 49.8 79.0 74.5 77.9 64.0 35.3 67.9 55.7 68.7 62.6\\nNUS NIN 62.4 77.9 73.1 62.6 39.5 43.3 69.1 66.4 78.9 39.1 68.1 50.0 77.2 71.3 76.1 64.7 38.4 66.9 56.2 66.9 62.7\\nR-CNN VGG BB [13] 62.4 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3\\nR-CNN VGG [13] 59.2 76.8 70.9 56.6 37.5 36.9 62.9 63.6 81.1 35.7 64.3 43.9 80.4 71.6 74.0 60.0 30.8 63.4 52.0 63.5 58.7\\nYOLO 57.9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8\\nFeature Edit [33] 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4 71.1 60.2 33.3 61.3 46.4 61.7 57.8\\nR-CNN BB [13] 53.3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\\nSDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7\\nR-CNN [13] 49.6 68.1 63.8 46.1 29.4 27.9 56.6 57.0 65.9 26.5 48.7 39.5 66.2 57.3 65.4 53.2 26.2 54.5 38.1 50.6 51.6\\nTable 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of\\nNovember 6th, 2015. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the\\nonly real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN.\\nmAP increases by 3.2% to 75.0%. We also tried combining\\nthe top Fast R-CNN model with several other versions of\\nFast R-CNN. Those ensembles produced small increases in\\nmAP between .3 and .6%, see Table 2 for details.\\nThe boost from YOLO is not simply a byproduct of\\nmodel ensembling since there is little beneﬁt from combin-\\ning different versions of Fast R-CNN. Rather, it is precisely\\nbecause YOLO makes different kinds of mistakes at test\\ntime that it is so effective at boosting Fast R-CNN’s per-\\nformance.\\nUnfortunately, this combination doesn’t beneﬁt from the\\nspeed of YOLO since we run each model seperately and\\nthen combine the results. However, since YOLO is so fast\\nit doesn’t add any signiﬁcant computational time compared\\nto Fast R-CNN.\\n4.4. VOC 2012 Results\\nOn the VOC 2012 test set, YOLO scores 57.9% mAP.\\nThis is lower than the current state of the art, closer to\\nthe original R-CNN using VGG-16, see Table 3. Our sys-\\ntem struggles with small objects compared to its closest\\ncompetitors. On categories like bottle, sheep, and\\ntv/monitor YOLO scores 8-10% lower than R-CNN or\\nFeature Edit. However, on other categories like cat and\\ntrain YOLO achieves higher performance.\\nOur combined Fast R-CNN + YOLO model is one of the\\nhighest performing detection methods. Fast R-CNN gets\\na 2.3% improvement from the combination with YOLO,\\nboosting it 5 spots up on the public leaderboard.\\n4.5. Generalizability: Person Detection in Artwork\\nAcademic datasets for object detection draw the training\\nand testing data from the same distribution. In real-world\\napplications it is hard to predict all possible use cases and\\nthe test data can diverge from what the system has seen be-\\nfore [3]. We compare YOLO to other detection systems on\\nthe Picasso Dataset [12] and the People-Art Dataset [3], two\\ndatasets for testing person detection on artwork.\\nFigure 5 shows comparative performance between\\nYOLO and other detection methods. For reference, we give\\nVOC 2007 detection AP on person where all models are\\ntrained only on VOC 2007 data. On Picasso models are\\ntrained on VOC 2012 while on People-Art they are trained\\non VOC 2010.\\nR-CNN has high AP on VOC 2007. However, R-CNN\\ndrops off considerably when applied to artwork. R-CNN\\nuses Selective Search for bounding box proposals which is\\ntuned for natural images. The classiﬁer step in R-CNN only\\nsees small regions and needs good proposals.\\nDPM maintains its AP well when applied to artwork.\\nPrior work theorizes that DPM performs well because it has\\nstrong spatial models of the shape and layout of objects.\\nThough DPM doesn’t degrade as much as R-CNN, it starts\\nfrom a lower AP.\\nYOLO has good performance on VOC 2007 and its AP\\ndegrades less than other methods when applied to artwork.\\nLike DPM, YOLO models the size and shape of objects,\\nas well as relationships between objects and where objects\\ncommonly appear. Artwork and natural images are very\\ndifferent on a pixel level but they are similar in terms of\\nthe size and shape of objects, thus YOLO can still predict\\ngood bounding boxes and detections.\\n5. Real-Time Detection In The Wild\\nYOLO is a fast, accurate object detector, making it ideal\\nfor computer vision applications. We connect YOLO to a\\nwebcam and verify that it maintains real-time performance,', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9c05140e-d4e2-45cf-ae41-69e89f3502ea', embedding=None, metadata={'page_label': '8', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Poselets\\nRCNN\\nD&T\\nHumans\\nDPM\\nYOLO\\n(a) Picasso Dataset precision-recall curves.\\nVOC 2007 Picasso People-Art\\nAP AP Best F1 AP\\nYOLO 59.2 53.3 0.590 45\\nR-CNN 54.2 10.4 0.226 26\\nDPM 43.2 37.8 0.458 32\\nPoselets [2] 36.5 17.8 0.271\\nD&T [4] - 1.9 0.051\\n(b) Quantitative results on the VOC 2007, Picasso, and People-Art Datasets.\\nThe Picasso Dataset evaluates on both AP and best F1 score.\\nFigure 5: Generalization results on Picasso and People-Art datasets.\\nFigure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it\\ndoes think one person is an airplane.\\nincluding the time to fetch images from the camera and dis-\\nplay the detections.\\nThe resulting system is interactive and engaging. While\\nYOLO processes images individually, when attached to a\\nwebcam it functions like a tracking system, detecting ob-\\njects as they move around and change in appearance. A\\ndemo of the system and the source code can be found on\\nour project website: http://pjreddie.com/yolo/.\\n6. Conclusion\\nWe introduce YOLO, a uniﬁed model for object detec-\\ntion. Our model is simple to construct and can be trained\\ndirectly on full images. Unlike classiﬁer-based approaches,\\nYOLO is trained on a loss function that directly corresponds\\nto detection performance and the entire model is trained\\njointly.\\nFast YOLO is the fastest general-purpose object detec-\\ntor in the literature and YOLO pushes the state-of-the-art in\\nreal-time object detection. YOLO also generalizes well to\\nnew domains making it ideal for applications that rely on\\nfast, robust object detection.\\nAcknowledgements: This work is partially supported by\\nONR N00014-13-1-0720, NSF IIS-1338054, and The Allen\\nDistinguished Investigator Award.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='744bd493-0616-4bd3-80f5-4878c3664d8f', embedding=None, metadata={'page_label': '9', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='References\\n[1] M. B. Blaschko and C. H. Lampert. Learning to localize ob-\\njects with structured output regression. In Computer Vision–\\nECCV 2008, pages 2–15. Springer, 2008. 4\\n[2] L. Bourdev and J. Malik. Poselets: Body part detectors\\ntrained using 3d human pose annotations. In International\\nConference on Computer Vision (ICCV), 2009. 8\\n[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The cross-\\ndepiction problem: Computer vision algorithms for recog-\\nnising objects in artwork and in photographs. arXiv preprint\\narXiv:1505.00110, 2015. 7\\n[4] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In Computer Vision and Pattern Recogni-\\ntion, 2005. CVPR 2005. IEEE Computer Society Conference\\non, volume 1, pages 886–893. IEEE, 2005. 4, 8\\n[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijaya-\\nnarasimhan, J. Yagnik, et al. Fast, accurate detection of\\n100,000 object classes on a single machine. In Computer\\nVision and Pattern Recognition (CVPR), 2013 IEEE Confer-\\nence on, pages 1814–1821. IEEE, 2013. 5\\n[6] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\\nvation feature for generic visual recognition. arXiv preprint\\narXiv:1310.1531, 2013. 4\\n[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards uniﬁed\\nobject detection and semantic segmentation. In Computer\\nVision–ECCV 2014, pages 299–314. Springer, 2014. 7\\n[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\\nobject detection using deep neural networks. In Computer\\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\\nence on, pages 2155–2162. IEEE, 2014. 5, 6\\n[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\\nject classes challenge: A retrospective.International Journal\\nof Computer Vision, 111(1):98–136, Jan. 2015. 2\\n[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 32(9):1627–1645, 2010. 1, 4\\n[11] S. Gidaris and N. Komodakis. Object detection via a multi-\\nregion & semantic segmentation-aware CNN model. CoRR,\\nabs/1505.01749, 2015. 7\\n[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\\nple in cubist art. InComputer Vision-ECCV 2014 Workshops,\\npages 101–116. Springer, 2014. 7\\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\\nture hierarchies for accurate object detection and semantic\\nsegmentation. In Computer Vision and Pattern Recognition\\n(CVPR), 2014 IEEE Conference on , pages 580–587. IEEE,\\n2014. 1, 4, 7\\n[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.\\n2, 5, 6, 7\\n[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-\\ntion and object detection. In Advances in neural information\\nprocessing systems, pages 655–663, 2009. 4\\n[16] B. Hariharan, P. Arbel ´aez, R. Girshick, and J. Malik. Simul-\\ntaneous detection and segmentation. In Computer Vision–\\nECCV 2014, pages 297–312. Springer, 2014. 7\\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\\nin deep convolutional networks for visual recognition. arXiv\\npreprint arXiv:1406.4729, 2014. 5\\n[18] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. Improving neural networks by pre-\\nventing co-adaptation of feature detectors. arXiv preprint\\narXiv:1207.0580, 2012. 4\\n[19] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\\nin object detectors. In Computer Vision–ECCV 2012, pages\\n340–353. Springer, 2012. 6\\n[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint\\narXiv:1506.06981, 2015. 5, 6\\n[21] R. Lienhart and J. Maydt. An extended set of haar-like fea-\\ntures for rapid object detection. In Image Processing. 2002.\\nProceedings. 2002 International Conference on , volume 1,\\npages I–900. IEEE, 2002. 4\\n[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR,\\nabs/1312.4400, 2013. 2\\n[23] D. G. Lowe. Object recognition from local scale-invariant\\nfeatures. In Computer vision, 1999. The proceedings of the\\nseventh IEEE international conference on , volume 2, pages\\n1150–1157. Ieee, 1999. 4\\n[24] D. Mishkin. Models accuracy on imagenet 2012\\nval. https://github.com/BVLC/caffe/wiki/\\nModels-accuracy-on-ImageNet-2012-val . Ac-\\ncessed: 2015-10-2. 3\\n[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general\\nframework for object detection. In Computer vision, 1998.\\nsixth international conference on , pages 555–562. IEEE,\\n1998. 4\\n[26] J. Redmon. Darknet: Open source neural networks in c.\\nhttp://pjreddie.com/darknet/, 2013–2016. 3\\n[27] J. Redmon and A. Angelova. Real-time grasp detection using\\nconvolutional neural networks. CoRR, abs/1412.3128, 2014.\\n5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\\nwards real-time object detection with region proposal net-\\nworks. arXiv preprint arXiv:1506.01497, 2015. 5, 6, 7\\n[29] S. Ren, K. He, R. B. Girshick, X. Zhang, and J. Sun. Object\\ndetection networks on convolutional feature maps. CoRR,\\nabs/1504.06066, 2015. 3, 7\\n[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\\nRecognition Challenge. International Journal of Computer\\nVision (IJCV), 2015. 3\\n[31] M. A. Sadeghi and D. Forsyth. 30hz object detection with\\ndpm v5. In Computer Vision–ECCV 2014 , pages 65–79.\\nSpringer, 2014. 5, 6\\n[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks. CoRR,\\nabs/1312.6229, 2013. 4, 5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='70e2e56d-ce95-42c7-93ab-2fbd02be75b9', embedding=None, metadata={'page_label': '10', 'file_name': '1506.02640v5.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1506.02640v5.pdf', 'file_type': 'application/pdf', 'file_size': 5296750, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[33] Z. Shen and X. Xue. Do more dropouts in pool5 feature maps\\nfor better object detection. arXiv preprint arXiv:1409.6911,\\n2014. 7\\n[34] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.\\nGoing deeper with convolutions. CoRR, abs/1409.4842,\\n2014. 2\\n[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\\nSmeulders. Selective search for object recognition. Inter-\\nnational journal of computer vision , 104(2):154–171, 2013.\\n4\\n[36] P. Viola and M. Jones. Robust real-time object detection.\\nInternational Journal of Computer Vision, 4:34–47, 2001. 4\\n[37] P. Viola and M. J. Jones. Robust real-time face detection.\\nInternational journal of computer vision , 57(2):137–154,\\n2004. 5\\n[38] J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable\\npart model for object detection. In Computer Vision and Pat-\\ntern Recognition (CVPR), 2014 IEEE Conference on , pages\\n2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object pro-\\nposals from edges. In Computer Vision–ECCV 2014, pages\\n391–405. Springer, 2014. 4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ef124069-39e1-4659-9988-323d7f4eb9f3', embedding=None, metadata={'page_label': '1', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7d75b593-f216-426f-b29a-51cb097a5730', embedding=None, metadata={'page_label': '2', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c2f3330c-e361-4b53-a308-dd041e564bb4', embedding=None, metadata={'page_label': '3', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='afecac96-1f3b-4552-b8d3-7ac45fed0870', embedding=None, metadata={'page_label': '4', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='67a65507-1004-4531-898d-8b1355e8d8f1', embedding=None, metadata={'page_label': '5', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5bb01e42-60b3-4fc1-aea8-7b0d996613bb', embedding=None, metadata={'page_label': '6', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4c874c80-4945-4137-9b9c-810bcfb10964', embedding=None, metadata={'page_label': '7', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1b9cb33b-3d54-4edc-94e3-54bd0904cdb1', embedding=None, metadata={'page_label': '8', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fa44b297-deac-4383-b7ed-ea136d9b00a3', embedding=None, metadata={'page_label': '9', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f6c22c18-7c6d-4f8d-8ac7-94ddaf0fda6b', embedding=None, metadata={'page_label': '10', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3faeae7f-0a1d-4d37-be2c-20d655937226', embedding=None, metadata={'page_label': '11', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c5acdb53-c364-457e-b2ba-4714d5154e9e', embedding=None, metadata={'page_label': '12', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5c3b704a-6405-4f2c-bffd-ecf8bb97bd59', embedding=None, metadata={'page_label': '13', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1002a115-d0ac-416a-8844-6af935d1a41a', embedding=None, metadata={'page_label': '14', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b8ee5cd8-74b0-48a4-998f-aa97c446b324', embedding=None, metadata={'page_label': '15', 'file_name': '1706.03762v7.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3e0d1981-cdc5-4869-9566-ca40b85e5bbc', embedding=None, metadata={'page_label': '1', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fda75eb2-0a05-4ec4-8d3c-bf526cb4b339', embedding=None, metadata={'page_label': '2', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The\\tDivine\\nComedy\\t(x) q \\nQuery \\nEncoder \\nq(x) \\nMIPS p θ \\nGenerator\\xa0pθ\\n(Parametric) \\nMargin- \\nalize \\nThis\\t14th\\tcentury\\twork\\nis\\tdivided\\tinto\\t3\\nsections:\\t\"Inferno\",\\n\"Purgatorio\"\\t&\\n\"Paradiso\"\\t\\t\\t\\t\\t\\t\\t\\t\\t(y)\\nEnd-to-End Backprop through q  and\\xa0p θ \\nBarack\\tObama\\twas\\nborn\\tin\\tHawaii.(x)\\nFact Veriﬁcation: Fact Query\\nsupports\\t(y)\\nQuestion Generation\\nFact Veriﬁcation:\\nLabel Generation\\nDocument \\nIndex \\nDefine\\t\"middle\\tear\"(x)\\nQuestion Answering:\\nQuestion Query\\nThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t(y)\\nQuestion Answering:\\nAnswer GenerationRetriever pη \\n(Non-Parametric) \\nz 4 \\nz 3 \\nz 2 \\nz 1 \\nd(z) \\nJeopardy Question\\nGeneration:\\nAnswer Query\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\\nIndex) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we\\ntreat zas a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\\ncan be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64, 55], stack-\\naugmented networks [25] and memory layers [ 30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.\\nOur results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequencexto retrieve text documents zand use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pη(z|x) with parameters ηthat returns (top-K truncated)\\ndistributions over text passages given a query xand (ii) a generator pθ(yi|x,z,y 1:i−1) parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='54b8ad33-7c1d-45c7-8f87-e7fe497912fd', embedding=None, metadata={'page_label': '3', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='by θthat generates a current token based on a context of the previous i−1 tokens y1:i−1, the original\\ninput xand a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence, the model uses the same document\\nto predict each target token. The second approach, RAG-Token, can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npη and pθ components, as well as the training and decoding procedure.\\n2.1 Models\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence(y|x) ≈\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(y|x,z) =\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)\\nN∏\\ni\\npθ(yi|x,z,y 1:i−1)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\\npRAG-Token(y|x) ≈\\nN∏\\ni\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x,z,y 1:i−1)\\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class\\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2 Retriever: DPR\\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\npη(z|x) ∝exp\\n(\\nd(z)⊤q(x)\\n)\\nd(z) =BERTd(z), q(x) =BERTq(x)\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\\ntop-k(pη(·|x)), the list of kdocuments zwith highest prior probability pη(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory.\\n2.3 Generator: BART\\nThe generator component pθ(yi|x,z,y 1:i−1) could be modelled using any encoder-decoder. We use\\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\\nxwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters θas the parametric memory henceforth.\\n2.4 Training\\nWe jointly train the retriever and generator components without any direct supervision on what\\ndocument should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3fe30fe0-ef6f-4cd5-aba4-93455fa1ef5c', embedding=None, metadata={'page_label': '4', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='minimize the negative marginal log-likelihood of each target, ∑\\nj−log p(yj|xj) using stochastic\\ngradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [20].\\nWe do not ﬁnd this step necessary for strong performance, and keep the document encoder (and\\nindex) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.\\n2.5 Decoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximatearg maxyp(y|x).\\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p′\\nθ(yi|x,y1:i−1) = ∑\\nz∈top-k(p(·|x)) pη(zi|x)pθ(yi|x,zi,y1:i−1) To\\ndecode, we can plug p′\\nθ(yi|x,y1:i−1) into a standard beam decoder.\\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses\\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis y we run an additional forward pass for each document z for which y does not\\nappear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across\\nbeams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer\\noutput sequences, |Y|can become large, requiring many forward passes. For more efﬁcient decoding,\\nwe can make a further approximation that pθ(y|x,zi) ≈0 where ywas not generated during beam\\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Y has\\nbeen generated. We refer to this decoding procedure as “Fast Decoding.”\\n3 Experiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\\nkdocuments for each query. We consider k∈{5,10}for training and set kfor test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1 Open-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)\\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\\nQA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2 Abstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\\ntext generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='de773657-7595-4860-8722-a6d7e02441f2', embedding=None, metadata={'page_label': '5', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\n“What is the weather in V olcano, CA?” so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3 Jeopardy Question Generation\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\nFor example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciﬁcity as high mutual dependence between the input and output [ 33]. We follow\\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptions—quuestion A is better, question B is better, both are good, or neither is good.\\n3.4 Fact Veriﬁcation\\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by\\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER\\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\\ntwo variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\\nretriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\\nencoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30397920-e9f9-4e3d-9d4c-91f17df04b67', embedding=None, metadata={'page_label': '6', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel NQ TQA WQ CT\\nClosed\\nBook\\nT5-11B [52] 34.5 - /50.1 37.4 -\\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBook\\nREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2\\nTable 2: Generation and classiﬁcation Test Scores.\\nMS-MARCO SotA is [4], FEVER-3 is [68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2\\nB-1 QB-1 R-L B-1 Label Acc.\\nSotA - - 49.8* 49.9* 76.8 92.2 *\\nBART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\\ncases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\\nspeciﬁc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows\\nan example. When generating “Sun”, the posterior is high for document 2 which mentions “The\\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\\ngenerated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\\nThis observation suggests that the generator can complete the titles without depending on speciﬁc\\ndocuments. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\\nﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding\"The\\nSun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\\nBART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A\\nwith \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows\\nhow parametric and non-parametric memories work together—the non-parametric component helps\\nto guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\\n4.4 Fact Veriﬁcation\\nTable 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='20db5973-b8f2-4d0a-86c9-63d8169435dd', embedding=None, metadata={'page_label': '7', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Document 1: his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\n”A Farewell to Arms”(1929) ...\\nDocument 2: ... artists of the 1920s ”Lost Generation” expatriate\\ncommunity. His debut novel,”The Sun Also Rises”, was published\\nin 1926.\\nBOS\\n”\\nTheSunAlso\\nR ises\\n” is a\\nnovel\\nby this\\nauthor\\nof ” A\\nFarewellto\\nArms\\n”\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5\\nFigure 2: RAG-Token document posterior p(zi|x,yi,y−i) for each generated token for input “Hem-\\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\\nTask Input Model Generation\\nMS-\\nMARCO\\ndeﬁne middle\\near\\nBART ?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotland\\nBART The currency needed in Scotland is Pound sterling.\\nRAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener\\n-ation\\nWashington\\nBART ?This state has the largest number of counties in the U.S.\\nRAG-T It’s the only U.S. state named for a U.S. president\\nRAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\\nFor 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the topkdocuments retrieved\\nby RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\\nmore diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\\nRAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5e1b4a4d-009f-4bab-9362-33c68704eea9', embedding=None, metadata={'page_label': '8', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality Speciﬁcity\\nBART better 7.1% 16.8%\\nRAG better 42.7% 37.4%\\nBoth good 11.7% 11.8%\\nBoth poor 17.7% 6.9%\\nNo majority 20.8% 20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO Jeopardy QGen\\nGold 89.6% 90.0%\\nBART 70.7% 32.4%\\nRAG-Token 77.8% 46.8%\\nRAG-Seq. 83.5% 53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.\\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\\nbetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n39\\n40\\n41\\n42\\n43\\n44NQ Exact Match RAG-Tok\\nRAG-Seq\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n40\\n50\\n60\\n70\\n80NQ Answer Recall @ K\\nRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n48\\n50\\n52\\n54\\n56Bleu-1 / Rouge-L score\\nRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\\ngeneration [36], dialogue [ 41, 65, 9, 13], translation [ 17], and language modeling [ 19, 27]. Our\\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ed5c6589-8167-44ef-9da0-0ca956ce893f', embedding=None, metadata={'page_label': '9', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-\\nmarks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.\\nFor further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval There is signiﬁcant work on learning to retrieve documents in information\\nretrieval, more recently with pre-trained, neural language models [ 44, 26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,\\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\\ncan be ﬁne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\\nwork. Other work improves the ability of dialog models to generate factual text by attending over\\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have proved successful in a number of domains including\\nMachine Translation [ 18, 22] and Semantic Parsing [21]. Our approach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6 Discussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85c85aa8-fb38-413d-9f80-5e1a6585adec', embedding=None, metadata={'page_label': '10', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Broader Impact\\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\\npages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .\\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9fcdd089-00bb-4a92-bb4e-e73ae446151e', embedding=None, metadata={'page_label': '11', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423.\\n[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\n[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082.\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\\nanthology/P19-1346.\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH.\\n[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,\\n2020. URL https://arxiv.org/abs/2004.07202.\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710.\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\\nexceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\\nhttp://arxiv.org/abs/1705.08807.\\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In AAAI Conference on Artiﬁcial Intelligence , 2018. URL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 , 32nd\\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.\\n32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,\\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4b08ec1f-4c6b-4595-b0b4-a177435eed6c', embedding=None, metadata={'page_label': '12', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems 31 , pages 10052–\\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\\nanthology/2020.acl-main.228.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\\naugmented recurrent nets. In Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1 , NIPS’15, page 190–198, Cam-\\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\\ntion through memorization: Nearest neighbor language models. In International Conference on\\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\\ntion Answering Research. Transactions of the Association of Computational Lin-\\nguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf .\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d13ccc85-eef6-40d9-9634-4c94e60abe10', embedding=None, metadata={'page_label': '13', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612.\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint\\narXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pages 110–119, San Diego, California, June 2016. Association for Computational\\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv\\npreprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\\nveriﬁability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\\n//arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\\nprecision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5c7ceb1d-6fee-4310-a2f8-b7545aee864d', embedding=None, metadata={'page_label': '14', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\\nProceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf.\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009.\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\\n2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.\\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong\\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models’ factual predictions. In\\nAutomated Knowledge Base Construction, 2020. URL https://openreview.net/forum?\\nid=025X0zPfn.\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\\nproving Language Understanding by Generative Pre-Training, 2018. URL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf.\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners, 2019. URL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\\nmodels_are_unsupervised_multitask_learners.pdf.\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\\n2002.08910.\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019.\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv, abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,Advances\\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='869f88f8-3a7c-46b3-a32e-dbeebda9ecc1', embedding=None, metadata={'page_label': '15', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\\nAAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\\nanthology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information\\nProcessing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\\nranking in open-domain question answering. In ICLR, 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence\\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\\nInternational Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713.\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0a844a62-641c-4727-87a2-b29e86bd822c', embedding=None, metadata={'page_label': '16', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:\\nState-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pages 2495–2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='95985390-81cf-426f-b578-c6de46e70ba4', embedding=None, metadata={'page_label': '17', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA Implementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB Human Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC Training setup Details\\nWe train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision\\nﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search\\nwith FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring∼100\\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found athttps://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nat https://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0373ccba-4151-4527-8652-4e13400821b4', embedding=None, metadata={'page_label': '18', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='D Further Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to ﬁnd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\\nthe model with each (q,a) pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-\\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\\nTo overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\nTriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\\nTriviaQA Web Development split. Roberts et al.[52] used the TriviaQA ofﬁcial Wikipedia test set\\ninstead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\\nappendix of [14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being\\nsimpler to answer from Wikipedia.\\nE Further Details on FEVER\\nFor FEVER classiﬁcation, we follow the practice from [ 32], and ﬁrst re-generate the claim, and\\nthen classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\\nwe explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to\\nus, directly tackling this task is not straightforward. We hope to address this in future work.\\nF Null Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\\nto model cases where no useful information could be retrieved for a given input. Here, ifkdocuments\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in\\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\\ndocuments for questions that are less likely to beneﬁt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG Parameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='89fcea6f-3865-48ac-861c-f14750e2c354', embedding=None, metadata={'page_label': '19', 'file_name': '2005.11401v4.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask Train Development Test\\nNatural Questions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\\npoint precision to manage memory and disk footprints.\\nH Retrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al.[46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c1115cc-2319-4a72-9e13-6a2f19de6859', embedding=None, metadata={'page_label': '1', 'file_name': '3422622.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\3422622.pdf', 'file_type': 'application/pdf', 'file_size': 1886889, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='NOVEMBER 2020  |  VOL. 63  |  NO. 11  |   COMMUNICATIONS OF THE ACM    139\\nGenerative Adversarial Networks\\nBy Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,  \\nDavid Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio\\nDOI:10.1145/3422622\\nAbstract\\nGenerative adversarial networks are a kind of artificial intel-\\nligence algorithm designed to solve the generative model-\\ning problem. The goal of a generative model is to study a \\ncollection of training examples and learn the probability \\ndistribution that generated them. Generative Adversarial \\nNetworks (GANs) are then able to generate more examples \\nfrom the estimated probability distribution. Generative \\nmodels based on deep learning are common, but GANs \\nare among the most successful generative models (espe-\\ncially in terms of their ability to generate realistic high-\\nresolution images). GANs have been successfully applied \\nto a wide variety of tasks (mostly in research settings) but \\ncontinue to present unique challenges and research \\nopportunities because they are based on game theory \\nwhile most other approaches to generative modeling are \\nbased on optimization.\\n1. INTRODUCTION\\nMost current approaches to developing artificial intelli-\\ngence are based primarily on machine learning. The most \\nwidely used and successful form of machine learning to date \\nis supervised learning. Supervised learning algorithms are \\ngiven a dataset of pairs of example inputs and example out-\\nputs. They learn to associate each input with each output \\nand thus learning a mapping from input to output exam-\\nples. The input examples are typically complicated data \\nobjects like images, natural language sentences, or audio \\nwaveforms, while the output examples are often relatively \\nsimple. The most common kind of supervised learning is \\nclassification, where the output is just an integer code iden-\\ntifying a specific category (a photo might be recognized as \\ncoming from category 0 containing cats, or category 1 con-\\ntaining dogs, etc.).\\nSupervised learning is often able to achieve greater than \\nhuman accuracy after the training process is complete, and \\nthus has been integrated into many products and services. \\nUnfortunately, the learning process itself still falls far short \\nof human abilities. Supervised learning by definition relies \\non a human supervisor to provide an output example for \\neach input example. Worse, existing approaches to super-\\nvised learning often require millions of training examples to \\nexceed human performance, when a human might be able \\nto learn to perform the task acceptably from a very small \\nnumber of examples.\\nIn order to reduce both the amount of human supervi-\\nsion required for learning and the number of examples \\nrequired for learning, many researchers today study \\nunsupervised learning, often using generative models. In \\nthis overview paper, we describe one particular approach \\nto unsupervised learning via generative modeling called \\ngenerative adversarial networks. We briefly review \\nThe original version of this paper is entitled “Generative \\nAdversarial Networks” and was published in Advances in \\nNeural Information Processing Systems 27 (NIPS 2014).\\napplications of GANs and identify core research problems \\nrelated to convergence in games necessary to make GANs a \\nreliable technology.\\n2. GENERATIVE MODELING\\nThe goal of supervised learning is relatively straightforward \\nto specify, and all supervised learning algorithms have \\nessentially the same goal: learn to accurately associate new \\ninput examples with the correct outputs. For instance, an \\nobject recognition algorithm may associate a photo of a dog \\nwith some kind of DOG category identifier.\\nUnsupervised learning is a less clearly defined branch of \\nmachine learning, with many different unsupervised learn-\\ning algorithms pursuing many different goals. Broadly \\nspeaking, the goal of unsupervised learning is to learn some-\\nthing useful by examining a dataset containing unlabeled \\ninput examples. Clustering and dimensionality reduction \\nare common examples of unsupervised learning.\\nAnother approach to unsupervised learning is generative \\nmodeling. In generative modeling, training examples x are \\ndrawn from an unknown distribution p data(x). The goal of a \\ngenerative modeling algorithm is to learn a p model(x) that \\napproximates pdata(x) as closely as possible.\\nA straightforward way to learn an approximation of pdata is \\nto explicitly write a function pmodel(x; θ) controlled by param-\\neters θ and search for the value of the parameters that makes \\npdata and pmodel as similar as possible. In particular, the most \\npopular approach to generative modeling is probably maxi-\\nmum likelihood estimation, consisting of minimizing the \\nKullback-Leibler divergence between p data and p model. The \\ncommon approach of estimating the mean parameter of a \\nGaussian distribution by taking the mean of a set of observa-\\ntions is one example of maximum likelihood estimation. \\nThis approach based on explicit density functions is illus-\\ntrated in Figure 1.\\nExplicit density modeling has worked well for traditional \\nstatistics, using simple functional forms of probability dis-\\ntributions, usually applied to small numbers of variables. \\nMore recently, with the rise of machine learning in general \\nand deep learning in particular, researchers have become \\ninterested in learning models that make use of relatively \\ncomplicated functional forms. When a deep neural net-\\nwork is used to generate data, the corresponding density \\nfunction may be computationally intractable. \\nTraditionally, there have been two dominant approaches \\nto confronting this intractability problem: (1) carefully \\ndesign the model to have a tractable density function \\n(e.g., Frey 11) and (2) design a learning algorithm based on \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='337fa10d-3350-4036-ab0a-dbc004284ac0', embedding=None, metadata={'page_label': '2', 'file_name': '3422622.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\3422622.pdf', 'file_type': 'application/pdf', 'file_size': 1886889, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='research highlights \\n \\n140     COMMUNICATIONS OF THE ACM  |  NOVEMBER 2020  |   VOL. 63  |   NO. 11\\na computationally tractable approximation of an intractable \\ndensity function (e.g., Kingma and Welling15). Both approaches \\nhave proved difficult, and for many applications, such as gen-\\nerating realistic high resolution images, researchers remain \\nunsatisfied with the results so far. This motivates further \\nresearch to improve these two paths, but also suggests that a \\nthird path could be useful.\\nBesides taking a point x as input and returning an esti-\\nmate of the probability of generating that point, a generative \\nmodel can be useful if it is able to generate a sample from \\nthe distribution p model. This is illustrated in Figure 2. Many \\nmodels that represent a density function can also generate \\nsamples from that density function. In some cases, generat-\\ning samples is very expensive or only approximate methods \\nof generating samples are tractable.\\nSome generative models avoid the entire issue of design-\\ning a tractable density function and learn only a tractable \\nsample generation process. These are called implicit genera-\\ntive models. GANs fall into this category. Prior to the intro-\\nduction of GANs, the state of the art deep implicit generative \\nmodel was the generative stochastic network 4 which is capa-\\nble of approximately generating samples via an incremental \\nprocess based on Markov chains. GANs were introduced in \\norder to create a deep implicit generative model that was \\nable to generate true samples from the model distribution \\nin a single generation step, without need for the incremen-\\ntal generation process or approximate nature of sampling \\nMarkov chains.\\nToday, the most popular approaches to generative mod-\\neling are probably GANs, variational autoencoders, 15 and \\nfully-visible belief nets (e.g., Frey 11, 26 ). None of these \\napproaches relies on Markov chains, so the reason for the \\ninterest in GANs today is not that they succeeded at their \\noriginal goal of generative modeling without Markov chains, \\nbut rather that they have succeded in generating high-qual-\\nity images and have proven useful for several tasks other \\nthan straightforward generation, as described in Section 5.\\n3. GENERATIVE ADVERSARIAL NETWORKS\\nGenerative adversarial networks are based on a game, in the \\nsense of game theory, between two machine learning models, \\ntypically implemented using neural networks.\\nOne network called the generator defines p model(x) implic-\\nitly. The generator is not necessarily able to evaluate the den-\\nsity function pmodel. For some variants of GANs, evaluation of \\nthe density function is possible (any tractable density model \\nfor which sampling is tractable and differntiable could \\nbe trained as a GAN generator, as done by Danihelka  \\nLearned\\nmodel\\nTraining data\\nGenerated samples\\nFigure 2. The goal of many generative models, as illustrated \\nhere, is to study a collection of training examples, then learn to \\ngenerate more examples that come from the same probability \\ndistribution. GANs learn to do this without using an explicit \\nrepresentation of the density function. One advantage of the \\nGAN framework is that it may be applied to models for which the \\ndensity function is computationally intractable. The samples \\nshown here are all samples from the ImageNet dataset, 8 \\nincluding the ones labeled “model samples.” We use actual \\nImageNet data to illustrate the goal that a hypothetical perfect \\nmodel would attain.\\nx\\np(x)\\nFigure 1. Many approaches to generative modeling are based \\non density estimation: observing several training examples of \\na random variable x  and inferring a density function p (x) that \\ngenerates the training data. This approach is illustrated here, \\nwith several data points on a real number line used to fit a \\nGaussian density function that explains the observed samples. \\nIn contrast to this common approach, GANs are implicit models  \\nthat infer the probability distribution p (x) without necessarily \\nrepresenting the density function explicitly.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='24eb7448-8fa7-4b89-b118-c6a4a261d967', embedding=None, metadata={'page_label': '3', 'file_name': '3422622.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\3422622.pdf', 'file_type': 'application/pdf', 'file_size': 1886889, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' \\nNOVEMBER 2020  |  VOL. 63  |  NO. 11  |   COMMUNICATIONS OF THE ACM    141\\net al. 6), but this is not required. Instead, the generator is \\nable to draw samples from the distribution p model. The gen-\\nerator is defined by a prior distribution p (z) over a vector z \\nthat serves as input to the generator function G( z; θ(G)) where \\nθ(G) is a set of learnable parameters defining the generator’s \\nstrategy in the game. The input vector z can be thought of as \\na source of randomness in an otherwise deterministic sys-\\ntem, analogous to the seed of pseudorandom number gen-\\nerator. The prior distribution p (z) is typically a relatively \\nunstructured distribution, such as a high-dimensional \\nGaussian distribution or a uniform distribution over a \\nhypercube. Samples z  from this distribution are then just \\nnoise. The main role of the generator is to learn the func-\\ntion G(z) that transforms such unstructured noise z  into \\nrealistic samples.\\nThe other player in this game is the discriminator. The \\ndiscriminator examines samples x and returns some esti-\\nmate D(x; θ(D)) of whether x is real (drawn from the training \\ndistribution) or fake (drawn from p model by running the gen-\\nerator). In the original formulation of GANs, this estimate \\nconsists of a probability that the input is real rather than \\nfake assuming that the real distribution and fake distribu-\\ntion are sampled equally often. Other formulations (e.g., \\nArjovsky et al. 1) exist but generally speaking, at the level of \\nverbal, intuitive descriptions, the discriminator tries to pre-\\ndict whether the input was real or fake.\\nEach player incurs a cost: J (G)(θ(G), θ(D)) for the generator \\nand J(D)(θ(G), θ(D)) for the discriminator. Each player attempts \\nto minimize its own cost. Roughly speaking, the discrimina-\\ntor’s cost encourages it to correctly classify data as real or \\nfake, while the generator’s cost encourages it to generate \\nsamples that the discriminator incorrectly classifies as real. \\nVery many different specific formulations of these costs are \\npossible and so far most popular formulations seem to per-\\nform roughly the same. 18 In the original version of  \\nGANs, J(D) was defined to be the negative log-likelihood that \\nthe discriminator assigns to the real-vs-fake labels given the \\ninput to the discriminator. In other words, the discriminator \\nis trained just like a regular binary classifier. The original \\nwork on GANs offered two versions of the cost for the gener-\\nator. One version, today called minimax GAN (M-GAN) \\ndefined a cost J (G) = −J (D), yielding a minimax game that is \\nstraightforward to analyze theoretically. M-GAN defines the \\ncost for the generator by flipping the sign of the discrimina-\\ntor’s cost; another approach is the non-saturating GAN \\n(NS-GAN), for which the generator’s cost is defined by flip-\\nping the discriminator’s labels. In other words, the genera-\\ntor is tried to minimize the negative log-likelihood that the \\ndiscriminator assigns to the wrong labels. The later helps to \\navoid gradient saturation while training the model.\\nWe can think of GANs as a bit like counterfeiters and \\npolice: the counterfeiters make fake money while the \\npolice try to arrest counterfeiters and continue to allow \\nthe spending of legitimate money. Competition between \\ncounterfeiters and police leads to more and more realistic \\ncounterfeit money until eventually the counterfeiters pro-\\nduce perfect fakes and the police cannot tell the difference \\nbetween real and fake money. One complication to this \\nanalogy is that the generator learns via the discriminator’s \\nReal data Fake data\\nDataset Generator\\nRandom\\nlatent\\nvariable\\nRandom\\nindex into\\ndataset\\nDiscriminator Discriminator\\nFigure 3. Training GANs involves training both a generator network \\nand a discriminator network. The process involves both real data \\ndrawn from a dataset and fake data created continuously by the \\ngenerator throughout the training process. The discriminator is \\ntrained much like any other classifier defined by a deep neural \\nnetwork. As shown on the left, the discriminator is shown data \\nfrom the training set. In this case, the discriminator is trained to \\nassign data to the “real” class. As shown on the right, the training \\nprocess also involves fake data. The fake data is constructed by first \\nsampling a random vector z from a prior distribution over latent \\nvariables of the model. The generator is then used to to produce a \\nsample x = G(z). The function G is simply a function represented by a \\nneural network that transforms the random, unstructured z vector \\ninto structured data, intended to be statistically indistinguishable \\nfrom the training data. The discriminator then classifies this fake \\ndata. The discriminator is trained to assign this data to the “fake” \\nclass. The backpropagation algorithm makes it possible to use \\nthe derivatives of the discriminator’s output with respect to the \\ndiscriminator’s input to train the generator. The generator is trained \\nto fool the discriminator, in other words, to make the discriminator \\nassign its input to the “real” class. The training process for the \\ndiscriminator is thus much the same as for any other binary \\nclassifier with the exception that the data for the “fake” class comes \\nfrom a distribution that changes constantly as the generator learns \\nrather than from a fixed distribution. The learning process for the \\ngenerator is somewhat unique, because it is not given specific \\ntargets for its output, but rather simply given a reward for producing \\noutputs that fool its (constantly changing) opponent.\\ngradient, as if the counterfeiters have a mole among the \\npolice reporting the specific methods that the police use \\nto detect fakes.\\nThis process is illustrated in Figure 3. Figure 4 shows a \\ncartoon giving some intution for how the process works.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bdc46e0d-9d80-4bd7-9744-f7bba31601d5', embedding=None, metadata={'page_label': '4', 'file_name': '3422622.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\3422622.pdf', 'file_type': 'application/pdf', 'file_size': 1886889, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='research highlights \\n \\n142     COMMUNICATIONS OF THE ACM  |  NOVEMBER 2020  |   VOL. 63  |   NO. 11\\nas demonstrated by Metz et al.,22 but the argmin operation is \\ndifficult to work with in this way. The most popular approach \\nis to regard this situation as a game between two players. \\nMuch of the game theory literature is concerned with games \\nthat have discrete and finite action spaces, convex losses, or \\nother properties simplifying them. GANs require use of \\ngame theory in settings that are not yet well-explored, where \\nthe costs are non-convex and the actions and policies are \\ncontinuous and high-dimensional (regardless of whether \\nwe consider an action to be choosing a specific parameter \\nvector θ(G) or whether we consider the action to be generating \\na sample x). The goal of a machine learning algorithm in this \\ncontext is to find a local Nash equilibrium 28: a point that is a \\nlocal minimum of each player’s cost with respect to that \\nplayer’s parameters. With local moves, no player can reduce \\nits cost further, assuming the other player’s parameters do \\nnot change.\\nThe most common training algorithm is simply to use a \\ngradient-based optimizer to repeatedly take simultaneous \\nsteps on both players, incrementally minimizing each play-\\ner’s cost with respect to that player’s parameters.\\nAt the end of the training process, GANs are often able to \\nproduce realistic samples, even for very complicated datas-\\nets containing high-resolution images. An example is shown \\nin Figure 5.\\nAt a high level, one reason that the GAN framework is suc-\\ncesful may be that it involves very little approximation. Many \\nother approaches to generative modeling must approximate \\nan intractable density functions. GANs do not involve any \\nThe situation is not straightforward to model as an opti-\\nmization problem because each player’s cost is a function of \\nthe other player’s parameters, but each player may control \\nonly its own parameters. It is possible to reduce the situa-\\ntion to optimization, where the goal is to minimize\\nx\\nz\\nx\\nz\\n(a)\\n(c) (d)\\n(b)\\nx\\nz\\nx\\nz\\nFigure 4. An illustration of the basic intuition behind the \\nGAN training process, illustrated by fitting a 1-D Gaussian \\ndistribution. In this example, we can understand the goal of the \\ngenerator as learning a simple scaling of the inverse cumulative \\ndistribution function of the data generating distribution. GANs \\nare trained by simultaneously updating the discriminator \\nfunction (D , blue, dashed line) so that it discriminates between \\nsamples from the data generating distribution (black, dotted \\nline) px from those of the generative distribution p model  (green, \\nsolid line). The lower horizontal line is the domain from which \\nz is sampled, in this case uniformly. The horizontal line above \\nis part of the domain of x . The upward arrows show how the \\nmapping x = G (z) imposes the non-uniform distribution p model  on \\ntransformed samples. G  contracts in regions of high density \\nand expands in regions of low density of p model . (a) Consider a \\npair of adversarial networks at initialization: p model  is initialized \\nto a unit Gaussian for this example while D  is defined by a \\nrandomly initialized deep neural network. (b) Suppose that \\nD were trained to convergence while G  were held fixed. In \\npractice, both are trained simultaneously, but for the purpose of \\nbuilding intuition, we see that if G were fixed, D  would converge \\nto . (c) Now suppose that we gradually train \\nboth G and D  for a while. The samples x  generated by G  flow in \\nthe direction of increasing D  in order to arrive at regions that \\nare more likely to be classified as data. Meanwhile the estimate \\nof D is updated in response to this update in G . (d) At the Nash \\nequilibrium, neither player can improve its payoff because p model \\n= pdata. The discriminator is unable to differentiate between \\nthe two distributions, that is, . This constant function \\nshows that all points are equally likely to have come from either \\ndistribution. In practice, G  and D  are typically optimized with \\nsimultaneous gradient steps, and it is not necessary for D  to \\nbe optimal at every step as shown in this intuitive cartoon. See \\nRefs. Fedus et al. 10 and Nagarajan and Kolter 24 for more realistic \\ndiscussions of the GAN equilibration process.\\nFigure 5. This image is a sample from a Progressive GAN14 depicting \\na person who does not exist but was “imagined” by a GAN after \\ntraining on photos of celebrities.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='24395b8a-d0c8-4d5f-a232-6e2a44c7ba83', embedding=None, metadata={'page_label': '5', 'file_name': '3422622.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\3422622.pdf', 'file_type': 'application/pdf', 'file_size': 1886889, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' \\nNOVEMBER 2020  |  VOL. 63  |  NO. 11  |   COMMUNICATIONS OF THE ACM    143\\nspurious Nash equilibria exist, 32 whether the learning algo-\\nrithm converges to a Nash equilibrium, 24 and if it does so, \\nhow quickly.21\\nIn many cases of practical interest, these theoretical \\nquestions are open, and the best learning algorithms seem \\nempirically to often fail to converge. Theoretical work to \\nanswer these questions is ongoing, as is work to design bet-\\nter costs, models, and training algorithms with better con-\\nvergence properties.\\n5. OTHER GAN TOPICS\\nThis article is focused on a summary of the core design con-\\nsiderations and algorithmic properties of GANs.\\nMany other topics of potential interest cannot be consid-\\nered here due to space consideration. This article discussed \\nusing GANs to approximate a distribution p(x) they have also \\nbeen extended to the conditional setting23, 25 where they gen-\\nerate samples corresponding to some input by drawing sam-\\nples from the conditional distribution p(x | y). GANs are \\nrelated to moment matching 16 and optimal transport. 1 A \\nquirk of GANs that is made especially clear through their \\nconnection to MMD and optimal transport is that they may \\nbe used to train generative models for which p model has sup-\\nport only on a thin manifold and may actually assign zero \\nlikelihood to the training data. GANs struggle to generate \\ndiscrete data because the back-propagation algorithm \\nneeds to propagate gradients from the discriminator \\nthrough the output of the generator, but this problem is \\nbeing gradually resolved. 9 Like most generative models, \\nGANs can be used to fill in gaps in missing data.34 GANs have \\nproven very effective for learning to classify data using very \\nfew labeled training examples.29 Evaluating the performance \\nof generative models including GANs is a difficult research \\narea in its own right. 29, 31, 32, 33  GANs can be seen as a way for \\nmachine learning to learn its own cost function, rather than \\nminimizing a hand-designed cost function. GANs can be \\nseen as a way of supervising machine learning by asking it to \\napproximation to their true underlying task. The only real \\nerror is the statistical error (sampling of a finite amount of \\ntraining data rather than measuring the true underlying \\ndata-generating distribution) and failure of the learning \\nalgorithm to converge to exactly the optimal parameters. \\nMany generative modeling strategies would introduce these \\nsources of error and also further sources of approximation \\nerror, based on Markov chains, optimization of bounds on \\nthe true cost rather than the cost itself, etc.\\nIt is difficult to give much further specific guidance regard-\\ning the details of GANs because GANs are such an active \\nresearch area and most specific advice quickly becomes out \\nof date. Figure 6 shows how quickly the capabilities of GANs \\nhave progressed in the years since their introduction.\\n4. CONVERGENCE OF GANS\\nThe central theoretical results presented in the original GAN \\npaper13 were that:\\n1. in the space of density functions pmodel and discrimina-\\ntor functions D, there is only one local Nash equilib-\\nrium, where pmodel = pdata.\\n2. if it were possible to optimize directly over such den-\\nsity functions, then the algorithm that consists of opti-\\nmizing D to convergence in the inner loop, then \\nmaking a small gradient step on p model in the outer \\nloop, converges to this Nash equilibrium.\\nHowever, the theoretical model of local moves directly in \\ndensity function space may not be very relevant to GANs as \\nthey are trained in practice: using local moves in parameter \\nspace of the generator function, among the set of functions \\nrepresentable by neural networks with a finite number of \\nparameters, with each parameter represented with a finite \\nnumber of bits.\\nIn many different theoretical models, it is interesting to \\nstudy whether a Nash equilibrium exists, 2 whether any \\nFigure 6. An illustration of progress in GAN capabilities over the course of approximately three years following the introduction of \\nGANs. GANs have rapidly become more capable, due to changes in GAN algorithms, improvements to the underlying deep learning \\nalgorithms, and improvements to underlying deep learning software and hardware infrastructure. This rapid progress means that  \\nit is infeasible for any single document to summarize the state-of-the-art GAN capabilities or any specific set of best practices;  \\nboth continue to evolve rapidly enough that any comprehensive survey quickly becomes out of date. Figure reproduced with \\npermission from Brundage et al. 5 The individual results are from Refs. Goodfellow, 13 Karras et al., 14 Liu and Tuzel, 17 and  \\nRadford et al. 27 respectively.\\n2014 2015 2016 2017\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f41bac9b-d49b-4ff5-bb47-129eb7632663', embedding=None, metadata={'page_label': '6', 'file_name': '3422622.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\3422622.pdf', 'file_type': 'application/pdf', 'file_size': 1886889, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='research highlights \\n \\n144     COMMUNICATIONS OF THE ACM  |  NOVEMBER 2020  |   VOL. 63  |   NO. 11\\nproduce any output that the machine learning algorithm \\nitself recognizes as acceptable, rather than by asking it to \\nproduce a specific example output. GANs are thus great for \\nlearning in situations where there are many possible correct \\nanswers, such as predicting the many possible futures that \\ncan happen in video generation.19 GANs and GAN-like mod-\\nels can be used to learn to transform data from one domain \\ninto data from another domain, even without any labeled \\npairs of examples from those domains (e.g., Zhu et al.35). For \\nexample, after studying a collection of photos of zebras and \\na collection of photos of horses, GANs can turn a photo of a \\nhorse into a photo of a zebra. 35 GANs have been used in sci-\\nence to simulate experiments that would be costly to run \\neven in traditional software simulators. 7 GANs can be used \\nto create fake data to train other machine learning models, \\neither when real data would be hard to acquire 30 or when \\nthere would be privacy concerns associated with real data. 3 \\nGAN-like models called domain-adversarial networks can be \\nused for domain adaptation.12 GANs can be used for a variety \\nof interactive digital media effects where the end goal is to \\nproduce compelling imagery. 35 GANs can even be used to \\nsolve variational inference problems used in other \\napproaches to generative modeling.20 GANs can learn useful \\nembedding vectors and discover concepts like gender of \\nhuman faces without supervision.27\\n6. CONCLUSION\\nGANs are a kind of generative model based on game theory. \\nThey have had great practical success in terms of generating \\nrealistic data, especially images. It is currently still difficult \\nto train them. For GANs to become a more reliable technol-\\nogy, it will be necessary to design models, costs, or training \\nalgorithms for which it is possible to find good Nash equilib-\\nria consistently and quickly. \\nReferences\\n 1. Arjovsky, M., Chintala, S., Bottou, L.  \\nWasserstein gan. arXiv preprint \\narXiv:1701.07875 (2017).\\n 2. Arora, S., Ge, R., Liang, Y., Ma, T., \\nZhang, Y. Generalization and \\nequilibrium in generative adversarial \\nnets (gans). arXiv preprint \\narXiv:1703.00573 (2017).\\n 3. Beaulieu-Jones, B.K., Wu, Z.S., \\nWilliams, C., Greene, C.S. Privacy-\\npreserving generative deep neural \\nnetworks support clinical data \\nsharing. bioRxiv (2017), 159756.\\n 4. Bengio, Y., Thibodeau-Laufer, E.,  \\nAlain, G., Yosinski, J. Deep generative \\nstochastic networks trainable by \\nbackprop. In ICML’2014 (2014).\\n 5. Brundage, M., Avin, S., Clark, J.,  \\nToner, H., Eckersley, P., Garfinkel, B.,  \\nDafoe, A., Scharre, P., Zeitzoff, T., Filar, B., \\nAnderson, H., Roff, H., Allen, G.C., \\nSteinhardt, J., Flynn, C., hÉigeartaigh, \\nS.Ó., Beard, S., Belfield, H., Farquhar, S., \\nLyle, C., Crootof, R., Evans, O., Page, M., \\nBryson, J., Yampolskiy, R., Amodei, D. \\nThe Malicious Use of Artificial \\nIntelligence: Forecasting, Prevention, \\nand Mitigation. ArXiv e-prints (Feb. 2018).\\n 6. Danihelka, I., Lakshminarayanan, B.,  \\nUria, B., Wierstra, D., Dayan, P.  \\nComparison of maximum likelihood \\nand GAN-based training of real nvps. \\narXiv preprint arXiv:1705.05263 \\n(2017).\\n 7. de Oliveira, L., Paganini, M., Nachman, \\nB. Learning particle physics by \\nexample: location-aware generative \\nadversarial networks for physics \\nsynthesis. Computing and Software for \\nBig Science 1 1(2017), 4.\\n 8. Deng, J., Dong, W., Socher, R.,  \\nLi, L.-J., Li, K., Fei-Fei, L. ImageNet: A \\nLarge-Scale Hierarchical Image \\nDatabase. In CVPR09 (2009).\\n 9. Fedus, W., Goodfellow, I.,  \\nDai, A.M. MaskGAN: Better text \\ngeneration via filling in the _____. In \\nInternational Conference on Learning \\nRepresentations (2018).\\n 10. Fedus, W., Rosca, M., \\nLakshminarayanan, B., Dai, A.M., \\nMohamed, S., Goodfellow, I. Many \\npaths to equilibrium: GANs do not \\nneed to decrease a divergence at \\nevery step. In International \\nConference on Learning \\nRepresentations (2018).\\n 11. Frey, B.J. Graphical Models for Machine \\nLearning and Digital Communication. \\nMIT Press, Boston, 1998.\\n 12. Ganin, Y., Lempitsky, V. Unsupervised \\ndomain adaptation by \\nbackpropagation. In International \\nConference on Machine Learning \\n(2015), 1180–1189.\\n 13. Goodfellow, I., Pouget-Abadie, J., \\nMirza, M., Xu, B., Warde-Farley, D.,  \\nOzair, S., Courville, A., Bengio, Y. \\nGenerative adversarial nets.  \\nZ. Ghahramani, M. Welling, C. Cortes, \\nN.D. Lawrence, K.Q. Weinberger, eds. \\nAdvances in Neural Information \\nProcessing Systems 27, Curran \\nAssociates, Inc., Boston, 2014, \\n2672–2680.\\n 14. Karras, T., Aila, T., Laine, S., Lehtinen, J. \\nProgressive growing of GANs for \\nimproved quality, stability, and variation. \\nCoRR, abs/1710.10196 (2017).\\n 15. Kingma, D.P., Welling, M. Auto-\\nencoding variational bayes. In \\nProceedings of the International \\nConference on Learning \\nRepresentations (ICLR) (2014).\\n 16. Li, Y., Swersky, K., Zemel, R.S. Generative \\nmoment matching networks. CoRR, \\nabs/1502.02761 (2015).\\n 17. Liu, M.-Y., Tuzel, O. Coupled generative \\nadversarial networks. D.D. Lee, M. \\nSugiyama, U.V. Luxburg, I. Guyon, R. \\nGarnett, eds. Advances in Neural \\nInformation Processing Systems 29, \\nCurran Associates, Inc., Boston, 2016, \\n469–477.\\n 18. Lucic, M., Kurach, K., Michalski, M., \\nGelly, S., Bousquet, O. Are GANs \\ncreated equal? a large-scale study. \\narXiv preprint arXiv:1711.10337 (2017).\\n 19. Mathieu, M., Couprie, C., LeCun, Y. \\nDeep multi-scale video prediction \\nbeyond mean square error. arXiv \\npreprint arXiv:1511.05440 (2015).\\n 20. Mescheder, L., Nowozin, S., Geiger, A. \\nAdversarial variational bayes: Unifying \\nvariational autoencoders and \\ngenerative adversarial networks. arXiv \\npreprint arXiv:1701.04722 (2017).\\n 21. Mescheder, L., Nowozin, S., Geiger, A. \\nThe numerics of gans. In Advances in \\nNeural Information Processing \\nSystems (2017), 1823–1833.\\n 22. Metz, L., Poole, B., Pfau, D., \\nSohl-Dickstein, J. Unrolled generative \\nadversarial networks. arXiv preprint \\narXiv:1611.02163 (2016).\\n 23. Mirza, M., Osindero, S. Conditional \\ngenerative adversarial nets. arXiv \\npreprint arXiv:1411.1784 (2014).\\n 24. Nagarajan, V., Kolter, J.Z. Gradient \\ndescent GAN optimization is locally \\nstable. I. Guyon, U.V. Luxburg, S. \\nBengio, H. Wallach, R. Fergus, S. \\nVishwanathan, R. Garnett, eds. \\nAdvances in Neural Information \\nProcessing Systems 30, Curran \\nAssociates, Inc., Boston, 2017, \\n5585–5595.\\n 25. Odena, A., Olah, C., Shlens, J. \\nConditional image synthesis with \\nauxiliary classifier gans. arXiv preprint \\narXiv:1610.09585 (2016).\\n 26. Oord, A. v. d., Li, Y., Babuschkin, I.,  \\nSimonyan, K., Vinyals, O., \\nKavukcuoglu, K., Driessche, G. v. d., \\nLockhart, E., Cobo, L.C., Stimberg, F.,  \\net al. Parallel wavenet: Fast  \\nhigh-fidelity speech synthesis. arXiv \\npreprint arXiv:1711.10433 (2017).\\n 27. Radford, A., Metz, L., Chintala, S.  \\nUnsupervised representation learning \\nwith deep convolutional generative \\nadversarial networks. arXiv preprint \\narXiv:1511.06434 (2015).\\n 28. Ratliff, L.J., Burden, and S.A., Sastry, \\nS.S. Characterization and computation \\nof local nash equilibria in continuous \\ngames. In Communication, Control, \\nand Computing (Allerton), 2013 51st \\nAnnual Allerton Conference on. IEEE, \\n(2013), 917–924.\\n 29. Salimans, T., Goodfellow, I.,  \\nZaremba, W., Cheung, V., Radford, A.,  \\nChen, X. Improved techniques for \\ntraining gans. In Advances in Neural \\nInformation Processing Systems \\n(2016), 2234–2242.\\n 30. Shrivastava, A., Pfister, T., Tuzel, O.,  \\nSusskind, J., Wang, W., Webb, R.  \\nLearning from simulated and \\nunsupervised images through \\nadversarial training.\\n 31. Theis, L., van den Oord, A., Bethge, \\nM. A note on the evaluation of \\ngenerative models. arXiv:1511.01844 \\n(Nov 2015).\\n 32. Unterthiner, T., Nessler, B., \\nKlambauer, G., Heusel, M., Ramsauer, \\nH., Hochreiter, S. Coulomb GANs: \\nProvably optimal Nash equilibria via \\npotential fields. arXiv preprint \\narXiv:1708.08819 (2017).\\n 33. Wu, Y., Burda, Y., Salakhutdinov, R.,  \\nGrosse, R. On the quantitative analysis \\nof decoder-based generative models. \\narXiv preprint arXiv:1611.04273 (2016).\\n 34. Yeh, R., Chen, C., Lim, T.Y., Hasegawa-\\nJohnson, M., Do, M.N. Semantic image \\ninpainting with perceptual and \\ncontextual losses. arXiv preprint \\narXiv:1607.07539 (2016).\\n 35. Zhu, J.-Y., Park, T., Isola, P., Efros, A.A.  \\nUnpaired image-to-image translation  \\nusing cycle-consistent adversarial \\nnetworks. arXiv preprint \\narXiv:1703.10593 (2017).\\nIan Goodfellow, written while at Google \\nBrain.\\nJean Pouget-Abadie, Mehdi Mirza,  \\nBing Xu, David Warde-Farley,  \\nSherjil Ozair, Aaron Courville, and  \\nYoshua Bengio, Université de Montréal.\\nFinal submitted 5/9/2018.\\nCopyright held by authors/owners. Publication rights licensed to ACM.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='33ec6d0a-c997-4271-8c0f-19e86c24c16c', embedding=None, metadata={'page_label': '1', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e83b2fea-a9d3-4a85-88b4-933323286917', embedding=None, metadata={'page_label': '2', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The\\tDivine\\nComedy\\t(x) q \\nQuery \\nEncoder \\nq(x) \\nMIPS p θ \\nGenerator\\xa0pθ\\n(Parametric) \\nMargin- \\nalize \\nThis\\t14th\\tcentury\\twork\\nis\\tdivided\\tinto\\t3\\nsections:\\t\"Inferno\",\\n\"Purgatorio\"\\t&\\n\"Paradiso\"\\t\\t\\t\\t\\t\\t\\t\\t\\t(y)\\nEnd-to-End Backprop through q  and\\xa0p θ \\nBarack\\tObama\\twas\\nborn\\tin\\tHawaii.(x)\\nFact Veriﬁcation: Fact Query\\nsupports\\t(y)\\nQuestion Generation\\nFact Veriﬁcation:\\nLabel Generation\\nDocument \\nIndex \\nDefine\\t\"middle\\tear\"(x)\\nQuestion Answering:\\nQuestion Query\\nThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t(y)\\nQuestion Answering:\\nAnswer GenerationRetriever pη \\n(Non-Parametric) \\nz 4 \\nz 3 \\nz 2 \\nz 1 \\nd(z) \\nJeopardy Question\\nGeneration:\\nAnswer Query\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\\nIndex) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we\\ntreat zas a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\\ncan be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64, 55], stack-\\naugmented networks [25] and memory layers [ 30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.\\nOur results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequencexto retrieve text documents zand use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pη(z|x) with parameters ηthat returns (top-K truncated)\\ndistributions over text passages given a query xand (ii) a generator pθ(yi|x,z,y 1:i−1) parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56bdb2bb-efe2-4891-bf7b-4c3ed2904564', embedding=None, metadata={'page_label': '3', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='by θthat generates a current token based on a context of the previous i−1 tokens y1:i−1, the original\\ninput xand a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence, the model uses the same document\\nto predict each target token. The second approach, RAG-Token, can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npη and pθ components, as well as the training and decoding procedure.\\n2.1 Models\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence(y|x) ≈\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(y|x,z) =\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)\\nN∏\\ni\\npθ(yi|x,z,y 1:i−1)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\\npRAG-Token(y|x) ≈\\nN∏\\ni\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x,z,y 1:i−1)\\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class\\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2 Retriever: DPR\\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\npη(z|x) ∝exp\\n(\\nd(z)⊤q(x)\\n)\\nd(z) =BERTd(z), q(x) =BERTq(x)\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\\ntop-k(pη(·|x)), the list of kdocuments zwith highest prior probability pη(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory.\\n2.3 Generator: BART\\nThe generator component pθ(yi|x,z,y 1:i−1) could be modelled using any encoder-decoder. We use\\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\\nxwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters θas the parametric memory henceforth.\\n2.4 Training\\nWe jointly train the retriever and generator components without any direct supervision on what\\ndocument should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2bc5ad15-21f7-409b-b74c-7b51525aed21', embedding=None, metadata={'page_label': '4', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='minimize the negative marginal log-likelihood of each target, ∑\\nj−log p(yj|xj) using stochastic\\ngradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [20].\\nWe do not ﬁnd this step necessary for strong performance, and keep the document encoder (and\\nindex) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.\\n2.5 Decoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximatearg maxyp(y|x).\\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p′\\nθ(yi|x,y1:i−1) = ∑\\nz∈top-k(p(·|x)) pη(zi|x)pθ(yi|x,zi,y1:i−1) To\\ndecode, we can plug p′\\nθ(yi|x,y1:i−1) into a standard beam decoder.\\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses\\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis y we run an additional forward pass for each document z for which y does not\\nappear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across\\nbeams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer\\noutput sequences, |Y|can become large, requiring many forward passes. For more efﬁcient decoding,\\nwe can make a further approximation that pθ(y|x,zi) ≈0 where ywas not generated during beam\\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Y has\\nbeen generated. We refer to this decoding procedure as “Fast Decoding.”\\n3 Experiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\\nkdocuments for each query. We consider k∈{5,10}for training and set kfor test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1 Open-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)\\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\\nQA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2 Abstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\\ntext generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='69ed252f-bb62-4b3c-a080-d78f6fb64e97', embedding=None, metadata={'page_label': '5', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\n“What is the weather in V olcano, CA?” so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3 Jeopardy Question Generation\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\nFor example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciﬁcity as high mutual dependence between the input and output [ 33]. We follow\\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptions—quuestion A is better, question B is better, both are good, or neither is good.\\n3.4 Fact Veriﬁcation\\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by\\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER\\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\\ntwo variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\\nretriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\\nencoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bc6fb33d-d8bd-40c8-9b34-b6827dd4bf96', embedding=None, metadata={'page_label': '6', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel NQ TQA WQ CT\\nClosed\\nBook\\nT5-11B [52] 34.5 - /50.1 37.4 -\\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBook\\nREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2\\nTable 2: Generation and classiﬁcation Test Scores.\\nMS-MARCO SotA is [4], FEVER-3 is [68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2\\nB-1 QB-1 R-L B-1 Label Acc.\\nSotA - - 49.8* 49.9* 76.8 92.2 *\\nBART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\\ncases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\\nspeciﬁc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows\\nan example. When generating “Sun”, the posterior is high for document 2 which mentions “The\\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\\ngenerated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\\nThis observation suggests that the generator can complete the titles without depending on speciﬁc\\ndocuments. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\\nﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding\"The\\nSun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\\nBART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A\\nwith \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows\\nhow parametric and non-parametric memories work together—the non-parametric component helps\\nto guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\\n4.4 Fact Veriﬁcation\\nTable 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='043100db-585f-48b4-be94-1910bc0253d2', embedding=None, metadata={'page_label': '7', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Document 1: his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\n”A Farewell to Arms”(1929) ...\\nDocument 2: ... artists of the 1920s ”Lost Generation” expatriate\\ncommunity. His debut novel,”The Sun Also Rises”, was published\\nin 1926.\\nBOS\\n”\\nTheSunAlso\\nR ises\\n” is a\\nnovel\\nby this\\nauthor\\nof ” A\\nFarewellto\\nArms\\n”\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5\\nFigure 2: RAG-Token document posterior p(zi|x,yi,y−i) for each generated token for input “Hem-\\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\\nTask Input Model Generation\\nMS-\\nMARCO\\ndeﬁne middle\\near\\nBART ?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotland\\nBART The currency needed in Scotland is Pound sterling.\\nRAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener\\n-ation\\nWashington\\nBART ?This state has the largest number of counties in the U.S.\\nRAG-T It’s the only U.S. state named for a U.S. president\\nRAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\\nFor 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the topkdocuments retrieved\\nby RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\\nmore diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\\nRAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a4cde31f-beaa-469d-a1e4-9df9d035bd10', embedding=None, metadata={'page_label': '8', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality Speciﬁcity\\nBART better 7.1% 16.8%\\nRAG better 42.7% 37.4%\\nBoth good 11.7% 11.8%\\nBoth poor 17.7% 6.9%\\nNo majority 20.8% 20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO Jeopardy QGen\\nGold 89.6% 90.0%\\nBART 70.7% 32.4%\\nRAG-Token 77.8% 46.8%\\nRAG-Seq. 83.5% 53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.\\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\\nbetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n39\\n40\\n41\\n42\\n43\\n44NQ Exact Match RAG-Tok\\nRAG-Seq\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n40\\n50\\n60\\n70\\n80NQ Answer Recall @ K\\nRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n48\\n50\\n52\\n54\\n56Bleu-1 / Rouge-L score\\nRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\\ngeneration [36], dialogue [ 41, 65, 9, 13], translation [ 17], and language modeling [ 19, 27]. Our\\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a2d3a388-7f5c-45dc-83b5-97bd311d6fac', embedding=None, metadata={'page_label': '9', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-\\nmarks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.\\nFor further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval There is signiﬁcant work on learning to retrieve documents in information\\nretrieval, more recently with pre-trained, neural language models [ 44, 26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,\\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\\ncan be ﬁne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\\nwork. Other work improves the ability of dialog models to generate factual text by attending over\\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have proved successful in a number of domains including\\nMachine Translation [ 18, 22] and Semantic Parsing [21]. Our approach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6 Discussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0503c191-aac2-49c9-b066-e83642236acc', embedding=None, metadata={'page_label': '10', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Broader Impact\\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\\npages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .\\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='abed981e-3cdd-4e07-8c4b-6687ab0f0a95', embedding=None, metadata={'page_label': '11', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423.\\n[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\n[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082.\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\\nanthology/P19-1346.\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH.\\n[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,\\n2020. URL https://arxiv.org/abs/2004.07202.\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710.\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\\nexceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\\nhttp://arxiv.org/abs/1705.08807.\\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In AAAI Conference on Artiﬁcial Intelligence , 2018. URL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 , 32nd\\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.\\n32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,\\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='45c70749-0e11-4d71-98ce-76427852c443', embedding=None, metadata={'page_label': '12', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems 31 , pages 10052–\\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\\nanthology/2020.acl-main.228.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\\naugmented recurrent nets. In Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1 , NIPS’15, page 190–198, Cam-\\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\\ntion through memorization: Nearest neighbor language models. In International Conference on\\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\\ntion Answering Research. Transactions of the Association of Computational Lin-\\nguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf .\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='73a3a6b7-60af-482d-80cd-58ee235945bf', embedding=None, metadata={'page_label': '13', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612.\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint\\narXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pages 110–119, San Diego, California, June 2016. Association for Computational\\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv\\npreprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\\nveriﬁability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\\n//arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\\nprecision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b6ab939e-1d47-4cd3-9c0e-4e3b408a30f0', embedding=None, metadata={'page_label': '14', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\\nProceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf.\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009.\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\\n2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.\\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong\\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models’ factual predictions. In\\nAutomated Knowledge Base Construction, 2020. URL https://openreview.net/forum?\\nid=025X0zPfn.\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\\nproving Language Understanding by Generative Pre-Training, 2018. URL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf.\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners, 2019. URL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\\nmodels_are_unsupervised_multitask_learners.pdf.\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\\n2002.08910.\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019.\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv, abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,Advances\\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3b890551-6fe3-4aa7-9955-34e3ec1dad6d', embedding=None, metadata={'page_label': '15', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\\nAAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\\nanthology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information\\nProcessing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\\nranking in open-domain question answering. In ICLR, 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence\\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\\nInternational Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713.\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='59308582-ab5e-449c-bb59-0d5258625552', embedding=None, metadata={'page_label': '16', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:\\nState-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pages 2495–2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='99d986e7-278a-4234-909a-8220933adffc', embedding=None, metadata={'page_label': '17', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA Implementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB Human Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC Training setup Details\\nWe train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision\\nﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search\\nwith FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring∼100\\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found athttps://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nat https://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d2890b4e-b62a-487e-85aa-39cc79fc094e', embedding=None, metadata={'page_label': '18', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='D Further Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to ﬁnd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\\nthe model with each (q,a) pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-\\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\\nTo overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\nTriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\\nTriviaQA Web Development split. Roberts et al.[52] used the TriviaQA ofﬁcial Wikipedia test set\\ninstead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\\nappendix of [14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being\\nsimpler to answer from Wikipedia.\\nE Further Details on FEVER\\nFor FEVER classiﬁcation, we follow the practice from [ 32], and ﬁrst re-generate the claim, and\\nthen classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\\nwe explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to\\nus, directly tackling this task is not straightforward. We hope to address this in future work.\\nF Null Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\\nto model cases where no useful information could be retrieved for a given input. Here, ifkdocuments\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in\\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\\ndocuments for questions that are less likely to beneﬁt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG Parameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56a6d0ff-845c-4c4e-acf6-3561a02c98b8', embedding=None, metadata={'page_label': '19', 'file_name': 'RAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\RAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask Train Development Test\\nNatural Questions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\\npoint precision to manage memory and disk footprints.\\nH Retrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al.[46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='64475150-1d44-410a-89ed-e18bdb1f96d1', embedding=None, metadata={'page_label': '1', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='XRAG: Cross-lingual Retrieval-Augmented Generation\\nWei Liu1*, Sony Trenous2, Leonardo F. R. Ribeiro2, Bill Byrne2, Felix Hieber2\\n1Heidelberg Institute for Theoretical Studies gGmbH\\n2Amazon AGI\\nwei.liu@h-its.org, {trenous,leonribe,willbyrn,fhieber}@amazon.com\\nAbstract\\nWe propose XRAG, a novel benchmark design-\\ned- to evaluate the generation abilities of LLMs\\nin cross-lingual Retrieval-Augmented Genera-\\ntion (RAG) settings where the user language\\ndoes not match the retrieval results. XRAG\\nis constructed from recent news articles to en-\\nsure that its questions require external know-\\nledge to be answered. It covers the real-world\\nscenarios of monolingual and multilingual re-\\ntrieval, and provides relevancy annotations for\\neach retrieved document. Our novel dataset\\nconstruction pipeline results in questions that\\nrequire complex reasoning, as evidenced by the\\nsignificant gap between human and LLM per-\\nformance. Consequently, XRAG serves as a\\nvaluable benchmark for studying LLM reason-\\ning abilities, even before considering the addi-\\ntional cross-lingual complexity. Experimental\\nresults on five LLMs uncover two previously\\nunreported challenges in cross-lingual RAG: 1)\\nin the monolingual retrieval setting, all evalu-\\nated models struggle with response language\\ncorrectness; 2) in the multilingual retrieval set-\\nting, the main challenge lies in reasoning over\\nretrieved information across languages rather\\nthan generation of non-English text.\\n1 Introduction\\nRetrieval-augmented generation (RAG) augments\\nlarge language models (LLMs) by retrieval of rele-\\nvant documents with the aim of improving response\\nquality (Lewis et al., 2020). The widespread adop-\\ntion of RAG has prompted many recent studies\\nto evaluate specific capabilities of LLMs in RAG\\nsettings, such as robustness to noise (Wang et al.,\\n2024), information integration (Chen et al., 2024b),\\ntime sensitivity (Kasai et al., 2023), multi-hop rea-\\nsoning (Tang and Yang, 2024) and conversational\\nQA (Roy et al., 2024). Notably, these evaluations\\nare in monolingual settings in which questions and\\nretrieved documents are in the same language.\\n*Work done during an internship at Amazon.\\nGerman Query English\\nRetrieve\\nDoc\\nPrompt\\nQueryLLMGerman Answer\\nInputGenerate\\nEnglish\\nDocuments\\nDoc Doc\\n(a) Cross-lingual RAG with monolingual retrieval.\\nGerman Query English\\nRetrieve\\nDoc\\nPrompt\\nQuery\\nDocuments\\nLLMGerman Answer\\nInputGenerate\\nEnglish\\nGerman\\nDocuments\\nDoc Doc\\nDoc\\nGerman\\nDoc Doc\\nRetrieve\\n(b) Cross-lingual RAG with multilingual retrieval.\\nFigure 1: Two cases of cross-lingual RAG: (a) mono-\\nlingual retrieval, where the LLM uses retrieved English\\ndocuments to respond to a German query; (b) multilin-\\ngual retrieval, where the LLM uses retrieved English\\nand German documents to respond to a German query.\\nReal-world deployments of RAG systems also\\nneed to handle cross-lingual use cases, where the\\nuser’s language does not match that of the retrieved\\ndocuments. The simplest scenario is Cross-lingual\\nRAG with Monolingual Retrieval (Asai et al.,\\n2023), where users in multiple locales are served by\\na single RAG system that accesses an English-only\\nknowledge base, as illustrated in Figure 1a. This\\nsetup applies to, for example, a general-purpose\\nRAG system that relies solely on English web\\nsearch or a corporate helpdesk with an internal\\ndatabase available only in English. A more com-\\nplex scenario is Cross-lingual RAG with Multilin-\\ngual Retrieval, where RAG systems combine infor-\\nmation from both English and the user’s language\\nto generate a response (see Figure 1b). This is a\\ncommon situation in that native-language sources\\noften contain culturally or geographically specific\\nknowledge, with English resources providing addi-\\narXiv:2505.10089v1  [cs.CL]  15 May 2025', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bff2202f-ab9c-4b49-9837-fa74d0003b6e', embedding=None, metadata={'page_label': '2', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tional, more general information.1\\nDue to the absence of relevant benchmarks, we\\nlack an understanding of how well LLMs can\\nhandle such cross-lingual RAG scenarios. A po-\\ntential solution is to use existing cross-lingual\\nopen-domain question-answering datasets, such\\nas XQA (Liu et al., 2019) and XOR QA (Asai\\net al., 2021), for evaluation (Chirkova et al., 2024).\\nYet these datasets only cover limited cross-lingual\\nscenarios; in particular, the documents used to an-\\nswer questions are in English, which hinders the\\nevaluation of LLMs in more complex multilingual\\nscenarios (i.e., Figure 1b). Moreover, the ques-\\ntions in these datasets tend to be relatively simple\\n(e.g. span extraction questions) and often can be\\nanswered without retrieval.2 Due to these short-\\ncomings, these datasets do not measure the true\\ncross-lingual capabilities of LLMs in RAG settings.\\nTo address this gap, we introduce XRAG, a\\nbenchmark for evaluating the Question Answer-\\ning capabilities of LLMs in cross-lingual RAG\\nscenarios, where some information must be ex-\\ntracted from retrieved documents that are not in the\\nuser’s language. The benchmark features natural-\\nsounding questions that require cross-document\\nreasoning and are challenging for LLMs even in\\nan English monolingual RAG setting (GPT-4o\\nachieves only 62.4% accuracy, see Table 4). We\\ndevelop a novel LLM-based question generation\\nworkflow using recent news articles, ensuring that\\ncurrent frontier models are unable to answer the\\nquestions without retrieval (GPT-4o accuracy is\\n6.3% without retrieval, see Table 3). To guarantee\\na high-quality dataset, we employ extensive human\\nQuality Assurance, resulting in few ambiguous or\\nnoisy questions (under 8%, see Section 5.2). In ad-\\ndition to English, the benchmark spansfour widely\\nspoken and linguistically diverse languages(Ara-\\nbic, Chinese, German, and Spanish).\\nXRAG comprises two sub-tasks, corresponding\\nto the monolingual retrieval and the multilingual\\nretrieval settings of cross-lingual RAG. For each\\nnon-English language, we provide a directly com-\\nparable English monolingual RAG baseline task.\\nEach instance in the XRAG benchmark consists of\\na question, a gold answer, two supporting articles\\nthat together answer the question, and six topically\\n1An initial study on a proprietary dataset of real-world\\nLLM traffic from non-English users in Germany, Japan, and\\nSpain found that using only English or native-language search\\nresults was inferior to combining both (see Appendix A).\\n2 Chirkova et al. (2024) shows that 47.5% of questions in\\nXORQA can be answered by Command-R without retrieval.\\nrelated but non-answering distracting articles. This\\nallows us to approximate realistic RAG settings\\nwith imperfect retrieval in evaluating the Question\\nAnswering abilities of LLMs.\\nWe evaluate five LLMs, including both closed-\\nand open-source models, on XRAG. In summary,\\nour contributions are:\\n(1) We introduce XRAG, a novel benchmark de-\\nsigned to evaluate the performance of LLMs\\nin two cross-lingual RAG scenarios.\\n(2) We propose a novel method for generating\\nchallenging cross-document QA pairs from\\nNews Crawl, resulting in natural questions\\nthat current LLMs cannot answer using only\\ntheir parametric knowledge.\\n(3) We find that in the monolingual retrieval set-\\nting, all evaluated LLMs face issues with Re-\\nsponse Language Correctness-an issue that\\nhas received little attention from the research\\ncommunity.\\n(4) In the multilingual retrieval setting, the pri-\\nmary challenge for LLMs does not lie in non-\\nEnglish generation, but in reasoning over re-\\ntrieved information across languages.\\n2 Related Work\\nThere are extensive recent investigations into char-\\nacterizing the Question Answering capabilities of\\nLLMs in RAG settings. Vu et al. (2024) construct a\\na dynamic QA benchmark, FreshQA, that tests the\\nability of LLMs to use up-to-date world knowledge\\nto solve questions. Chen et al. (2024c) introduce\\nRGB as a benchmark to analyze fundamental abili-\\nties of LLMs in RAG systems, such as noise robust-\\nness and negative rejection. Tang and Yang (2024)\\npropose MultiHop-RAG, which focuses on whether\\nretrieval-enhanced LLMs can retrieve and reason\\nover multiple pieces of supporting evidence. The\\nComprehensive RAG Benchmark, created by Yang\\net al. (2024), aims to assess whether LLMs are able\\nto answer different types of questions, ranging from\\nsimple to complex. Thakur et al. (2024b) present\\nMIRAGE-Bench, a multilingual RAG benchmark\\nconstructed from Wikipedia, to evaluate RAG sys-\\ntems performance in different languages. As noted,\\nthese are in monolingual settings, whereas we aim\\nto benchmark LLMs performance in cross-lingual\\nRAG scenarios. Chirkova et al. (2024) is one of few\\nstudies that evaluate the cross-lingual capabilities\\nof LLMs in RAG systems. They conduct an analy-\\nsis of existing cross-lingual open-domain question-', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ffe3f8aa-d5d6-43dc-b5d6-43209df89a00', embedding=None, metadata={'page_label': '3', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='RAG SettingField LanguageContentQuestionGermanWie viel haben Walmart und ALDI zusammen für die Opfer des Hurrikans Helene 2024 gespendet?AnswerGermanDie gesamten Spenden überstiegen 11 Millionen Dollar.EnglishWalmart, Sam’s Club and the Walmart Foundation are increasing their commitment to $10 million to Hurricane Helene Relief Effort...SupportingArticlesEnglishThe American Red Cross recognizes ALDIfor its pledge of $1,000,000. By making a donation to Hurricane Helene Relief...EnglishWalmart Canada reaches new giving milestone of $750 million raised and donated to charities and non-profits across Canada...\\nMonolingualRetrievalDistractingArticlesEnglishAldi has donated £2,000 to charities in Gloucestershire to help support those in need during the school holidays. The donations...QuestionGermanWelches Land gewann seine erste Goldmedaille bei den Olympischen Spielen 2024 früher, die Vereinigten Staaten oder Deutschland?\\nAnswerGermanDeutschland. Beide Länder gewannen am 27. Juli bei den Schwimmwettbewerben ihre ersten Goldmedaillen, aber die USA gewannenihre Medaille erst im letzten Wettkampf des Tages – später als Deutschland.GermanLukas Märtens hat am 27. Juli in Paris den olympischen Titel über 400 m Freistil gewonnen. Der Magdeburger siegte in 3:41,78...SupportingArticlesEnglishIn the last swimming race of July 27, the U.S. took its first gold medal of the 2024 Olympics, winning the 4×100-meter freestyle...GermanIm Rahmen von noch bevorstehenden Qualifikationsevents können sich weitere Sportler noch für die Spiele in Paris qualifizieren...\\nMultilingualRetrievalDistractingArticlesEnglishThe United States Olympic & Paralympic Committee have announced the 592-member 2024 U.S. Olympic team ready to compete...\\nTable 1: Two instances from XRAG, each consisting of a question, a gold answer, two supporting articles, and six\\ndistracting articles (two are shown). In the monolingual retrieval setting, all supporting and distracting articles are\\nin English; in the multilingual retrieval setting, the supporting and distracting articles are in the question language\\nand in English. LLMs should answer these questions based on the supporting articles while ignoring the distractors.\\nanswering datasets (Asai et al., 2021). Motivated\\nby this prior work, XRAG is a new cross-lingual\\nbenchmark that covers a wider range of scenarios\\nand consists of questions designed to require exter-\\nnal knowledge to answer, thereby providing a more\\naccurate reflection of the cross-lingual capabilities\\nof LLMs in RAG.\\nEvaluation of cross-lingual NLP systems is a\\nlong-standing research problem. Relatively recent\\nwork has focused on performance in specific NLP\\ntasks such as NLI (Conneau et al., 2018), summa-\\nrization (Wang et al., 2022), retrieval question an-\\nswering (Roy et al., 2020), and open-domain ques-\\ntion answering (Toutanova et al., 2021). With the\\nadvent of large language models, cross-lingual eval-\\nuation has expanded to include few-shot or even\\nzero-shot settings. Wang et al. (2023) investigate\\nGPT-4 performance for cross-lingual summariza-\\ntion in a zero-shot setting and find that it performs\\ncompetitively with finetuned mBART-50. Ahuja\\net al. (2023) further evaluate the performance of\\ngenerative models on 15 tasks, covering classifica-\\ntion, sequence labeling, and generation.\\nWe note that our focus is on retrieval augmented\\ngeneration from multilingual document retrieval,\\nand not on the document retrieval task itself. How-\\never in Sections 4.1 and 4.4 we discuss how we\\nuse monolingual and multilingual dense document\\nretrieval techniques in constructing XRAG. Our\\nwork aligns with recent efforts in evaluating cross-\\nlingual performance of LLMs, with a focus on re-\\ntrieval augmented generation and cross-lingual an-\\nswer generation in particular.\\n3 XRAG - Cross-lingual RAG Benchmark\\nWe define the task of cross-lingual RAG as fol-\\nlows: given a question q, the LLM is prompted\\nto generate an answer ˜a in the same language as\\nthe question by referring to a collection of m arti-\\ncles D ={d1, d2, ..., dm} that contains articles in\\na language different than the question:\\n˜a ← LLM(q, D, prompt)\\nLanguage(q) =Language(˜a)\\n∃di ∈ D, Language(di) ̸= Language(q)\\n(1)\\nFigure 1 shows two cases, in which LLMs need to\\nuse information from English articles to generate\\nGerman responses to German questions. The goal\\nof our benchmark is to enable an understanding\\nof how well LLMs perform generation in such\\ncross-lingual RAG scenarios.\\nEach instance (q, a,D+, D−) in XRAG consists\\nof a question q, a golden answer a, two support-\\ning articles D+, and several distracting articles D−.\\nThe supporting articles each contribute partial in-\\nformation needed to answer the question, and only\\ntogether do they provide a complete answer; in con-\\ntrast, the distracting articles are topically related\\nbut cannot answer the question. Taken together,\\nthis simulates a realistic RAG scenario with imper-\\nfect retrieval, where we can control the quality of\\nthe grounding by the inclusion of distractors. Ques-\\ntions in XRAG are cross-document questions, re-\\nquiring reasoning across the two supporting articles\\nto answer, while ignoring the distracting articles.\\nOur benchmark considers two real-world cross-\\nlingual RAG scenarios: the monolingual retrieval\\nscenario and the multilingual retrieval scenario.\\nIn the monolingual retrieval setting, LLMs rely\\non English articles to generate an answer. This\\noccurs when users in multiple locales are served by\\na single cross-lingual RAG system that has access\\nonly to an English knowledge base. In this paper,\\nwe consider questions in four languages: German', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bfca6dad-6c5d-4223-9598-f1fe843a327f', embedding=None, metadata={'page_label': '4', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Monolingual Retrieval Setting\\nMultilingual Retrieval Setting\\nNews\\n1 Identifying\\npairs of\\nrelated articles\\n2 Cross-\\ndocument Q&A\\ngeneration\\n3 Quality\\nControl\\n4 Human\\nTranslation\\n5 English\\nRetrieval+ Filtering\\n5 Language X\\nRetrieval+ Filtering\\nNews\\nDoc 1 Doc 2\\nEnglish + English\\nDoc 1 Doc 2\\nX + English\\nDiscard\\nNo\\nYes\\nDoc 8Doc 7Doc 6Doc 5Doc 4Doc 3\\nEnglish\\nDoc 8Doc 7Doc 6Doc 5Doc 4Doc 3\\nEnglish X\\nQ&A Q&A\\nX + English\\nD+\\nD+\\nD−\\nD−\\n𝑞,𝑎\\nFigure 2: Each instance (q, a,D+, D−) in XRAG — where q is the question, a the gold answer, D+ the supporting\\narticles, and D− the distractors — is constructed as follows: (1) find two related articles; (2) generate an English\\ncross-document Q&A pair using the two articles; (3) evaluate the quality of the Q&A pair; (4) translate the Q&A\\npair into language X ∈ {German, Spanish, Chinese, Arabic}; and (5) collect distracting articles for the question.\\n(de), Spanish (es), Chinese (zh), and Arabic (ar):\\nLanguage(q) ∈ {de, es, zh, ar}\\nLanguage(D+) =Language(D−) =en (2)\\nThese four are widely used in the research commu-\\nnity (Macko et al., 2023) and represent a range of\\ncross-lingual challenges, ranging from easy (es-en)\\nto challenging (zh-en) (Yang et al., 2022).\\nIn a multilingual retrieval setting, LLMs use\\narticles in both the question language and other lan-\\nguages to answer a question. This corresponds to\\na cross-lingual RAG scenario where documents in\\na resource-rich language provide additional infor-\\nmation for LLMs to answer questions in a second\\nlanguage. Similarly, we consider four languages:\\nLanguage(q) ∈ {de, es, zh, ar}\\nLanguage(D+) ={en, Language(q)}\\nLanguage(D−) ={en, Language(q)}\\n(3)\\nTable 1 gives examples of monolingual retrieval\\nand multilingual retrieval from XRAG.\\n4 XRAG Construction\\nFigure 2 shows the overall XRAG construction\\nprocess. We begin with English, German, Span-\\nish, Chinese, and Arabic news articles from News\\nCrawl between June 1, 2024, and November 30,\\n2024. This timeframe ensures that the articles are\\ndated after the knowledge cutoff of LLMs such as\\nGPT-4 and Claude 3.5. Questions created from\\nthese articles are more likely to require external\\nknowledge to answer.\\n4.1 Identifying pairs of related articles\\nTo generate natural cross-document questions from\\na pair of articles, the articles must be topically re-\\nlated; otherwise, the generated questions may seem\\nartificial (Welbl et al., 2018).\\nFor the monolingual retrieval setting, we con-\\nstruct a bipartite graph linking English articles with\\nthe entities in their titles. We then use depth-first\\nsearch to find pairs of articles that share at least\\ntwo entities in their titles. These article pairs serve\\nas related English articles for generating cross-\\ndocument questions.\\nFor the multilingual retrieval setting, we use\\ninternational events from Wiki 2024 and a multi-\\nlingual dense retriever to search across different\\nlanguages for articles related to the events. We\\nthen group articles in different languages about the\\nsame event to form related article pairs.\\nWe provide a more detailed explanation of how\\nto locate relevant articles in English or across lan-\\nguages in Appendices B.2.\\n4.2 English cross-document Q&A generation\\nWe design an LLM-based workflow3 to generate\\nnatural and coherent English cross-document\\nquestions from news articles. Figure 3 shows\\nan overview of the generation workflow.\\nStep 1: Summary Generation . Given a pair of\\nrelated articles either in English or in English and\\nanother language, we prompt the LLM to create a\\nsummary for each article that (1) is accurate and\\nconcise; (2) covers the key points; and (3) has lit-\\ntle lexical overlap with the article (the prompt is\\nshown in Figure 10). These summaries are then\\nused to generate questions in Step 2. There are two\\nreasons for this: generating questions directly from\\narticles often leads to questions with high overlap\\n3We use GPT-4o-2024-08-06.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='73af176e-55b7-42be-b10b-c70190afdd3a', embedding=None, metadata={'page_label': '5', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='llmpdf\\n1 Summary Generation\\nAm 27. Juli besiegte Lukas Märtens im \\n400-Meter-Freistil Elijah Winnington und \\ngewann die erste Goldmedaille für \\nDeutschland bei den Olympischen \\nSpielen in Paris.\\nThe United States won its first Olympic \\ngold medal on July 27 in the final \\nswimming event of the day—the 4×100-\\nmeter freestyle relay.\\nllmpdf\\n2 Simple Q&A Generation\\n1 Q: In which event did Germany win its first gold medal?\\nA: 400-meter freestyle on July 27.\\n2 Q:Which swimmer did Lukas Märtens beat to win the gold? \\nA: Elijah Winnington.\\n3 Q: When did the U.S. win its first gold medal?\\nA: July 27.\\n4 Q: In which event did the U.S. win the gold medal?\\nA: The final swimming event on July 27.\\nllmpdf\\n3 Cross-document Q&A Generation\\nSelect Q&A pairs that have connections\\n1\\n3\\n4\\nQ: Which country won its first gold medal earlier at the 2024 Olympic Games, the U.S. \\nor Germany?\\nA: Germany. Both won their first gold on July 27 in swimming, but the U.S. won its \\nmedal in the final race of the day, after Germany.\\nLLM\\nfirst gold, July 27,\\nswimming, final…\\nLLM\\nLLM\\nGenerate cross-document Q&A from the selected simple QA pairs and two summaries\\nLLM\\nLLM\\nLLM\\nSummary 1\\nSummary 2\\nFigure 3: LLM-based workflow for generating English cross-document questions from a pair of related articles: (1)\\ngenerate a summary for each article; (2) create simple English Q&A pairs from each summary that require only\\none-step reasoning; (3) identify connections between the two sets of Q&A pairs, select related ones, and construct a\\nnew Q&A pair that requires reasoning across multiple pieces of information from the selected pairs and summaries.\\nin wording, making it easy to answer through string\\nmatching; and direct question generation from arti-\\ncles can focus on trivial details, whereas generating\\nquestions from summaries tends to produce ques-\\ntions about the main points of the articles.\\nStep 2: Simple Q&A Generation. Our goal is to\\ncreate cross-document questions that require infor-\\nmation from two articles to answer. However, we\\nfind that generating such questions in one step is\\ndifficult. LLMs often create questions that simply\\nlink two separate questions with \"and\". Instead,\\nwe first prompt the LLM to generate simple En-\\nglish Q&A pairs from each summary that can be\\nanswered with one step of reasoning (the prompt\\nis shown in Figure 11). For example, from a Ger-\\nman report about Germany’s first gold medal at\\nthe 2024 Olympics, the LLM generates Q&A pairs\\nlike: (q: In which event did Germany win its first\\ngold medal? a: 400-meter freestyle on July 27).\\nStep 3: Cross-document Q&A Generation. After\\ngenerating simple Q&A pairs from two summaries,\\nwe prompt the LLM to: (1) identify connections be-\\ntween the two sets of Q&A pairs; (2) select related\\nQ&A pairs from the two sets, ensuring that at least\\none pair is chosen from each source; and (3) for-\\nmulate new questions that require reasoning across\\nmultiple pieces of information drawn from the se-\\nlected Q&A pairs. Since the selected Q&A pairs\\noriginate from different source articles, answering\\nthe newly generated questions necessitates integrat-\\ning information from both sources, thus resulting\\nin cross-document questions. For example, using\\nthe simple Q&A pairs in Figure 3, the LLM finds\\nlinks such as \"first gold medal\", \"swimming race\",\\n\"final\" and the date \"July 27\" between the two sets\\nof simple Q&A pairs. The LLM then generates a\\ncomparison question: \"Which country won its first\\ngold medal earlier at the 2024 Olympic Games, the\\nU.S. or Germany?\". We then ask the LLM to gen-\\nerate an answer to the question using information\\nfrom the selected simple Q&A pairs and the two\\nsummaries (the prompt for answer generation is\\nin Figure 16). Inspired by Yang et al. (2024), we\\nfocus on four types of cross-document questions:\\naggregation, comparison, multi-hop, and set ques-\\ntions. We present the definition of the four types of\\nquestions in Table 8, and the prompts to generate\\nthese questions in Figures 12, 13, 14, and 15.\\n4.3 Quality Control and Human Translation\\nThe generated Q&A pairs may contain factual er-\\nrors due to LLM hallucinations (Huang et al., 2024).\\nTo avoid these, we ask a professional multilingual\\nannotation team to verify the quality of the gener-\\nated Q&A pairs (the annotation guideline is shown\\nin Figure 17). They select examples where the ques-\\ntion is natural and answerable and the answer is\\neither correct or correctable by them. For themono-\\nlingual retrieval setting, we engage a professional\\ntranslation team to translate the verified Q&A pairs\\ninto German, Spanish, Chinese, and Arabic. For\\nthe multilingual retrieval setting, translations are', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='9753c029-18d2-4a6f-a8c2-61369e006ad2', embedding=None, metadata={'page_label': '6', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Monolingual RetrievalMultilingual RetrievalDe / Es / Zh / ArDe Es Zh ArExample Number1000 300 300 300 300\\nQuestion\\nAggregation313 86 99 106 95Comparison260 98 109 81 85Multi-hop 215 45 40 65 57Set 212 71 52 48 63\\nAnswerOriginal 872 291 296 284 263Corrected 128 9 4 16 37\\nTable 2: XRAG question type statistics for monolingual\\nand multilingual retrieval settings. Answers requiring\\ncorrection in quality control are also noted.\\nperformed only into language X for Q&A pairs\\nderived from X-English article pairs (e.g., into Ger-\\nman for Q&A pair created from German-English\\narticle pairs). See Appendix B.5 for more details\\non human translation.\\nTable 2 presents the dataset statistics after human\\nverification and translation.\\n4.4 Selecting the Distracting Articles\\nThe grounding articles for each question consist of\\na set of supporting documents and distracting doc-\\numents. The two articles used in cross-document\\nquestion generation serve as supporting documents.\\nFor distracting documents, we search for doc-\\numents that are topically related to the question\\nbut do not answer it. In the monolingual retrieval\\nsetting, we use a multilingual dense retriever to\\nsearch for English documents. In the multilingual\\nretrieval setting we search for documents in both\\nEnglish and the question language. In both settings\\nwe select distracting documents that are published\\nat least two weeks before the supporting articles to\\nensure that the distracting documents do not answer\\nthe question.\\nThis process yields a set of grounding documents\\nfor each question. In the monolingual retrieval set-\\nting, each question will have six distracting doc-\\numents and two supporting documents, all in En-\\nglish. In the multilingual retrieval setting, each\\nquestion will have one supporting document and\\nthree distracting documents in English, and the\\nsame again in the question language.\\n5 Benchmarking with XRAG\\n5.1 Experimental Settings\\nModels. We benchmark five models on XRAG:\\nGPT-4o (OpenAI, 2024), Claude Sonnet-3.5 v1\\n(Anthropic, 2024), Mistral-large (Jiang et al., 2023),\\nCommand-R+ (Cohere, 2024), and Nova Pro (Ama-\\nzon, 2024). These are leading closed- and open-\\nsource multilingual LLMs, and have been widely\\nused in RAG research. Unless otherwise specified,\\n𝑞 LLM 𝑎\\nPanel of LLMs-as-Judges\\n𝑎\\nCorrect\\nIncorrect\\nor\\nDocDoc DocDocDocDocDocDoc\\n~\\nD+ D−\\nLanguage Correctness\\n&\\nFigure 4: Evaluation workflow on XRAG: (1) the evalu-\\nated LLM generates a response ˜a for a question q based\\non two supporting articles D+, and six distracting arti-\\ncles D−; (2) the response is checked for language cor-\\nrectness; (3) a panel of three LLM judges independently\\nassess the factual accuracy of the response based on the\\nquestion q and a gold answer a, with the final judgment\\nbased on majority vote; (4) the final evaluation com-\\nbines the factual judgment and language correctness.\\nthe evaluation is conducted by providing the LLM\\nwith a question, two supporting documents, and six\\ndistracting documents (we show the prompt used\\nto instruct LLMs in using articles to answer ques-\\ntions in Figure 18). Figure 4 shows the evaluation\\nworkflow on XRAG.\\nEvaluation Metrics. The answers in XRAG are\\nusually simple facts stated in one or two sentences.\\nFollowing previous work (Yang et al., 2024; Wang\\net al., 2024; Thakur et al., 2024a), we use the LLM-\\nas-a-Judge method (Zheng et al., 2023), which\\nhas proven good at recognizing when two short\\nanswers mean the same thing (Kamalloo et al.,\\n2023). To avoid self-preference (Panickssery et al.,\\n2024), we use a panel of three LLM judges (GPT-\\n4o, Claude Sonnet-3.5, and Mistral-large) with a\\nmajority vote. We also use a language detector4 to\\ncheck if the model answer is in the same language\\nas the question; it is marked as incorrect otherwise.\\nTo confirm that automatic judging works well, we\\ncompare the LLM judge panel decisions with hu-\\nman judges and find a Cohen’s kappa score of 0.71.\\nFinally, we report each model’s accuracy (%), as de-\\ntermined by the LLM judge panel, which includes\\nthe assessment of language correctness. The tem-\\nplate used for the LLM-as-a-Judge is shown in\\nFigure 20, and more details regarding the correla-\\ntion experiments between the LLM judge panel and\\nhuman evaluations are provided in Appendix C.2.\\n5.2 Establishing QA Performance Bounds\\nOur goal is to create a cross-lingual RAG bench-\\nmark with two key properties: (1) questions should\\nnot be answerable using only the parametric knowl-\\nedge of LLMs, and (2) the task includes challeng-\\ning questions that require complex reasoning to an-\\nswer. To assess whether XRAG meets these proper-\\n4https://github.com/pemistahl/lingua', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2419ee89-9e42-4410-b8b0-6a3471e70cae', embedding=None, metadata={'page_label': '7', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='No Retrieval Oracle Retrieval\\nGPT-4o 6.30 75.40\\nClaude Sonnet 3.5 11.70 67.60\\nMistral-Large 15.20 66.40\\nCommand-R+ 15.30 63.50\\nNova-Pro 13.70 68.20\\nTable 3: LLM QA accuracy in answering XRAG ques-\\ntions without retrieval, and with XRAG supporting ar-\\nticles (but without distracting articles). Questions and\\nsupporting articles are in English (see Figure 21).\\nties, we evaluate performance of several LLMs on\\nEnglish questions from the monolingual retrieval\\nsetting of XRAG (see English Q&A in Figure 2) un-\\nder two conditions: without retrieval, and with the\\ncorrect supporting articles. Table 3 presents the re-\\nsults of models evaluated by the LLM judge panel.\\nAll LLMs perform poorly without retrieval, with ac-\\ncuracy rates falling below 16%. This indicates that\\nthese LLMs cannot answer XRAG questions by\\nrelying solely on their parametric knowledge. Even\\nwhen given supporting articles—simulating ideal\\nretrieval— the best result, achieved by GPT-4o,5\\nreaches only 75.40% accuracy, which is still far\\nbelow human accuracy, as we discuss next. These\\nfindings show that XRAG questions are challeng-\\ning even for advanced LLMs.\\nPerformance Upper Bounds. To establish a hu-\\nman upper bound on our dataset, we hire human\\nannotators to answer 200 English questions from\\nthe monolingual retrievalsetting (see English Q&A\\nin Figure 2) by carefully reading the article pairs\\nused to create the questions. Their performance is\\nevaluated at 85% by the LLM judge panel, which\\nis much higher than that of the best LLM. 5 This\\nshows that even without the cross-lingual challenge,\\nthe dataset is a strong benchmark for LLM reason-\\ning. A manual review of answers judged incorrect\\nby the automated evaluator (see Appendix B.4 for\\nmore details on this manual review) finds that 2%\\nare actually correct, 5% are wrong with gold an-\\nswers being correct, and 8% involve noisy or am-\\nbiguous questions. This sets two separate upper\\nbounds: 85% for human performance, and 92%\\nallowing for noisy questions.\\n5.3 XRAG in Monolingual Retrieval Setting\\nWe first benchmark LLMs in cross-lingual RAG\\nwith the monolingual retrievalsetting of XRAG. To\\nhighlight the cross-lingual challenges, we compare\\nresults with an English monolingual RAG baseline,\\nwhere the input question, supporting articles, and\\n5N.B.: QA pairs are generated by GPT-4o, and evaluation\\nmay be biased in its favor.\\nDoc. Lang. En\\nQuery. Lang.En De Es Zh Ar Avg.\\nGPT-4o 62.4055.90 56.80 54.70 54.70 55.50\\nClaude 3.542.8037.40 40.10 37.60 38.50 38.40\\nMistral-large43.3036.50 39.50 30.60 18.90 31.40\\nCommand-R+45.7039.80 41.20 34.30 33.80 37.30\\nNova-Pro54.0044.80 49.30 37.30 34.30 41.43\\nTable 4: LLM QA accuracy in the XRAG monolingual\\nretrieval setting. Grounding documents consist of two\\nsupporting articles and six distracting articles, all in En-\\nglish (see Figure 22). QA accuracy with English queries\\nprovides a monolingual RAG baseline for comparison.\\n3.7%\\n7.2%\\n12.3%\\n1.0%\\n0.3%\\n5.1%\\n5.9%\\n4.3%\\nGPT-4o Claude 3.5 Mistral-large Command-R+\\nQuery Language\\nGerman\\nChinese\\n2\\n4\\n6\\n8\\n10\\n12Percentage of English Responses (%)\\nLLM\\nFigure 5: Percentage of instances in cross-lingual RAG\\nwith monolingual retrieval (English documents) where\\nLLMs respond in English instead of the German or\\nChinese question language.\\ndistracting articles are all in English. Grounding\\narticles in the monolingual retrieval setting are al-\\nready in English so this baseline experiment simply\\nreplaces the translated question with its original\\nEnglish (see English Q&A in Figure 2).\\nTable 4 shows the results as assessed by the LLM\\njudge panel. XRAG in the monolingual retrieval\\nsetting poses a significant challenge for LLMs,\\nwith all models performing poorly. Among them,\\nGPT-4o achieves the highest average accuracy at\\n55.50%,5 while others score considerably lower,\\nranging from 31.4% to 41.43%. The cross-lingual\\ncapabilities of LLMs vary across languages. Com-\\npared to the English monolingual RAG baseline,\\nall models experience a performance drop when\\nanswering non-English questions, but the severity\\nof this drop differs. GPT-4o and Claude exhibit\\nthe smallest and most consistent declines across\\nlanguages, suggesting more robust multilingual\\nhandling. Command-R+ and Mistral show larger\\nvariability, indicating potential language-specific\\nweaknesses—particularly for Mistral, which suf-\\nfers a 56.3% relative drop in Arabic.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f7f67d13-ec9c-46f6-8b1f-693b213ff109', embedding=None, metadata={'page_label': '8', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Doc. Lang. En+EnDe En+De En+EnEs En+Es En+EnZh En+Zh En+EnAr En+Ar Avg.\\nQuery. Lang. En De En Es En Zh En Ar (crossling.)\\nGPT-4o 63.33 61.67 59.33 56.00 63.00 59.33 60.67 53.33 57.58\\nClaude 3.5 51.00 45.67 46.33 42.67 46.67 48.00 47.33 39.67 44.00\\nMistral-large 45.67 42.00 43.00 39.33 48.67 37.33 43.67 32.00 37.67\\nCommand-R+ 43.67 40.00 42.33 40.33 49.67 36.33 43.33 32.00 37.17\\nNova-Pro 56.33 53.00 49.67 45.33 57.67 49.33 57.33 44.67 48.08\\nTable 5: LLM QA accuracy in the XRAG multilingual retrieval setting, which for each language X consists of a set\\nof questions each accompanied by a supporting document and three distracting documents in language X and the\\nsame again in English. EnX refers to English translations of documents from language X using Google Translate\\n(see Figure 23). En+EnX is a monolingual retrieval baseline setting for the language pair En+X.\\nSurprisingly, we find that LLMs have issues\\nwith Response Language Correctness(RLC), i.e.,\\nthey respond in English instead of the question lan-\\nguage. Figure 5 lists the percentage of cases in the\\nmonolingual retrieval setting where LLMs respond\\nin the wrong language. GPT-4o and Command-R+\\nproduce the fewest RLC errors, while Mistral-large\\nis most affected.\\n5.4 XRAG in Multilingual Retrieval Setting\\nWe now benchmark LLMs in cross-lingual RAG\\nwith the multilingual retrieval setting of XRAG.\\nAs with monolingual retrieval, we construct an\\nEnglish monolingual RAG setting for comparison.\\nWe replace the original non-English questions with\\ntheir English counterparts (questions before human\\ntranslation; see English Q&A in Figure 2). We also\\ntranslate non-English supporting and distracting\\narticles into English using Google Translate.\\nTable 5 presents LLM performance in cross-\\nlingual QA in the multilingual retrieval setting of\\nXRAG. All models exhibit poor performance in\\nthis cross-lingual scenario, with GPT-4o having the\\nhighest average accuracy at 57.58% and Command-\\nR+ the lowest at 37.17%. LLMs also show accu-\\nracy degradations relative to their corresponding\\nEnglish monolingual RAG baseline, despite the\\nlatter being constructed with the assistance of ma-\\nchine translation.\\nTo identify the most challenging aspect of the\\ncross-lingual RAG with multilingual retrieval, we\\nconduct a controlled analysis by gradual conversion\\nto the English monolingual RAG setting. Specif-\\nically, we successively replace the question, sup-\\nporting articles, and distracting articles by their\\nEnglish counterparts from the monolingual RAG\\nbaseline. Table 6 shows the results of GPT-4o.\\nChanging the question (and expected answer) lan-\\nguage from non-English to English only brings a\\nrelatively small average improvement, suggesting\\nthat non-English generation may not be the core\\nGPT-4o En+De En+Es En+Zh En+ArAvg.\\nXRAG-MultiR61.67 56.00 59.33 53.3357.58\\n+EQ 63.00 52.67 60.67 56.6758.25\\n+EQ, +ES65.00 57.33 62.00 60.3361.16\\nMonoRAG63.33 59.33 63.00 60.6761.58\\nTable 6: Controlled analysis of GPT-4o on the mul-\\ntilingual retrieval setting of XRAG, replacing ques-\\ntions (EQ), supporting articles (ES), and distracting ar-\\nticles (ED) with their English counterparts from the\\nEnglish monolingual RAG settings (see Figure 24).\\n\"XRAG-MultiR\" is the multilingual retrieval setting,\\nand \"MonoRAG\" (+EQ, +ES, +ED) is the English mono-\\nlingual RAG baseline setting.\\nchallenge.6 By contrast, replacing non-English\\nsupporting articles with their English translations\\nimproves average accuracy noticeably, indicating\\nthat reasoning over retrieved information across\\nlanguages is challenging for LLMs . Translat-\\ning distracting articles into English also improves\\nperformance, implying that identifying useful in-\\nformation in a mixed-language context is harder\\nthan from a wholly English one. Similar results are\\nobserved with other LLMs, see Appendix D.1.\\n6 Conclusions\\nWe introduce XRAG, a benchmark for evaluating\\nthe generation abilities of LLMs in cross-lingual\\nRAG settings. We introduce a novel LLM-based\\nworkflow for creating questions that require com-\\nplex reasoning and external documents to answer.\\nExperiments reveal that LLMs significantly under-\\nperform humans on XRAG—even without cross-\\nlingual elements—highlighting its utility for assess-\\ning reasoning ability. Further analysis shows that\\nLLMs struggle with response language correctness\\nin the XRAG monolingual retrieval setting and\\nwith reasoning over retrieved content across lan-\\nguages in the XRAG multilingual retrieval setting.\\n6We also find that LLMs rarely have issues with Response\\nLanguage Correctness in the multilingual retrieval setting.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30b033fa-518f-4cad-9a07-3a619ef6b2c3', embedding=None, metadata={'page_label': '9', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='7 Limitations\\nOur work has some limitations. First, our multi-\\nlingual retrieval setting solely covers the scenario\\nof two languages (English and the question lan-\\nguage). However, there may be cases involving\\nretrieval across a set of languages (more than two).\\nNote that our construction pipeline can support\\nexploration in this setup by using more articles\\nto generate questions, which we leave for future\\nwork. Second, we only benchmarked five models\\nin this work because of legal concerns. It would\\nbe interesting to see how other LLMs perform on\\nXRAG. Finally, we could conduct more insightful\\ncontrolled analyses on XRAG, such as exploring\\nthe impact of the number of distracting articles.\\nDue to space limitations, we leave this for future\\nwork.\\nReferences\\nKabir Ahuja, Harshita Diddee, Rishav Hada, Milli-\\ncent Ochieng, Krithika Ramesh, Prachi Jain, Ak-\\nshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed\\nAhmed, Kalika Bali, and Sunayana Sitaram. 2023.\\nMEGA: Multilingual evaluation of generative AI.\\nIn Proceedings of the 2023 Conference on Empir-\\nical Methods in Natural Language Processing, pages\\n4232–4267, Singapore. Association for Computa-\\ntional Linguistics.\\nAmazon. 2024. The amazon nova family of models:\\nTechnical report and model card.\\nAnthropic. 2024. Claude sonnet-3.5: Next-generation\\nai model.\\nAkari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee,\\nEunsol Choi, and Hannaneh Hajishirzi. 2021. XOR\\nQA: Cross-lingual open-retrieval question answering.\\nIn Proceedings of the 2021 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\npages 547–564, Online. Association for Computa-\\ntional Linguistics.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2023. Self-RAG: Learning to\\nretrieve, generate, and critique through self-reflection.\\narXiv preprint arXiv:2310.11511.\\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun\\nLuo, Defu Lian, and Zheng Liu. 2024a. M3-\\nembedding: Multi-linguality, multi-functionality,\\nmulti-granularity text embeddings through self-\\nknowledge distillation. In Findings of the Associa-\\ntion for Computational Linguistics ACL 2024, pages\\n2318–2335, Bangkok, Thailand and virtual meeting.\\nAssociation for Computational Linguistics.\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\\n2024b. Benchmarking large language models in\\nretrieval-augmented generation. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence, pages\\n17754–17762.\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\\n2024c. Benchmarking large language models in\\nretrieval-augmented generation. In Proceedings of\\nthe AAAI Conference on Artificial Intelligence, vol-\\nume 38, pages 17754–17762.\\nNadezhda Chirkova, David Rau, Hervé Déjean, Thibault\\nFormal, Stéphane Clinchant, and Vassilina Nikoulina.\\n2024. Retrieval-augmented generation in multi-\\nlingual settings. In Proceedings of the 1st Work-\\nshop on Towards Knowledgeable Language Models\\n(KnowLLM 2024), pages 177–188, Bangkok, Thai-\\nland. Association for Computational Linguistics.\\nCohere. 2024. Introducing command-r+: Advanced\\nretrieval-augmented generation model.\\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\\nWilliams, Samuel Bowman, Holger Schwenk, and\\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\\nlingual sentence representations. In Proceedings of\\nthe 2018 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 2475–2485, Brus-\\nsels, Belgium. Association for Computational Lin-\\nguistics.\\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\\nZhangyin Feng, Haotian Wang, Qianglong Chen,\\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\\nLiu. 2024. A survey on hallucination in large lan-\\nguage models: Principles, taxonomy, challenges, and\\nopen questions. ACM Trans. Inf. Syst. Just Accepted.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\\nand William El Sayed. 2023. Mistral 7b. Preprint,\\narXiv:2310.06825.\\nEhsan Kamalloo, Nouha Dziri, Charles Clarke, and\\nDavood Rafiei. 2023. Evaluating open-domain ques-\\ntion answering in the era of large language models.\\nIn Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), pages 5591–5606, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nJungo Kasai, Keisuke Sakaguchi, yoichi takahashi,\\nRonan Le Bras, Akari Asai, Xinyan Velocity Yu,\\nDragomir Radev, Noah A. Smith, Yejin Choi, and\\nKentaro Inui. 2023. Realtime QA: What’s the an-\\nswer right now? In Thirty-seventh Conference on\\nNeural Information Processing Systems Datasets and\\nBenchmarks Track.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fa2a1433-f88a-4b91-8046-2bba5c5d372f', embedding=None, metadata={'page_label': '10', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks. In Advances in Neural Infor-\\nmation Processing Systems, volume 33, pages 9459–\\n9474. Curran Associates, Inc.\\nTian Liang, Xing Wang, Mingming Yang, Yujiu Yang,\\nShuming Shi, and Zhaopeng Tu. 2024. Addressing\\nentity translation problem via translation difficulty\\nand context diversity. In Findings of the Associa-\\ntion for Computational Linguistics ACL 2024, pages\\n11628–11638, Bangkok, Thailand and virtual meet-\\ning. Association for Computational Linguistics.\\nJiahua Liu, Yankai Lin, Zhiyuan Liu, and Maosong Sun.\\n2019. XQA: A cross-lingual open-domain question\\nanswering dataset. In Proceedings of the 57th An-\\nnual Meeting of the Association for Computational\\nLinguistics, pages 2358–2368, Florence, Italy. Asso-\\nciation for Computational Linguistics.\\nDominik Macko, Robert Moro, Adaku Uchendu, Ja-\\nson Lucas, Michiharu Yamashita, Matúš Pikuliak,\\nIvan Srba, Thai Le, Dongwon Lee, Jakub Simko, and\\nMaria Bielikova. 2023. MULTITuDE: Large-scale\\nmultilingual machine-generated text detection bench-\\nmark. In Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 9960–9987, Singapore. Association for Com-\\nputational Linguistics.\\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\\nKar, and Oleg Rokhlenko. 2022. SemEval-2022 task\\n11: Multilingual complex named entity recognition\\n(MultiCoNER). In Proceedings of the 16th Interna-\\ntional Workshop on Semantic Evaluation (SemEval-\\n2022), pages 1412–1437, Seattle, United States. As-\\nsociation for Computational Linguistics.\\nSebastian Nagel. 2016. News dataset available. Ac-\\ncessed: 4 October 2016.\\nOpenAI. 2024. Introducing gpt-4o.\\nArjun Panickssery, Samuel R. Bowman, and Shi Feng.\\n2024. LLM evaluators recognize and favor their own\\ngenerations. In The Thirty-eighth Annual Conference\\non Neural Information Processing Systems.\\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\\nChristopher D. Manning. 2020. Stanza: A python\\nnatural language processing toolkit for many human\\nlanguages. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics:\\nSystem Demonstrations, pages 101–108, Online. As-\\nsociation for Computational Linguistics.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing, pages 2383–2392, Austin,\\nTexas. Association for Computational Linguistics.\\nNirmal Roy, Leonardo F. R. Ribeiro, Rexhina Blloshmi,\\nand Kevin Small. 2024. Learning when to retrieve,\\nwhat to rewrite, and how to respond in conversa-\\ntional QA. In Findings of the Association for Com-\\nputational Linguistics: EMNLP 2024, pages 10604–\\n10625, Miami, Florida, USA. Association for Com-\\nputational Linguistics.\\nUma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua,\\nAaron Phillips, and Yinfei Yang. 2020. LAReQA:\\nLanguage-agnostic answer retrieval from a multilin-\\ngual pool. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing\\n(EMNLP), pages 5919–5930, Online. Association for\\nComputational Linguistics.\\nYixuan Tang and Yi Yang. 2024. MultiHop-RAG:\\nBenchmarking retrieval-augmented generation for\\nmulti-hop queries. In First Conference on Language\\nModeling.\\nNandan Thakur, Luiz Bonifacio, Crystina Zhang,\\nOdunayo Ogundepo, Ehsan Kamalloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Box-\\ning Chen, Mehdi Rezagholizadeh, and Jimmy Lin.\\n2024a. “knowing when you don‘t know”: A multilin-\\ngual relevance assessment dataset for robust retrieval-\\naugmented generation. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2024, pages\\n12508–12526, Miami, Florida, USA. Association for\\nComputational Linguistics.\\nNandan Thakur, Suleman Kazi, Ge Luo, Jimmy\\nLin, and Amin Ahmad. 2024b. Mirage-\\nbench: Automatic multilingual benchmark arena for\\nretrieval-augmented generation systems. Preprint,\\narXiv:2410.13716.\\nKristina Toutanova, Anna Rumshisky, Luke Zettle-\\nmoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven\\nBethard, Ryan Cotterell, Tanmoy Chakraborty, and\\nYichao Zhou, editors. 2021. Proceedings of the 2021\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics, Online.\\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry\\nWei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny\\nZhou, Quoc Le, and Thang Luong. 2024. Fresh-\\nLLMs: Refreshing large language models with search\\nengine augmentation. In Findings of the Associa-\\ntion for Computational Linguistics ACL 2024, pages\\n13697–13720, Bangkok, Thailand and virtual meet-\\ning. Association for Computational Linguistics.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Beiqi Zou,\\nZhixu Li, Jianfeng Qu, and Jie Zhou. 2023. Zero-\\nshot cross-lingual summarization via large language\\nmodels. In Proceedings of the 4th New Frontiers in\\nSummarization Workshop, pages 12–23, Singapore.\\nAssociation for Computational Linguistics.\\nJiaan Wang, Fandong Meng, Duo Zheng, Yunlong\\nLiang, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2022.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='33cb3816-b559-408e-8ef1-b7c1a96f77e7', embedding=None, metadata={'page_label': '11', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A survey on cross-lingual summarization. Transac-\\ntions of the Association for Computational Linguis-\\ntics, 10:1304–1323.\\nJianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin,\\nPeng Xu, Shengjun Li, Xiangyu Wang, Xiangzhou\\nGuo, Chengming Li, Xiaohai Xu, Kun Yu, Yux-\\ning Yuan, Yinghao Zou, Jiquan Long, Yudong Cai,\\nZhenxiang Li, Zhifeng Zhang, Yihua Mo, Jun Gu,\\nRuiyi Jiang, Yi Wei, and Charles Xie. 2021. Mil-\\nvus: A purpose-built vector data management sys-\\ntem. In Proceedings of the 2021 International Con-\\nference on Management of Data, SIGMOD ’21, page\\n2614–2627, New York, NY , USA. Association for\\nComputing Machinery.\\nShuting Wang, Jiongnan Liu, Shiren Song, Jiehan\\nCheng, Yuqi Fu, Peidong Guo, Kun Fang, Yutao Zhu,\\nand Zhicheng Dou. 2024. Domainrag: A chinese\\nbenchmark for evaluating domain-specific retrieval-\\naugmented generation. Preprint, arXiv:2406.05654.\\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel.\\n2018. Constructing datasets for multi-hop reading\\ncomprehension across documents. Transactions of\\nthe Association for Computational Linguistics, 6:287–\\n302.\\nJian Yang, Shaohan Huang, Shuming Ma, Yuwei Yin,\\nLi Dong, Dongdong Zhang, Hongcheng Guo, Zhou-\\njun Li, and Furu Wei. 2022. CROP: Zero-shot cross-\\nlingual named entity recognition with multilingual\\nlabeled sequence translation. In Findings of the Asso-\\nciation for Computational Linguistics: EMNLP 2022,\\npages 486–496, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nXiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla,\\nXiangsen Chen, Sajal Choudhary, Rongze Gui, Ziran\\nJiang, Ziyu JIANG, Lingkun Kong, Brian Moran, Ji-\\naqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang,\\nEting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nico-\\nlas SCHEFFER, Yue Liu, Nirav Shah, Rakesh Wanga,\\nAnuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024.\\nCRAG - comprehensive RAG benchmark. In The\\nThirty-eight Conference on Neural Information Pro-\\ncessing Systems Datasets and Benchmarks Track.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. 2018. HotpotQA: A dataset for\\ndiverse, explainable multi-hop question answering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n2369–2380, Brussels, Belgium. Association for Com-\\nputational Linguistics.\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\\nZhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\\nLLM-as-a-judge with MT-bench and chatbot arena.\\nIn Thirty-seventh Conference on Neural Information\\nProcessing Systems Datasets and Benchmarks Track.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3acd857c-ad22-4891-af39-ad0221542783', embedding=None, metadata={'page_label': '12', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='User Locale Native English Both\\nGermany 59.8% 56.3% 71.5%\\nJapan 61.1% 44.3% 68.3%\\nSpain 57.9% 48.4% 68.9%\\nTable 7: Percentage of satisfactory retrieval results us-\\ning Google search as retriever on real-world information\\nseeking LLM traffic. Search results are evaluated by\\nClaude 3.5 Sonnet if they contain sufficient information\\nfor a satisfactory answer to the user query. Native uses\\nGoogle search in the user locale, English performs En-\\nglish Google search using a translation of the user query,\\nand Both combines the two search results. LLM traffic\\nis obtained via SimilarWeb.\\nA Retrieval Quality Investigation\\nWe evaluate the relevance of English and native-\\nlanguage search results for real-world queries\\nsubmitted to large language models (LLMs) by\\nnon-English users in Germany, Spain, and Japan.\\nThe analysis is based on a proprietary dataset of\\nreal LLM traffic provided by SimilarWeb7. User\\nqueries are filtered by locale and language using\\nlangid to retain only those submitted in the respec-\\ntive native languages. For each query, we retrieve\\ntwo sets of search results: one from a U.S.-based\\nGoogle search using the English translation of the\\nquery, and another from a country-specific Google\\ndomain using the original native-language version.\\nRelevance is assessed using Claude 3.5 Sonnet,\\nwhich evaluates whether the retrieved results con-\\ntain sufficient information to generate a satisfac-\\ntory response, taking into account the user’s locale\\n(e.g., a tax-related query from Germany must in-\\nclude references to German tax regulations). Table\\n7 reports the percentage of queries for which the\\nEnglish-only, native-only, or both sets of results\\nindependently provided sufficient information. The\\nresults indicate that combining English and native-\\nlanguage search results significantly improves the\\nproportion of queries for which a comprehensive re-\\nsponse can be generated, compared to using either\\nlanguage alone.\\nB Data Construction\\nB.1 Data Source\\nWe use articles in News Crawl (Nagel, 2016) as\\nthe data source to create questions and prepare\\nsupporting and distracting articles. Specifically, we\\ndownload news articles between June 1, 2024 and\\nNovember 30, 2024 from NEWS Crawl using new-\\n7https://www.similarweb.com/\\nArt1 Art2 Art3 Art4\\n𝑒1 𝑒2 𝑒3 𝑒4\\nFigure 6: Example of a bipartite graph between articles\\nand entities.\\nplease8. This timeframe exceeds the knowledge\\ncutoff of widely used LLMs, such as GPT-4o and\\nClaude 3.5 sonnet. Therefore, questions created\\nfrom these articles are more likely to require LLMs\\nto use external knowledge to answer. We only keep\\narticles that contain more than 1200 tokens and are\\nin English, German, Spanish, Chinese, or Arabic,\\nobtaining around 1700k, 250k, 600k, 180k, and\\n460k news articles in these languages.\\nB.2 Identify Pairs of Related Articles\\nB.2.1 Identify English-English Article pairs\\nInspired by the concept of \"bridge entity\" in Yang\\net al. (2018), we use a bipartite graph between\\narticles and entities to find related English article\\npairs. Specifically, we randomly sample around\\n100k English articles from the data downloaded\\nfrom News Crawl, covering various topics such\\nas Politics, Sports, Economy, and Entertainment.\\nThen, we use stanza (Qi et al., 2020) to identify\\nentities in the titles of the sampled articles and\\nconstruct a bipartite graph between the articles and\\nentities (as shown in Figure 6). In this graph, nodes\\nare entities and articles, and an edge will be added\\nbetween an entity and an article if the entity is\\ncontained in the title of the article. Finally, we\\nperform the Depth-First Search on the graph to\\nfind pairs of articles sharing at least two entities in\\ntheir titles (e.g., articles 1 and 2 in Figure 6) and\\nhaving a publication time gap of no more than two\\nweeks. Here we use the title instead of the main\\ntext because entities in the title are often the key\\nentities of the news, and two articles containing the\\nsame key entities are more likely to be related.\\nB.2.2 Identify X-English Article Pairs\\nTo construct cross-document questions for the mul-\\ntilingual retrieval setting in XRAG, we need to find\\nrelated articles across languages. We empirically\\nfind that the article-entity graph performs poorly\\nhere, due to (1) inaccurate entity recognition in\\nnon-English texts (Malmasi et al., 2022), and (2)\\n8https://github.com/fhamborg/news-please', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='21270f52-afc4-4173-aba8-de4d9d0a254f', embedding=None, metadata={'page_label': '13', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Event Language Topic\\n2024 presidential election 2024\\nHalbfinale der UEFA EURO 2024\\n(en: semi-finals of UEFA EURO 2024)\\nInundaciones repentinas en Valencia\\n(en: Flash floods in Valencia)\\n秘鲁钱凯港开港\\n(en: Opening of the Port of Chancay, Peru)\\n(en: Ceasefire in the Gaza Strip)\\nوقف إطلاق النار في قطاع غزة\\nPolitics\\nSports\\nDisaster\\nEconomy\\nPolitics\\nEnglish\\nGerman\\nSpanish\\nChinese\\nArabic\\nFigure 7: Examples of events collected from Wiki 2024\\nbetween June and November, which will be used to\\nretrieve related articles across languages.\\nOlympics 2024\\nMultilingual\\nDense Retriever\\nArt1\\nen\\nArt2\\nen\\nMultilingual\\nDense Retriever\\nArt1\\nde\\nArt2\\nde\\nArt1\\nen Art1\\nde\\nArt1\\nen Art2\\nde\\nArt2\\nen Art1\\nde\\nArt2\\nen Art2\\nde\\nFigure 8: Example of using the event \"Olympics 2024\"\\nand a multilingual retrieve to find related articles in\\nEnglish and German.\\nchallenges in cross-lingual entity mapping (Liang\\net al., 2024).\\nTo address this, we collect 117 international\\nevents between June and November 2024 from dif-\\nferent language versions of Wiki 20249, covering\\ntopics such as politics, sports, astronomy, and natu-\\nral disasters (see some examples in Figure 7). We\\nalso build a multilingual dense retriever10 based on\\nthe multilingual model BGE-M3 (as text encoder,\\nChen et al., 2024a) and Milvus (as vector database,\\nWang et al., 2021). The retriever operates on news\\narticles downloaded from NEWS Crawl. Then, we\\nuse the retriever to search across languages for arti-\\ncles related to the events. Finally, we group articles\\nin different languages about the same event to form\\nrelated article pairs. Figure 8 show an example,\\nwhere we use the event \"Olympics 2024\" to search\\nfor English and German articles and create related\\nEnglish-German article pairs.\\nIn both cases, we further use GPT-4o to verify\\nthe topical relevance of the found article pairs, pass-\\n9https://en.wikipedia.org/wiki/2024\\nhttps://de.wikipedia.org/wiki/2024\\nhttps://es.wikipedia.org/wiki/2024\\nhttps://zh.wikipedia.org/zh-cn/2024\\nhttps://ar.wikipedia.org/wiki/2024\\n10The dense retriever is also used in selecting distracting\\narticles for each question, see Section 4.4.\\nTopic relevance verification\\nYou are an AI assistant tasked with verifying whether the given two texts are \\ntopically related. Considering the following steps to generate your response:\\n1. Read the two texts carefully and get the “high level” topics.\\n2. Compare whether the two texts are about related entities.\\n3. Compare whether the high level topics of the two texts are related (e.g., \\nboth describe stock price, sport, film, a country, a party, or the same event).\\n4. Do not pay much attention to details but high level topics!\\n5. Create your response.\\nHere are the two given texts:\\n<text1> {{ text1 }} </text1>\\n<text2> {{ text2 }} </text2>\\nIf they are topically related, format your response as: <answer>yes</answer>\\notherwise, output: <answer>no</answer>\\nFigure 9: Prompt for topic relevance verification.\\nSummary generation\\nYou are an AI assistant tasked with generating a summary for a given article. \\nThe generated summary should:\\n1. Have the same language as the article\\n2. Be ACCURATE, clear, specific, and concise\\n3. Cover the key information of the given article, such as names, places, time \\nstrings, events, results, and ect\\n4. Be abstract and have little lexicon overlap with the article\\n5. Not use pronouns or partial names to refer something in your summary. \\nUse its actual name or full name (e.g., “Joe Biden” instead of “Biden”, \\n“Olympics 2024” instead of “Olympics”)\\n6. Not exceed 180 words\\nHere is the given article: <article> {{ article }} <\\\\article>\\nFormat your response as: <summary>[your generated summary]</summary>\\nFigure 10: Prompt for summary generation, as described\\nin Step 1 of Section 4.2.\\ning only those confirmed to be truly related to the\\nnext step. Figure 9 shows the prompt we used to\\ninstruct GPT-4o for relevancy verification. Fur-\\nthermore, if computational resources were not a\\nconstraint, a more generalized approach to identi-\\nfying pairs of related articles would involve using\\nrandomly selected documents as queries and re-\\ntrieving related pairs through a multilingual dense\\nretriever. This method would be applicable across\\na wide range of domains and languages.\\nB.3 English cross-document Q&A Generation\\nFigure 10 shows the prompt used to generate a\\nsummary from an article, as described in Step 1 of\\nSection 4.2. Figure 11 shows the prompt for gener-\\nating a set of simple Q&A pairs from a summary,\\nas described in Step 2 of Section 4.2. Table 8 shows\\nthe definition of four types of cross-document ques-\\ntions: aggregation, comparison, multi-hop, and set\\nquestions, as mentioned in Step 3 of Section 4.2.\\nWe show the prompts used for generating these\\nfour types of questions in Figures 12, 13, 14, and\\n15. The prompt used for generating an answers to\\nthe created cross-document question is presented\\nin Figure 16.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='60926aa5-3927-429b-8fdc-4658277ceb7d', embedding=None, metadata={'page_label': '14', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Quesiton typeDefinition\\nAggregationQuestions that require the aggregation of information across articles to answer (e.g., “how many Oscar awards\\ndid Meryl Streep win?”)\\nComparisonQuestions that compare information in two articles (e.g., “who started performing earlier, Adele or Ed Sheeran?”)\\nMulti-hopQuestions that require chaining multiple pieces of information from two articles to compose the answer (e.g.,\\n“who acted in Ang Lee’s latest movie?”)\\nSet Questions that expect a set of entities, objects, or events from two articles as the answer (e.g., “what are the continents\\nin the southern hemisphere?”)\\nTable 8: Definition of four types of cross-document questions: aggregation, comparison, multi-hop, and set questions,\\nas mentioned in Step 3 of Section 4.2.\\nSimple QA generation\\nYou are an AI assistant tasked with generating ENGLISH question-answer \\npairs based on facts in a given text.\\nThe generated questions should:\\n1. Be clear and unambiguous\\n2. Not use pronouns or partial names to refer something in your question. \\nUse its actual name or full name (e.g., \"Joe Biden\" instead of \"Biden\", \\n\"Olympics 2024\" instead of \"Olympics\")\\n3. Be in English\\n4. Not exceed 20 words\\nThe corresponding answers should:\\n1. Be accurate and supported by facts in the given text\\n2. Be concise and NOT exceed 12 words\\n3. Not use pronouns or partial names to refer something in your answer. Use \\nits actual name or full name\\n4. Be in English\\nThe number of question-answer pairs can range from 1 to 6, depending on \\nthe amount of information (facts) in the given text.\\nHere is the given text: <text> {{ text }} </text>\\nFormat your response as:\\n<list>\\n<question>[your first question]</question>\\n<answer>[answer to your first question]</answer>\\n<question>[your second question]</question>\\n<answer>[answer to your second question]</answer>\\n...\\n</list>\\nExample of generated questions:\\nwhich movie won the oscar best visual effects in 2021?\\nwhat\\'s the name of nashville\\'s hockey team?\\nwho was the coach for the seattle seahawks?\\n…more examples\\nFigure 11: Prompt for simple Q&A generation, as de-\\nscribed in Step 2 of Section 4.2.\\nB.4 Human Verification\\nDue to the existence of LLM hallucinations (Huang\\net al., 2024), the Q&A pairs generated by the LLM\\nmay contain factual errors. Therefore, we ask a\\nprofessional annotation team to verify the quality\\nof the generated Q&A pairs. Figure 17 presents the\\nguidelines we prepared for the annotation team.\\nWe generate 2,950 raw cross-document Q&A\\npairs from English article pairs. Following manual\\nverification, approximately 90% of the questions\\nare deemed natural, 72% are considered answer-\\nable, and 54% of the answers (including some that\\nwere manually corrected) are judged correct, yield-\\nAggregation question generation\\nYou are an AI assistant tasked with generating questions based on two sets of \\nsub-question-answer pairs. Your goal is to create aggregation questions that \\nrequire performing addition, subtraction, union, or intersection operations on \\nthe information from multiple sub-questions and their results to answer.\\nHere are the two sets of sub-question-answer pairs:\\n<sub_question_answer_pairs_1>\\n{% for (idx, question, answer) in group1 %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs_1>\\n<sub_question_answer_pairs_2>\\n{% for (idx, question, answer) in group2 %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs_2>\\nTo synthesize your response, follow these steps:\\n1. Look for common themes or connections between two sets of sub-\\nquestion-answer pairs\\n2. Select AT LEAST ONE sub-question-answer pair from each set\\n3. Based on high-level topics, generate aggregation questions that require \\naggregating multi-pieces of information from selected sub-question-answer \\npairs to obtain answers from the selected pairs\\n4. Avoid creating questions by simply connecting two sub-questions with a \\nconjunction \\n5. Not use pronouns or partial names to refer something in your question. \\nUse its actual name or full name\\n6. The questions should NOT have more than 25 words and must be succinct. \\nDon\\'t make the generated questions too long!\\nGenerate no more than two questions, and present your results as:\\n<output>\\n<question>[Your first question]</question>\\n...\\n</output>\\nExample of generated elaborate questions:\\nhow many Oscar awards did Meryl Streep win?\\n…More examples\\nFigure 12: Prompt for aggregation question generation,\\nas described in Step 3 of Section 4.2.\\ning a final set of approximately 1,500 verified Q&A\\npairs. To further assess quality, we sample 200\\nQ&A pairs and task a separate group of annotators\\nto answer the questions by carefully reading the\\narticle pairs used to generate these questions. An-\\nnotators are explicitly instructed to answer no more\\nthan one question per hour (to ensure thorough', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f6d7f043-043f-4fbd-8988-cd15ff9b10b7', embedding=None, metadata={'page_label': '15', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Comparison question generation\\nYou are an AI assistant tasked with generating questions based on two sets of \\nsub-question-answer pairs. Your goal is to create comparison questions that \\nwill require comparing numbers, time strings, facts, or other information in \\nmultiple sub-questions and their results to answer.\\nHere are the two sets of sub-question-answer pairs:\\n<sub_question_answer_pairs_1>\\n{% for (idx, question, answer) in group1 %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs_1>\\n<sub_question_answer_pairs_2>\\n{% for (idx, question, answer) in group2 %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs_2>\\nTo synthesize your response, follow these steps:\\n1. Look for common themes or connections between two sets of sub-\\nquestion-answer pairs\\n2. Select AT LEAST ONE sub-question-answer pair from each set\\n3. Based on high-level topics, generate comparison questions that require \\ncomparing multi-pieces of information from selected sub-question-answer \\npairs to obtain answers\\n4. Avoid creating questions that can be answered by simply combining or \\nconcating answers of sub-questions\\n5. Not use pronouns or partial names to refer something in your question. \\nUse its actual name or full name\\n6. The questions should NOT have more than 25 words and must be succinct. \\nDon't make the generated questions too long!\\nGenerate no more than two questions, and present your results as:\\n<output>\\n<question>[Your first question]</question>\\n...\\n</output>\\nExample of generated elaborate questions:\\nwho has more spotify plays, drake or taylor swift?\\n... More examples\\nFigure 13: Prompt for comparison question generation,\\nas described in Step 3 of Section 4.2.\\nreading and accurate responses). If their answers\\nalign with the reference answers, it suggests high-\\nquality Q&A pairs. We use the majority vote of\\nthree LLM-as-a-Judge (see Evaluation Metrics in\\nSection 5.1) to calculate the accuracy of human re-\\nsponses to 200 questions, resulting in 85%. Upon\\nmanual review of the 30 failed cases, we find: (i) 4\\nhuman answers are correct but incorrectly judged\\nby the LLMs; (ii) 10 are genuinely incorrect, with\\nthe reference answers being valid; and (iii) 16 are\\ndifficult to assess due to ambiguity or poor ques-\\ntion quality, and are thus categorized as low-quality\\nexamples. This performance is comparable to es-\\ntablished QA benchmarks—for instance, human\\naccuracy on SQuAD (Rajpurkar et al., 2016) is\\n86.8%. We then randomly sample 1000 examples\\nfrom the 1500 Q&A pairs and send them for human\\ntranslation.\\nMultihop question generation\\nYou are an AI assistant tasked with generating questions based on two sets of sub-\\nquestion-answer pairs. Your goal is to create multi-hop questions that will require \\nchaining multiple pieces of information from multiple sub-questions and their \\nresults to answer.\\nHere are the two sets of sub-question-answer pairs:\\n<sub_question_answer_pairs_1>\\n{% for (idx, question, answer) in group1 %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs_1>\\n<sub_question_answer_pairs_2>\\n{% for (idx, question, answer) in group2 %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs_2>\\nTo synthesize your response, follow these steps:\\n1. Look for common themes or connections between two sets of sub-question-\\nanswer pairs\\n2. Select A T LEAST ONE sub-question-answer pair from each set\\n3. Based on high-level topics, generate multi-hop questions which need reasoning \\nover multi-pieces of information from selected sub-question-answer to obtain \\nanswers\\n4. Avoid creating questions that can be answered by simply combining or concating \\nanswers of sub-questions.\\n5. Not use pronouns or partial names to refer something in your question. Use its \\nactual name or full name.\\n6. The questions should NOT have more than 25 words and must be succinct. Don't \\nmake the generated questions too long!\\nGenerate no more than two questions, and present your results as:\\n<output>\\n<question>[Your first question]</question>\\n...\\n</output>\\nExample of generated elaborate questions:\\nwhere did the ceo of salesforce previously work?\\n... More examples\\nFigure 14: Prompt for multi-hop question generation,\\nas described in Step 3 of Section 4.2.\\nWe generate approximately 1,000 Q&A pairs\\neach from article pairs of the following language\\npairs: English–German, English–Spanish, En-\\nglish–Chinese, and English–Arabic. After man-\\nual verification, about 90% of the questions from\\nthe English–German and English–Spanish sets\\nare deemed natural, while the proportion for\\nEnglish–Chinese and English–Arabic is slightly\\nlower, at approximately 84%. Finally, we obtain\\n487, 680, 332, and 420 high-quality Q&A pairs\\nfrom the English–German, English–Spanish, En-\\nglish–Chinese, and English–Arabic article pairs,\\nrespectively. From each set, 300 Q&A pairs are\\nrandomly sampled and submitted for translation.\\nB.5 Human Translation\\nQ&A pairs generated from English-English article\\npairs are translated into German, Spanish, Chinese,\\nand Arabic to simulate a cross-lingual retrieval-\\naugmented generation (RAG) with monolingual\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='85274343-eaea-414e-9153-142ef775a3d8', embedding=None, metadata={'page_label': '16', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Set question generation\\nYou are an AI assistant tasked with generating questions based on two sets of sub-\\nquestion-answer pairs. Your goal is to create \"set\" questions that expect a set of entities \\nor objects of the same category as answers.\\nHere are the two sets of sub-question-answer pairs:\\n<sub_question_answer_pairs_1>\\n{% for (idx, question, answer) in group1 %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs_1>\\n<sub_question_answer_pairs_2>\\n{% for (idx, question, answer) in group2 %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs_2>\\nTo synthesize your response, follow these steps:\\n1. Look for common themes or connections between two sets of sub-question-answer \\npairs\\n2. Select AT LEAST ONE sub-question-answer pair from each set\\n3. Based on high-level topics, generate \"set\" questions which need entities or objects of \\nsame category from selected sub-question-answer pairs to obtain answers\\n4. Avoid creating questions by simply connecting two sub-questions with a conjunction\\n5. Not use pronouns or partial names to refer something in your question. Use its actual \\nname or full name\\n6. The questions should NOT have more than 25 words and must be succinct. Don\\'t \\nmake the generated questions too long!\\nGenerate no more than two questions, and present your results as:\\n<output>\\n<question>[Your first question]</question>\\n...\\n</output>\\nExample of generated elaborate questions:\\nwhat are ben stiller 3 most recent movies?\\n... More examples\\nFigure 15: Prompt for set question generation, as de-\\nscribed in Step 3 of Section 4.2.\\nretrieval. Given the importance of named enti-\\nties in Q&A, translators are instructed to consult\\nWikipedia or other reliable sources to find com-\\nmonly used translations in the target language. If\\nno appropriate translation exists, the original En-\\nglish term is retained. For example, \"Microsoft\\nupdated the Copilot\" is translated into Chinese as\\n\"微软更新了Copilot,\" where \"Microsoft\" is trans-\\nlated (微软) and \"Copilot\" remains in English due\\nto the absence of a standard Chinese equivalent.\\nFor Q&A pairs generated from article pairs in\\ndifferent languages, such as English-German and\\nEnglish-Chinese, translation is performed only into\\nthe language of the non-English input article. For\\nexample, Q&A pairs from English–German article\\npairs are translated into German, and so on for\\nothers. These examples are used to simulate a cross-\\nlingual RAG with multilingual retrieval.\\nC Experimental Settings\\nC.1 Models\\nWe benchmark five models on XRAG, including\\nAnswer generation for cross-document question\\nYou are an AI assistant tasked with generating an ENGLISH answer for a \\ngiven question. Your goal is to create an ENGLISH answer based solely on \\nthe given sub-question-answer pairs, two reference texts, and their \\npublication dates.\\nThe given question is: <question> {{ question }} </question>\\nHere are the given sub-question-answer pairs:\\n<sub_question_answer_pairs>\\n{% for (idx, question, answer) in group %}\\n<qa_{{ idx }}>\\nquestion: {{ question }}\\nanswer: {{ answer }}\\n</qa_{{ idx }}>\\n{% endfor %}\\n</sub_question_answer_pairs>\\nHere are the two reference texts and their publication dates:\\n<text_1> {{ text1 }} </text_1>\\n<date_1> {{ date1 }} </date_1>\\n<text_2> {{ text2 }} </text_2>\\n<date_2> {{ date2 }} </date_2>\\n1. Look for connections between the given questions and sub-questions\\n2. Read answers of sub-questions and the two reference texts\\n3. Generate an ENGLISH answer based solely on those information. DO \\nNOT generate answers use information OUTSIDE of sub-question-answer \\npairs and two reference texts, such as your own knowledge\\n4. Format your response as: <answer>[your answer]</answer>\\nFigure 16: Prompt for creating an answer for a gener-\\nated cross-document question, as described in Step 3 of\\nSection 4.2.\\nGPT-4o-2024-08-06, Claude 3.5 Sonnet (2024-06-\\n20), Mistral-Large-Instruct-2407, Command-r+,\\nand Nova-pro. Figure 18 shows the template we\\nuse to prompt LLMs to respond to a given answer\\nby reading the retrieved articles. Figure 19 shows\\nthe template used to prompt LLMs to answer ques-\\ntions using their own parametric knowledge.\\nC.2 Evaluation Metrics\\nWe use LLM-as-a-Judge to determine whether an\\nLLM’s response is correct. Specifically, each time\\nwe input a question, a golden answer, and an an-\\nswer generated by a model to an LLM and ask the\\nLLM to determine whether the generated answer is\\ncorrect or incorrect following the guideline we pro-\\nvided (see prompt in Figure 20). To avoid the self-\\npreference problem (Panickssery et al., 2024), we\\nuse three LLM judges, including GPT-4o-2024-08-\\n06, Claude Sonnet-3.5 (2024-06-20), and Mistral-\\nLarge-Instruct-2407, and take the majority vote as\\nthe final result. In the prompt, we explicitly instruct\\nLLM judges to consider the language of models’\\nresponses, but they sometimes fail to do so. To\\naddress this, we apply a language detection tool,\\nlingua11, to verify whether the response is in the\\n11https://github.com/pemistahl/lingua', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a15b42d3-1cf2-4a6b-9659-0da7071c23cd', embedding=None, metadata={'page_label': '17', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='You are shown a question, an answer and two news articles. The question is generated by a Large Language Model, your task is to \\nverify the quality of the question, answer and articles. Please also refer to example annotations provided in the attachment file.\\n1. Read the question and make sure it is intelligible and natural. Don’t take more than ten seconds to decide. If it is not clear what the \\nquestion is asking for, mark it as not intelligible. Unnatural questions are questions that sound forced and unlikely to be asked by a \\nhuman, often linking unrelated facts from the two articles. Decide based on your gut feeling. Some examples of unnatural and hard \\nto understand questions:\\na. Unclear what the question is asking:\\ni. How many regions are targeted by Israel\\'s aggressive stance and confirmed Ismail Haniyeh\\'s death?\\nb. Simple Linking of unrelated facts:\\ni. How many Spanish cities will install giant screens for the Euro 2024 final and which teams reached the semifinals?\\nii. Which country is a leading uranium producer, and who are the main contenders in the 2024 US Presidential Election?\\nc. Forced Combination of unrelated facts:\\ni. How many regions are targeted by Israel\\'s aggressive stance and confirmed Ismail Haniyeh\\'s death?\\nd. If you determine that the question is unnatural or unintelligible, mark this and stop here.\\n2. Determine if the question is answerable based on the information in the articles alone:\\na. Search within the two provided articles for the entities in the question to speed up the process.\\nb. You can look up background knowledge in an encyclopaedia to get context if you are not familiar with the subject, but the \\nanswer to the question has to be in the articles.\\nc. If you determine that the question is not answerable based on the information in the articles, mark this and stop here.\\n3. Determine if the provided answer is correct.\\na. If the answer is incorrect, please provide the correct answer based on the information in the articles\\n4. For each of the two articles, mark what information they contain:\\na. Sufficient: The information in the article is sufficient to answer the question.\\nb. Some: The article contains necessary information to answer the question but is not sufficient.\\nc. None: The article does not contain relevant information to answer the question.\\nGuideline for QA verification\\nFigure 17: Guidelines for verifying the quality of generated cross-document Q&A pairs, as described in Section 4.3.\\nWe also provide additional examples to guide the annotation.\\nAnswer question with retrieved articles\\nYou are an AI assistant tasked with answering a given question. Your goal is \\nto generate an answer for a given question based on the provided articles and \\ntheir publication dates. The answer should:\\n1. Fully answer the question\\n2. Be brief and concise\\n3. Use the SAME LANGUAGE as the given question\\n4. Be supported by the articles.\\nHere are the given question and articles: \\n<question> {{ question }} </question>\\n<articles>\\n{% for (idx, text, date) in articles %}\\n<text_{{ idx }}> {{ text }} </text_{{ idx }}>\\n<date_{{ idx }}> {{ date }} </date_{{ idx }}>\\n{% endfor %}\\n</articles>\\nNote, you should generate the answer based solely on the information of \\narticles. DO NOT use information outside of the given articles, such as your \\nown knowledge.\\nFormat your response as follows: <answer>[Your generated answer. Use one \\nor two sentences at most. Keep the answer as concise as possible.]</answer>\\nFigure 18: Prompt used to instruct LLMs in using arti-\\ncles to answer questions, as described in Section 5.1.\\nsame language as the corresponding input question,\\nand if not, we consider it incorrect. Finally, we\\nreport each model’s accuracy by the LLM judge\\npanel, which includes the assessment of language\\nAnswer question using parametric knowledge\\nYou are an AI assistant tasked with answering questions. Your goal is to \\ngenerate an answer for a given question using your own parametric \\nknowledge.\\nHere is the given question: <question> {{ question }} </question>\\nIf you are able to answer the question, format your response as: \\n<answer>[your answer]</answer>\\nOtherwise, output: <answer>None</answer>\\nFigure 19: Prompt for answering question using para-\\nmetric knowledge of LLMs. This corresponds to the\\n\"No Retrieval\" setting in Section 5.2 and Table 3.\\ncorrectness.\\nTo assess the reliability of the LLM judge panel,\\nwe compare its evaluations with those provided\\nby human annotators. Specifically, we collect\\nresponses from five different LLMs to 300 En-\\nglish questions in the monolingual retrieval setting,\\nyielding a total of 1,500 responses. The LLM judge\\npanel is then used to evaluate the correctness of\\neach response against the gold answer. In parallel,\\nwe recruit three annotators via Amazon Mechani-\\ncal Turk12 to independently assess the same set of\\n12Given the straightforward nature of the evaluation task,\\nwe opted to use Mechanical Turk instead of a professional\\nannotation team.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='30dbac70-3516-4d69-8604-2dd63f763bf2', embedding=None, metadata={'page_label': '18', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LLM-as-a-Judge\\nYou are an AI assistant tasked with result evaluation. Given a question, a \\ngolden answer, and a generated answer, your goal is to judge whether the \\ngenerated answer is correct according to the question and golden answer.\\nHere is the question:\\n<question> {{ question }} </question>\\nHere is the golden answer:\\n<golden_answer> {{ answer }} </golden_answer>\\nHere is the generated answer:\\n<generated_answer> {{ pred }} </generated_answer>\\nTo synthesize your response, follow these steps:\\n1. Analyze the question type and conditions in the question\\n2. Identify the key information in the golden answer that can completely \\nsolve the question (e.g., names for \"Who\" questions and numbers for \"how \\nmany\" questions)\\n3. Check whether the key information exists in the generated answer\\n4. Ignore differences in punctuation and phrasing between the golden answer \\nand the generated answer. It is OK if the generated answer contains more \\ninformation than the golden answer, as long as it does not contain any \\nconflicting statements.\\n5. If the generated answer is in a different language than the golden answer, \\nit should be considered as wrong.\\nYour response should be in json format as follows:\\n{\\n    \"justification\": [Explain why you think the GENERA TED ANSWER is \\nCorrect or Incorrect. Use one or two sentences at most. Keep the \\nexplaination as concise as possible.],\\n    \"answer\": [correct or incorrect]\\n}\\nFigure 20: Prompt used for LLM-as-a-Judge, as de-\\nscribed in Section 5.1.\\nGerman Question\\nDoc 2Doc 1\\nDoc 8Doc 7Doc 6Doc 5Doc 4Doc 3\\nsupporting distracting\\nEnglish Question\\nEnglish\\nHuman Translate\\nLLM\\n- No doc\\nLLM- 2 En supporting docs\\nEnglish Answer English Answer\\n“Oracle Retrieval”in Table 3 “No Retrieval”in Table 3\\nFigure 21: Experimental settings in Table 3.\\n1,500 responses. These annotators follow the same\\nevaluation guidelines as those used by the LLM-as-\\na-Judge (see Figure 20). The majority vote among\\nthe three annotators is taken as the final human\\njudgment. To quantify the level of agreement be-\\ntween the LLM judge panel and the human eval-\\nuators, we compute Cohen’s kappa, which yields\\na score of 0.71—indicating substantial agreement\\nbetween the two evaluation approaches.\\nC.3 Settings in Different Tables\\nTo facilitate the interpretation of the results pre-\\nsented in different tables, we provide diagrams il-\\nlustrating the corresponding experimental setups.\\nSpecifically, Figures 21, 22, 23, and 24 depict the\\nexperimental configurations associated with Tables\\nClaude 3.5En+De En+Es En+Zh En+ArAvg.\\nXRAG-MultiR45.67 42.67 48.00 39.6744.00+EQ 49.00 40.67 45.33 42.6744.42+EQ, ES 46.67 44.00 46.33 47.6746.17MonoRAG51.00 46.33 46.67 47.3347.83\\nTable 9: Controlled analysis of Claude Sonnet 3.5 on\\nthe multilingual retrieval setting of XRAG, replacing\\nquestions (EQ), supporting articles (ES), and distract-\\ning articles (ED) with their English counterparts from\\nthe English monolingual RAG settings (see Figure 24).\\n\"XRAG-MultiR\" is the multilingual retrieval setting,\\nand \"MonoRAG\" (+EQ, +ES, +ED) is the English mono-\\nlingual RAG baseline setting.\\nMistral-largeEn+De En+Es En+Zh En+ArAvg.\\nXRAG-MultiR42.00 39.33 37.33 32.0037.67+EQ 42.67 34.67 40.67 38.0039.00+EQ, ES 43.33 40.00 47.33 45.3344.00MonoRAG45.67 43.00 48.67 43.6745.25\\nTable 10: Controlled analysis of Mistral-large on the\\nmultilingual retrieval setting of XRAG, replacing ques-\\ntions (EQ), supporting articles (ES), and distracting ar-\\nticles (ED) with their English counterparts from the\\nEnglish monolingual RAG settings (see Figure 24).\\nCommand-R+En+De En+Es En+Zh En+ArAvg.\\nXRAG-MultiR40.00 40.33 36.33 32.0037.17+EQ 41.00 34.00 40.67 40.3339.00+EQ, ES 43.33 36.00 50.33 43.3343.25MonoRAG43.67 42.33 49.67 43.3344.75\\nTable 11: Controlled analysis of Command-R+ on the\\nmultilingual retrieval setting of XRAG, replacing ques-\\ntions (EQ), supporting articles (ES), and distracting ar-\\nticles (ED) with their English counterparts from the\\nEnglish monolingual RAG settings (see Figure 24).\\n3, 4, 5, and 6, respectively.\\nD More Experimental Results\\nD.1 Controlled Analysis\\nIn Section 5.4, we perform a controlled analysis\\non the language of query, distracting articles, and\\nsupporting articles in the cross-lingual RAG with\\nmultilingual retrieval, using GPT-4o as the pri-\\nmary model. Here, we extend the analysis to ad-\\nditional LLMs: results for Claude 3.5 Sonnet are\\nshown in Table 9, Mistral-large in Table 10, and\\nCommand-R+ in Table 11. Similar results are ob-\\nserved across these LLMs: translating the support-\\ning articles from non-English to English leads to\\nthe most improvement (i.e., +ES). This suggests\\nthat the primary challenge does not appear to lie in\\nnon-English text generation but rather in reasoning\\nover retrieved information across languages.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2da70898-371d-4385-afc9-a7f7a7dc2731', embedding=None, metadata={'page_label': '19', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='German Question\\nDoc 2Doc 1\\nDoc 8Doc 7Doc 6Doc 5Doc 4Doc 3\\nsupporting distracting\\nEnglish Question\\nEnglish\\nHuman Translate\\nLLM\\n- 2 En supporting docs\\n- 6 En distracting docs\\nGerman Answer\\nLLM\\nEnglish Answer\\n- 2 En supporting docs\\n- 6 En distracting docs\\nDoc. Lang.=En, Query. Lang.=De in Table 4 Doc. Lang.=En, Query. Lang.=En in Table 4\\nFigure 22: Experimental settings in Table 4. Here, we use English (En) and German (De) as examples.\\nGerman Question\\nDoc 2 Doc 1\\nDoc 8Doc 7Doc 6 Doc 5Doc 4Doc 3\\nsupporting\\ndistracting\\nsupporting\\ndistracting\\nEnglish German\\nHuman Translate English Question\\nGoogle Translate\\nDoc 1\\nDoc 5Doc 4Doc 3\\nsupporting\\ndistracting\\nEnglish\\n(via Translation)\\nLLM\\nGerman Answer\\nLLM\\nEnglish Answer\\nDoc. Lang.=En+De, Query. Lang.=De in Table 5 Doc. Lang.=En+EnDe, Query. Lang.=En in Table 5\\n- 2 En supporting doc\\n- 6 En distracting doc\\n- 1 De supporting doc\\n- 1 En supporting doc\\n- 3 De distracting doc\\n- 3 En distracting doc\\nDoc 2Doc 1 Doc 8Doc 7Doc 6Doc 5Doc 4Doc 3\\nsupporting distracting\\nEnglish + German\\nDoc 2Doc 1\\nDoc 8Doc 7Doc 6Doc 5Doc 4Doc 3\\nsupporting distracting\\nEnglish + English\\nFigure 23: Experimental settings in Table 5. Here, we use English + German (En+De) as an example.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cc1c549d-f477-4080-943e-a456a30a4e75', embedding=None, metadata={'page_label': '20', 'file_name': 'XRAG-paper.pdf', 'file_path': 'c:\\\\Users\\\\fredr\\\\OneDrive\\\\Desktop\\\\llm-utilities\\\\llama--index\\\\data\\\\XRAG-paper.pdf', 'file_type': 'application/pdf', 'file_size': 1707029, 'creation_date': '2025-10-22', 'last_modified_date': '2025-10-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='German Question\\nDoc 2 Doc 1\\nDoc 8Doc 7Doc 6 Doc 5Doc 4Doc 3\\nsupporting\\ndistracting\\nsupporting\\ndistracting\\nEnglish Question\\nDoc 2 Doc 1\\nDoc 8Doc 7Doc 6 Doc 5Doc 4Doc 3\\nsupporting\\ndistracting\\nsupporting\\ndistracting\\nEnglish Question\\nDoc 2 Doc 1\\nDoc 8Doc 7Doc 6 Doc 5Doc 4Doc 3\\nsupporting\\ndistracting\\nsupporting\\ndistracting\\nEnglish Question\\nDoc 2 Doc 1\\nDoc 8Doc 7Doc 6 Doc 5Doc 4Doc 3\\nsupporting\\ndistracting\\nsupporting\\ndistracting\\nLLM\\nGerman Answer\\nLLM\\nEnglish Answer\\n- De question\\n- 1 De supporting doc \\n- 1 En supporting doc\\n- 3 De distracting docs \\n- 3 En distracting docs \\n- En question\\n- 1 De supporting doc \\n- 1 En supporting doc\\n- 3 De distracting docs \\n- 3 En distracting docs \\nLLM\\nEnglish Answer\\n- En question\\n- 2 En supporting doc \\n- 3 De distracting docs \\n- 3 En distracting docs \\nLLM\\nEnglish Answer\\n- En question\\n- 2 En supporting doc\\n- 6 En distracting docs \\n“XRAG-MultiR”in Table 6 “+EQ”in Table 6 “+EQ, ES”in Table 6 “MonoRAG”in Table 6\\n+ EQ\\n+ EQ, ES\\n+ EQ, ES, ED\\nEQ: use the English question before human translation, see English Q & A in Figure 2.\\nES: use Google Translation to translate the non-English supporting article to English\\nED: use Google Translation to translate the non-English distracting article to English\\n: denotes English\\n: denotes non-English, e.g., German\\nFigure 24: Experimental settings in Table 6. Here, we use English + German (En+De) as an example.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "llm = Groq(api_key=os.environ[\"GROQ_API_KEY\"], model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data(show_progress=True)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd8fe7",
   "metadata": {},
   "source": [
    "### Index Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a4c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 89/89 [00:00<00:00, 292.99it/s]\n",
      "Generating embeddings: 100%|██████████| 147/147 [00:13<00:00, 10.66it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, show_progress=True)\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "826d1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "postprocessor = [SimilarityPostprocessor(similarity_cutoff=0.3)]\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=postprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1f63e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 15:40:42,757 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: “Attention Is All You Need” introduces the Transformer, a neural‑network architecture that replaces recurrence and\n",
      "convolution with pure attention mechanisms. The model is built from stacked encoder and decoder blocks, each consisting of a multi‑head\n",
      "self‑attention layer followed by a position‑wise fully‑connected feed‑forward network. Residual connections and layer‑normalization are\n",
      "applied around every sub‑layer.  The core attention operation is **scaled dot‑product attention**: queries, keys, and values are projected\n",
      "into vectors, the dot products of queries with all keys are computed, scaled by the inverse square root of the key dimension, and passed\n",
      "through a softmax to obtain weights that are applied to the values. Multiple such attention heads run in parallel (multi‑head attention),\n",
      "allowing the model to capture different types of relationships simultaneously.  By stacking six identical encoder layers and six identical\n",
      "decoder layers, the Transformer can model long‑range dependencies efficiently, as each token can attend directly to any other token in the\n",
      "sequence. This design enables high‑performance sequence‑to‑sequence learning without the need for recurrent or convolutional components.\n",
      "____________________________________________________________________________________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: 5c287010-12fb-4d92-a43a-b284a24cf66b\n",
      "Similarity: 0.44075273766033246\n",
      "Text: Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since\n",
      "2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a\n",
      "majority of American governments have passed new laws since 2009 making the regis...\n",
      "____________________________________________________________________________________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 6a5faf38-6976-40e0-913e-d1b9b2deaf03\n",
      "Similarity: 0.37050245557029826\n",
      "Text: Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention\n",
      "consists of several attention layers running in parallel. of the values, where the weight assigned to each value is computed by a\n",
      "compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product A...\n",
      "____________________________________________________________________________________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: d7c048d5-01cb-47a8-912a-88ee115944ec\n",
      "Similarity: 0.31827600712232523\n",
      "Text: Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion .\n",
      "<EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>\n",
      "Input-Input Layer5 The Law will never be perfect , but its application sho...\n",
      "____________________________________________________________________________________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: d42eb78c-1ee6-4dbf-bc66-69148e3c566a\n",
      "Similarity: 0.3166574436679038\n",
      "Text: Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and\n",
      "point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1\n",
      "Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 i...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is Attention is All you need?\")\n",
    "pprint_response(response, show_source=True, wrap_width=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7619a5",
   "metadata": {},
   "source": [
    "### Using Persistent Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 15:41:13,296 - INFO - Loading all indices.\n",
      "2025-10-23 15:41:14,445 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Transformers are a type of neural‑network architecture that processes sequences using stacked self‑attention mechanisms\n",
      "instead of recurrence or convolution. The model is built from an encoder‑decoder structure, each composed of several identical layers.    In\n",
      "the encoder, each layer contains a multi‑head self‑attention sub‑layer followed by a position‑wise fully connected feed‑forward network;\n",
      "both sub‑layers are wrapped with residual connections and layer‑normalization.    The decoder mirrors the encoder but adds a third sub‑layer\n",
      "that performs multi‑head attention over the encoder’s output. Its self‑attention is masked so that each position can only attend to earlier\n",
      "positions, ensuring that predictions depend only on already generated tokens.    Overall, the Transformer relies on parallelizable attention\n",
      "operations, residual links, and normalization to model relationships within and across sequences.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, show_progress=True)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context=storage_context, embed_model=embed_model)\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"What are Transformers?\")\n",
    "\n",
    "pprint_response(response, wrap_width=140)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
